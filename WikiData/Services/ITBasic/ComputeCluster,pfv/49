%META:TOPICINFO{author="alles" comment="" date="1692360807" format="1.1" reprev="49" version="49"}%
%META:TOPICPARENT{name="WebHome"}%
<div style="float:right;background-color:white; z-index:999; border: solid 2px lightblue;">
---++++ !!Content
%TOC%
</div>

---+!! Slurm Compute Cluster

---++ *%RED%Currently known issues%ENDCOLOR%*
| *first reported* | *when we'll fix it* | *issue description* |
| 01/2023 | next downtime - 09/23 | accessing the private IP address of the login node does not work (e.g. for debugging, accessing ports other than SSH/22) |
| 01/2023 | next downtime - 09/23 | for some people, ssh sessions to the login node might be dropped irregularly | 
| 08/2023 | 08/23 | the walktrough usage example needs to be adapted to the new qos and partitions |
| 08/2023 | ongoing | the documentation / FAQ might be missing / incorrect / not complete at some points - if you feel so please let us know via ticket so we can add things.|

---++ *%BLUE%[[ComputeClusterFAQ][ðŸ‘‰ Frequently Asked Questions ðŸ‘ˆ]]%ENDCOLOR%*

<div style="border: solid 2px lightblue; font-weight:800; padding: 5px">
<p align="left">
*Check [[ComputeClusterFAQ][this FAQ]] first before opening a ticket with system-admin!*
</p>
</div>

---++ *%BLUE%[[https://public.ukp.informatik.tu-darmstadt.de/slurm-tutorial/slurm-tutorial.zip][ðŸ‘‰ Walkthrough Usage Example ðŸ‘ˆ]]%ENDCOLOR%*

<div style="border: solid 2px lightblue; font-weight:800; padding: 5px">
<p align="left">
Please <a href="https://public.ukp.informatik.tu-darmstadt.de/slurm-tutorial/slurm-tutorial.zip">download</a> and follow this tutorial project for our Slurm compute cluster to get started!

*We recommend to also read on first and understand the basic principle of SLURM, accounts, partitions and qos as well as how to use the central storage.*
</p>
</div>

---++ <u>Principle of Slurm</u>
Slurm is a job scheduling service. With Slurm, one performs compute experiments by connecting to a headnode where one can submit a compute job into a queue of compute jobs. Slurm prioritizes compute jobs in a way that every user gets his/her share of the compute resources that he/she deserves. If a compute job has a high enough priority and a node (== compute server) with sufficient resources is free, Slurm will automatically start the compute experiment. Once a job finishes, an email notification is sent.

---++ <u>The Cluster Headnode (slurm / slurm-login)</u>
---+++ Resource limits

   * /home folder
      * Your home folder of the login node is not reachable from compute nodes, hence you need to put your code and data on our [[ComputeClusterStorage][central cluster storage]]. 
      * The space is limited in =/home=, so please do not store more than 10GB, better not more than 1GB in your home folder!
   * CPU and RAM
      * Every user can use 2 CPU cores and 2 GB of RAM on the login node
      * If you run intensive jobs here (what you should NEVER do) you might break your ssh session to the login node. If you do so open up a [[https://ticket.ukp.informatik.tu-darmstadt.de][ticket]] with system-admin.
      * For installing and building things you can submit e.g. a cpu job to a compute node to avoid those issues. 

---+++ Access: TU Da wide

   * The cluster login node is reachable by SSH at =slurm.ukp.informatik.tu-darmstadt.de= from within the network of TU Darmstadt
   * This includes UKP network
   * If you work remotely... 
      * and are a **student**, consider using the [[https://www.hrz.tu-darmstadt.de/services/it_services/vpn/index.en.jsp][HRZ VPN]]
      * and are a **researcher** of UKP Lab consider using the [[/Services/Lab/VpnService][UKP VPN]]

---+++ Access: special for UKP Lab

%RED%currently the is no access to the private IP of the login node - see currently known issues at the top of this page.%ENDCOLOR%

---++ <u>Storage: Moving code and data into the cluster</u>

See [[ComputeClusterStorage]].

---++ <u>Quota, Accounts, Partitions and Resources</u>

Before explaining how user quotas work, you need to learn about accounts in Slurm.

---+++ !FairShare
Account quotas in Slurm are not hard limits. Instead, every account has a !FairShare value between 0.0 and 1.0 attached to it. Jobs can be submitted independent of the !FairShare value. However, *jobs by accounts with higher !FairShare value are prioritized by the scheduler*. By running jobs, the !FairShare value decreases depending on the duration and the resource requirements of the jobs. The !FairShare value recovers over time (it has a half-life of two weeks).

%INCLUDE{"ComputeClusterDetermineFairShare"}%

---+++ Accounts
Slurm tracks resource usage via so-called *accounts*. Slurm accounts work a bit like bank accounts, i.e. a single user can have multiple accounts.
The total compute power of the cluster is distributed across all accounts, meaning every account receives a percentage of the total compute power. Your affiliation (the lab you belong to) and your employment relation (researcher or student) decides over your accounts and their quota. If your affiliation is UKP/ATHENE, you will have one UKP account and one ATHENE account, giving you a larger share of the total compute power.

You can see your accounts via =sshare -U=. Add =--account=SOME_ACCOUNT= to your =sbatch= or =srun= call to run using a different account. Note: resource limits in partitions (see below) apply _per user_, not _per account_.

The following accounts exist and will be applied like follows:

| *Lab* | *%GREEN%UKP Lab%ENDCOLOR%* ||| *ATHENE* ||| *TK Lab* ||
| *Account* | ukp-researcher | ukp-student | ukp-gpu-large | athene-researcher | athene-student | athene-gpu-large | tk-researcher | tk-student |
| *Assigned to* | all UKP researchers | for UKP students on supervisor request (account form) | UKP researchers on special request with a very good reason for limited time| all ATHENE researchers (in adition to UKP researcher if both applys) | for ATHENE students on supervisor request | ATHENE researchers on special request with a very good reason for limited time | all TK researchers that request cluster access | for TK students on supervisor request |
| *Allowed qos* | cpu, cpu-long, gpu, yolo, interactive | cpu, gpu-small, yolo, interactive | gpu-large | cpu, cpu-long, gpu, yolo, interactive | cpu, gpu-small, yolo, interactive | gpu-large | cpu, cpu-long, gpu, yolo, interactive  | cpu, gpu-small, yolo, interactive |
| *Default qos* | gpu | gpu-small | gpu-large | gpu | gpu-small | gpu-large | gpu | gpu-small |

For access to account *-gpu-large open up a ticket at UKP system-admin, we'll check your application. As this is another account, you need to act like mentioned above: add =--account=SOME_ACCOUNT= to your =sbatch= or =srun= call.
To know more on partitions and QOS, please read on.

---+++ Partitions and Resource Limits (QOS)

When submitting a job in Slurm, one has to specify a *partition* into which the job will be queued. We have several partitions in our cluster for different purposes and hardware. Some are for gpu server only, some for cpu server only and for day-in-day-out experiments, another one is for interactive jobs with srun and so on. The partition =gpu= is default for all jobs. *If you are athene or tk related, you need to specify another partition in your jobs.*

Basically, the partition just selects the possible nodes briefly. 

---++++ Partitions
The follwoing partitions exist:

| *Lab* | *%GREEN%UKP Lab%ENDCOLOR%* ||| *ATHENE* || *TK Lab* ||
| *Available Partition* | gpu (default) | cpu | yolo | gpu-athene | yolo | gpu-tk | yolo |
| *Permission for* | UKP Researcher & Students | UKP Researcher & Students | UKP Researcher & Students | ATHENE Researcher & Students | ATHENE Researcher & Students | TK Researcher & Students | TK Researcher & Students |

---++++ QOS
QOS (quality of service) controls the runtime and amount of resources a user can use for a single job and in total of one of those partitions. 

*When submitting a job, one can request different QOS settings. The QOS influences the resource limits.*

The following QOS limits apply:

| *QOS* | *cpu* | *cpu-long* | *gpu* | *gpu-large* | *gpu-small* | *yolo* | *interactive* |
| *Allowed Accounts* | *-researcher, *-student | *-researcher | *-researcher | *-gpu-large | *-student | *-researcher, *-student | *-researcher, *-student |
| *Allowed Partitons* (depending on account, see above) | cpu, gpu-tk, gpu-athene | cpu-long, gpu-tk, gpu-athene | gpu, gpu-tk, gpu-athene | gpu, gpu-tk, gpu-athene | gpu, gpu-tk, gpu-athene | yolo | cpu, gpu, gpu-tk, gpu-athene |
| Max nodes per job | 1 | 1 | 1 | 1 | 1 | 1 | 1 |
| Max logical CPUs per job | 64 | 64 | 16 | 256 | 16 | unlimited | 16 |
| Max logical CPUs per user | 128 | 64 | 64 | 256 | 64 | unlimited | 64 |
| Max GPUs per job | 0 | 0 | 4 | 8 | 2 | unlimited | 2 |
| Max GPUs per user | 0 | 0 | 4 | 8 | 2 | unlimited | 2 |
| Max memory per job (GB) | 256| 256 | 256 | 1024 | 256 | unlimited | 256 |
| Max memory per user (GB) | 512 | 256 | 512 | 1014 | 256 | unlimited | 256 |
| Max job duration (days) | 3 | 7 | 3 | 1 | 3 | 3 | 8 hours |

---++++ Examples
To run a job in a specific partition use =-p PARTITIONNAME=. 

   * <u>For example, if you are associated with ATHENE</u>, you would want to use 
      * <verbatim>sbatch -p gpu-athene -a athene-researcher your_script.sh</verbatim>
      * *Explanation:* 
      * There is no need to specify the qos, as "gpu" is the default qos for account athene-researcher
      * But might need to specify the account using =-a athene-researcher= to use the correct account if you have been assigned multiple ones (remmber to check your accounts using =sshare -U=)

   * <u>For example, if you are associated with UKP and want to use the CPU partition</u>, you would want to use 
      * <verbatim>sbatch -p cpu -q cpu your_script.sh</verbatim>
      * or
      * <verbatim>sbatch -p cpu -q cpu-long your_script.sh</verbatim>
      * *Explanation:* 
      * You need to specify the QOS using =-q cpu= or =-q cpu-long=, as the default QOS for account "ukp-resarcher" is "gpu" which match the "cpu" partition!

   * <u>For example, if you are associated with UKP and want to use "gpu-large" QOS (on partition "gpu")</u> after your application for account "ukp-gpu-large" is complete, you would want to use 
      * <verbatim>sbatch -p gpu -a ukp-gpu-large your_script.sh</verbatim>
      * *Explanation:* 
      * You need to specify the account using =-a ukp-gpu-large= to use the correct account as you now have multiple ones ("ukp-gpu-large" and maybe "ukp-researcher", remmber to check your accounts using =sshare -U=)
      * There is no need to specify the QOS, as "gpu-large" is the default qos for the account "ukp-gpu-large"

---++++ Special Partitions
---+++++ The yolo partition
The =yolo= partition is useful if you have many medium to low priority jobs which are not time-critical. It is available for researchers and students.

Jobs in this partition:
   * will utilize any currently idling and unclaimed compute resources of the cluster and
   * will *bypass all resource limits* (max GPUs per user, etc.) but
   * will be *cancelled and requeued if a higher priority job from a different partition demands resources*
The =yolo= partition contains *all* nodes in the cluster, i.e. it makes it possible to run jobs on compute nodes from other labs users normally would not have access to. Jobs are limited to a runtime of 3 days, and !FairShare is computed with the same rates as usual. For the sake of the climate, please make use of checkpointing to avoid pointless recomputation in case your job gets cancelled.

To submit a job in the =yolo= partition, run<verbatim>
sbatch -q yolo -p yolo myscript.sh
</verbatim>

---+++++ The interactive partition
The =interactive= partition is useful if you want to run interactive jobs using srun. It is available for researchers and students.

Jobs submitted to any qos / partition that use srun will automatically moved to the interactive partition to keep the cluster resources free. 

Jobs in this partition:
   * will run max. 8 hours

A detailed overview of all available partitions is shown with =scontrol show partition=

---++ <u>Monitoring</u>

Visit https://monitoring.ukp.informatik.tu-darmstadt.de to view the load of the whole cluster and individual nodes. This page is accessible from within the TU Darmstadt network.

---++ <u>Slurm Command Documentation / Ext. References</u>

| *Command* | *Purpose* | *Example* | *Documentation* |
| =srun= | Schedule a compute job. Executed in the foreground (directly on your terminal). | =srun --pty bash=  | [[https://slurm.schedmd.com/srun.html]] |
| =sbatch= | Schedule a compute job. Executed in the background. | =sbatch myscript.sh= | [[https://slurm.schedmd.com/sbatch.html]] |
| =squeue= | Show the job queue and status. | =squeue --me= | [[https://slurm.schedmd.com/squeue.html]] |
| =scancel= | Cancel a job. | =scancel [job_id]= | [[https://slurm.schedmd.com/scancel.html]] |
| =sinfo= | Show status of cluster nodes and available resources. | =sinfo --Format=nodehost,statelong,cpus,memory,gres= | [[https://slurm.schedmd.com/sinfo.html]] |
| =sshare= | Display accounts and !FairShare. | =sshare -a= | [[https://slurm.schedmd.com/sshare.html]] |

---++ <u>Running Container in SLURM</u>

---+++ Why Apptainer instead of docker?

   * There are some important differences between Docker and Apptainer:
      * Docker and Apptainer have their own container formats.
      * Docker containers may be imported to run via Apptainer.
      * Docker containers need root privileges for full functionality which is not suitable for a shared HPC environment.
      * Apptainer allows working with containers as a regular user.

---+++ Apptainer on the Cluster

   * *As we have only recently implemented this feature, there may be issues or situations that have not been tested or are uncomfortable to use. If you notice anything that needs to be changed or adapted, please let us know via ticket@ukp...*
   * Apptainer is available only on the compute nodes on the cluster. Therefore, to use it you need to either start an interactive job or submit a batch-job to the available SLURM queues.
   * In the below examples we illustrate the interactive use of Apptainer in an interactive bash shell.
   * grab a bash shell, e.g.
      * <verbatim>srun -p yolo --qos yolo --gres gpu:1 --pty bash -i</verbatim>
   * check apptainer version
      * <verbatim>bash-4.4$ apptainer --version
apptainer version 1.1.9-1.el8 </verbatim>
   * the most up-to-date help comes from apptainer itself
      * <verbatim>bash-4.4$ apptainer --help

Linux container platform optimized for High Performance Computing (HPC) and
Enterprise Performance Computing (EPC)

Usage:
  apptainer [global options...]

Description:
  Apptainer containers provide an application virtualization layer enabling
[...] shortened
</verbatim>

---+++ Getting existing images onto the cluster

   * Apptainer uses container images which you can scp or rsync to the cluster as you would do with any other file. See Copying Data to & from the cluster for more information.
   * Note: your folder on the central storage of UKP will be used. You should see a folder /ukp-storage-1/$USERNAME/.apptainer/ that contains all your Apptainer data. 
   * Apptainer is a fork of Singularity, so must of Singularitys commands and images should work, too.
   * You can also use the pull or build commands to download pre-built images from external resources, such as Singularity Hub (as of April 26th 2021, Singularity Hub is a read-only archive), the Sylabs Container Library or Docker Hub. For instance, you can download a native Singularity image with its default name from Singularity Hub with:
      * <verbatim>bash-4.4$ apptainer pull shub://vsoch/hello-world
INFO:    Downloading shub image
59.8MiB / 59.8MiB [====================================================================================================================================================================================================================================] 100 % 23.3 MiB/s 0s
bash-4.4$ </verbatim>
   * The downloaded image file is hello-world_latest.sif. You can also pull the image with a customized name; e.g., hello.sif:
      * <verbatim>apptainer pull --name hello.sif shub://vsoch/hello-world</verbatim>
      * <verbatim>bash-4.4$ apptainer pull --name hello.sif shub://vsoch/hello-world
INFO:    Use cached image
bash-4.4$ </verbatim>
      * Note: the image was already downloaded by me sometime ago, so it is still there and the cached one will be used.
   * Similarly, you can pull images from Docker Hub:
      * <verbatim>apptainer pull docker://ghcr.io/apptainer/lolcow</verbatim> 

---+++ Working with images

   * When working with images you could either start an interactive session, or submit a Apptainer job to the available queues. For these examples, we will use a lolcow image from Docker Hub in an interactive bash shell.
      * <verbatim>srun -p yolo --qos yolo --gres gpu:1 --pty bash -i</verbatim>
      * <verbatim>bash-4.4$ apptainer pull docker://ghcr.io/apptainer/lolcow
INFO:    Converting OCI blobs to SIF format
INFO:    Starting build...
Getting image source signatures
Copying blob 5ca731fc36c2 done
Copying blob 16ec32c2132b done
Copying config fd0daa4d89 done
Writing manifest to image destination
Storing signatures
2023/07/18 14:49:51  info unpack layer: sha256:16ec32c2132b43494832a05f2b02f7a822479f8250c173d0ab27b3de78b2f058
2023/07/18 14:49:52  info unpack layer: sha256:5ca731fc36c28789c5ddc3216563e8bfca2ab3ea10347e07554ebba1c953242e
INFO:    Creating SIF file...
bash-4.4$</verbatim>
   * check on the conatiner files
      * <verbatim>bash-4.4$ ls
hello.sif lolcow_latest.sif</verbatim>
   * run a shell
      * <verbatim>bash-4.4$ apptainer shell lolcow_latest.sif
INFO:    underlay of /etc/localtime required more than 50 (77) bind mounts
Apptainer> </verbatim>
   * run a command
      * <verbatim>Apptainer> cowsay moo
 _____
< moo >
 -----
        \   ^__^
         \  (oo)\_______
            (__)\       )\/\
                ||----w |
                ||     ||
Apptainer> </verbatim>
   * you can also pass arguments to the containers
      * <verbatim>bash-4.4$ apptainer exec lolcow_latest.sif cowsay moo
INFO:    underlay of /etc/localtime required more than 50 (77) bind mounts
 _____
< moo >
 -----
        \   ^__^
         \  (oo)\_______
            (__)\       )\/\
                ||----w |
                ||     ||
bash-4.4$ </verbatim> 

---+++ GPU example

   * To access Nvidia GPU card driver installed inside of Apptainer container you need to use =--nv= option while executing the container. To verify that you have access to the requested GPUs, run =nvidia-smi= inside the container:
   * <verbatim>bash-4.4$ singularity exec --nv lolcow_latest.sif /bin/bash
INFO:    underlay of /etc/localtime required more than 50 (77) bind mounts
INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (329) bind mounts
Apptainer> nvidia-smi
Tue Jul 18 14:56:51 2023
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla P100-PCIE-16GB            On | 00000000:86:00.0 Off |                    0 |
| N/A   51C    P0               31W / 250W|      0MiB / 16384MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
Apptainer> </verbatim>
   * you can see that the requested GPU is available in Apptainer

---+++ Accessing files from container

   * Files and directories on the cluster are accessible from within the container. By default, directories under =/scratch=, =/mnt/beegfs/work/$USERNAME/= (central storage), =/mnt/beegfs/shared/= (central storage) and =/tmp= are available at runtime inside the container:
      * <verbatim>Apptainer> df -h
Filesystem             Size  Used Avail Use% Mounted on
tmpfs                   16M   12K   16M   1% /
squashfuse_ll           72M   72M     0 100% /opt
/dev/sda2               69G  6.4G   59G  10% /etc/hosts
devtmpfs               189G     0  189G   0% /dev
tmpfs                  189G  8.0K  189G   1% /dev/shm
slurm-nfs:/nfs-mounts  503G   75G  403G  16% /nfs
beegfs_nodev           466T  202T  265T  44% /mnt/beegfs
/dev/sda3              809G   36K  768G   1% /scratch
tmpfs                  189G     0  189G   0% /sys/fs/cgroup
tmpfs                  189G   50M  188G   1% /run/nvidia-persistenced/socket
Apptainer></verbatim>

---+++ Singularity containers as SLURM jobs

   * once you have build and moved the container to your storage location, you can also submit Apptainer jobs with sbatch
      * =srun apptainer_test.sh=
      * with hello_world.sif container at =/ukp-storage-1/$USERNAME/hello_world.sif=
      * apptainer_test.sh:
         * <verbatim>#!/bin/bash
#
#SBATCH --job-name=apptainer_test
#SBATCH --output=/ukp-storage-1/$USERNAME/result.txt
#SBATCH --error=/ukp-storage-1/$USERNAME/error.txt
#SBATCH --mail-type=ALL
#SBATCH --mail-user=$USERNAME@ukp.tu-darmstadt.de
#SBATCH --partition=gpu
#SBATCH --time=10:00
#SBATCH --mem-per-cpu=100

# Apptainer command line options
apptainer exec hello_world.sif cat /etc/os-release</verbatim>
   * result.txt:
      * <verbatim>
NAME="Ubuntu"
VERSION="14.04.6 LTS, Trusty Tahr"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 14.04.6 LTS"
VERSION_ID="14.04"
HOME_URL="http://www.ubuntu.com/"
SUPPORT_URL="http://help.ubuntu.com/"
BUG_REPORT_URL="http://bugs.launchpad.net/ubuntu/"</verbatim> 

%META:TOPICMOVED{by="bugert" date="1570804848" from="Services.ComputeCluster" to="Services/Campus.ComputeCluster"}%

%META:TOPICINFO{author="LeonardSwiezinski" date="1343079784" format="1.1" version="14"}%
%META:TOPICPARENT{name="Main.WebHome"}%
---+ WebCorpus

---++ About
This project aims to generate information (using hadoop) from a large corpus of content from webpages (Leipzig Corpus). 
FIXME

---++ Project Goals
   * Segment documents, paragraphs, sentences, tokens. 
   * Filter duplicates. 
   * Filter unexpected languages. 
   * Generate corpus of n-grams. 
   * Generate corpus of cooccurrences. 
   * Integrate with DKPro

---+ Pipeline

---++ Visualization
<img src="%ATTACHURLPATH%/main.gif" alt="main.gif" width="928" height="997" />
 
---++ Hadoop Jobs
All jobs are located in the package webcorpus.hadoopjobs.

---+++ DocumentJob
   * Takes raw input data and retrieves single documents. 
   * Filters documents with missing URL, or content=null or content="null".
   * Wraps URL entries in source metadata (*sm*) with CDATA markup. 
   * Normalizes Encoding to UT8 if other source encoding is given in source metadata. 
   * Adds processing metadata (*pm*) and adds document length as entry "length". 
   * Detects paragraphs by multiple line breaks and wraps them with p-tags. 
   * Normalizes whitespace (replace all line breaks, tabs and multiple whitespaces with single whitespaces) and trim text. 
   * Detects duplicates by same URL, same length and same content in first and last n characters. 
   * Output one document per line. Format (replace # with tab): *URL#sm#pm#document*

---+++ DeduplicationJob
   * Create key, by concatenating document length and first and last n characters. 
   * Use BloomFilter, to test, if document already appeared. 
   * If *Conf.dropFilteredItems* is set to false, every item with reoccurring URL will be labeled with _is_duplicate:=[true|false]_.

---+++ UTF8Job
   * Labels (_encoding_error:=[true|false]_) or removes documents with defective encoding.
   * Detects defective encoding just by looking for "&#65533;" (unknown glyph).

---+++ SentenceJob
   * Wraps sentences with "s"-Tags.
   * webcorpus_conf/segmentizer.zip 
      * boundariesFile.txt 
      * postList.txt 
      * postRules.txt 
      * preList.txt 
      * preRules.txt
Adding these files as single files to the distributed cache has been problematic (reason unclear yet), but adding them as an archive file finally worked as expected. 

<verbatim>
// Add to the cache with symlink "segmentizer" in public int run(String[] args):
DistributedCache.addCacheArchive(new URI("webcorpus_conf/segmentizer.zip#segmentizer"), conf);
DistributedCache.createSymlink(conf);

// The segmentizer expects an URL for every config file as parameter 
// create a HashMap to store all the URLs in the Mapper
private Map<String, URL> urlMap;

// retrieve from cache in mapper
private void getResources(){
	URL resource = job.getResource("segmentizer");
	File inputPath = new File(resource.getPath());
	for (File f : inputPath.listFiles()) {
		File file = new File(f.getAbsolutePath());
		if(file.exists()){
			try {
				urlMap.put(file.getName(), file.toURI().toURL());
			} catch (MalformedURLException e) {}
		}
	}
}
</verbatim>

---+++ LanguageJob
Sentence based language detection using jlani. 
   * Takes optional parameter [language_name] (ISO 639-1) to set language to look for. 
      * hadoop jar webcorpus.jar webcorpus.hadoopjobs.LanguageJob webcorpus_data/sentence webcorpus_data/language [language_name] 
   * Sentences, where the detected language (lani) matches language_name, will be labeled with: lang=language_name, lani=language_name
   * Where lani does not match language_name for a sequence of sentences, following rules apply:
      * if sequence is at the beginning of a paragraph: lang=lani, lani=lani
      * if sequence is at the end of a paragraph: lang=lani, lani=lani
      * else if sequence is not longer than Conf.maximumUnknownLanguageLength: lang=language_name, lani=lani
      * else: lang=lani, lani=lani
   * if Conf.dropFilteredItems is true, only sentences with lang=language_name will be kept. 
   * jlani expects some files. In order to work with hadoop, they are packed into jlani.zip, which remains in the hdfs (webcorpus_conf/jlani.zip): 
      * blacklist_utf8.txt 
      * mappings.txt 
      * de.ser.gz 
      * en.ser.gz 
      * ... (models for more languages) 
   * The files are loaded from the distributed cache in a similar way, SentenceJob loads files.

---+++ TokenJob
   * Tokenizes sentences and wraps all tokens with t-tag. 
   * Employs Stanford Tokenizer from the Stanford POS-Tagger package. 
   * Writes number of tokens to processing metadata: token_count

---+++ NGramJob
   * Counts n-grams based on tokens. 
   * expects a pameter to set n: 
      * hadoop jar webcorpus.jar webcorpus.hadoopjobs.NGramJob webcorpus_data/token webcorpus_data/ngram [n]

---+ How to run

   * Extract the contents of the project archive to a directory (e.g. "webcorpus")
      * The resulting structure should look like this:
         * webcorpus/
            * example/
               * jobconf.txt
               * run.sh
            * conf/
               * jlani.zip
               * segmentizer.zip
            * webcorpus.jar
   * *> cd webcorpus*
   * Choose a name for your project (e.g. "mycorpus") and create a copy of "example/" with this name.
      * *> mkdir mycorpus*
      * the resulting structure should look like: webcorpus/mycorpus/
   * Create a HDFS-directory for your project
      * *> hadoop dfs -mkdir webcorpus_mycorpus*
   * Create a HDFS-directory called "webcorpus_conf" for configuration resources, required by some of the jobs.
      * *> hadoop dfs -mkdir webcorpus_conf*
   * Copy the contents of webcorpus/conf/ to the previously created directory:
      * *> hadoop dfs -copyFromLocal conf/jlani.zip webcorpus_conf/*
      * *> hadoop dfs -copyFromLocal conf/segmentizer.zip webcorpus_conf/*
   * edit webcorpus/mycorpus/run.sh :
      * change the HDFS-path according to the previously chosen path:
         * *HDFS_DIRECTORY=webcorpus_mycorpus*
      * Optionally change the language setting.
   * Copy raw data to webcorpus_mycorpus/raw/
      * hadoop dfs -copyFromLocal /path/to/raw/data webcorpus_mycorpus/raw/
   * Edit webcorpus/mycorpus/jobconf.txt according to your requirements.
   * *> cd mycorpus*
   * *> sh run.sh*

---++ Shellscript to run the jobs (run.sh)
<verbatim>
HDFS_DIRECTORY=example
JAR=../webcorpus.jar
LANGUAGE=de

# run
hadoop jar ${JAR} webcorpus.hadoopjobs.DocumentJob ${HDFS_DIRECTORY}/raw ${HDFS_DIRECTORY}/document
hadoop jar ${JAR} webcorpus.hadoopjobs.DeduplicationJob ${HDFS_DIRECTORY}/document ${HDFS_DIRECTORY}/deduplication
hadoop jar ${JAR} webcorpus.hadoopjobs.UTF8Job ${HDFS_DIRECTORY}/deduplication ${HDFS_DIRECTORY}/utf8
hadoop jar ${JAR} webcorpus.hadoopjobs.SentenceJob ${HDFS_DIRECTORY}/utf8 ${HDFS_DIRECTORY}/sentence
hadoop jar ${JAR} webcorpus.hadoopjobs.LanguageJob ${HDFS_DIRECTORY}/sentence ${HDFS_DIRECTORY}/language ${LANGUAGE}
hadoop jar ${JAR} webcorpus.hadoopjobs.TokenJob ${HDFS_DIRECTORY}/language ${HDFS_DIRECTORY}/token
hadoop jar ${JAR} webcorpus.hadoopjobs.NGramJob ${HDFS_DIRECTORY}/token ${HDFS_DIRECTORY}/1gram 1
hadoop jar ${JAR} webcorpus.hadoopjobs.NGramJob ${HDFS_DIRECTORY}/token ${HDFS_DIRECTORY}/2gram 2
hadoop jar ${JAR} webcorpus.hadoopjobs.NGramJob ${HDFS_DIRECTORY}/token ${HDFS_DIRECTORY}/3gram 3
hadoop jar ${JAR} webcorpus.hadoopjobs.CooccurrenceJob ${HDFS_DIRECTORY}/token ${HDFS_DIRECTORY}/cooccurrence 3

</verbatim>

---+ Configuration
All configuration is done per project in a dedicated configuration file called "jobconf.txt".

---+ Documented example configuration file
<verbatim>

# Length of tested String (each at start and end)  for deduplication.
# int, (0, MAX_STRING_LENGTH / 2] - default: 1000 
dedupTestLength=1000

# The number of bits in the vector. => |Expected Values| * log_2(1 / FP-Rate)
# int, (0, MAX_INT] - default: 256 
dedupBloomVectorSize=256

# Deduplication BloomFilter: The number of hash function to consider.
# int - default: 1024
dedupBloomNbHash=1024

# Deduplication Bloom Filter hash function
# {murmur, jenkins} - default: jenkins
dedupBloomHashFunction=jenkins

# Maximum length of text, with other language detected than expected.
# int - default: 200
maximumUnknownLanguageLength=200

# Drop filtered documents, intead of labeling them.
# bool - default: false (for testing, use true for production)
dropFilteredItems=true

# Use sequence files instead of text files.
# bool - default: false (for testing, use true for production)
useSequenceFiles=false

# Number of MapTasks
# int - default: 100
numberOfMapTasks=100

# Number of ReduceTasks
# int - default: 100
numberOfReduceTasks=100

# Options for Stanford Tokenizer
# string - default: ""
#tokenizerOptions=

# Separator for multiple keys (e.g. for CooccurrenceJob)
# string - default: @@
keySeparator=@@

# Separator for sentences
# string - default: <s>
sentenceSeparator=<s>
</verbatim>

---+ Data example
FIXME
---++ Example of raw data entry
<verbatim>
<source><location>http://www.arturbecker.de/Prosa/Milchstrasse/Milchstrasse_Leseprobe/milchstrasse_leseprobe.html</location><date>2011-02-02</date><user>Treasurer</user><original_encoding>utf-8</original_encoding><language>deu</language><issue>none</issue></source>
Artur Becker machte mit seiner Lesung aus dem Roman »Onkel Jimmy, die Indianer und ich« in Klagenfurt zwar keinen Preis, gewann dafür aber beim 
Publikum, bei Rezensenten und Lesern Komplizen fürs Leben.
</verbatim>

---++ Same data after sentence splitting
<verbatim>
http://www.arturbecker.de/Prosa/Milchstrasse/Milchstrasse_Leseprobe/milchstrasse_leseprobe.html <source><location><![CDATA[http://www.arturbecker.de/Prosa/Milchstrasse/Milchstrasse_Leseprobe/milchstrasse_leseprobe.html]]></location><date>2011-02-02</date><user>Treasurer</user><original_encoding>utf-8</original_encoding><language>deu</language><issue>none</issue></source> <process><length>5934</length><is_duplicate>false</is_duplicate><is_duplicate>false</is_duplicate><encoding_error>false</encoding_error></process>  <p><s>Artur Becker machte mit seiner Lesung aus dem Roman »Onkel Jimmy, die Indianer und ich« in Klagenfurt zwar keinen Preis, gewann dafür aber beim Publikum, bei Rezensenten und Lesern Komplizen fürs Leben.</s></p>
</verbatim>



%META:FILEATTACHMENT{name="main.png" attachment="main.png" attr="h" comment="" date="1333904121" path="main.png" size="224788" user="Main.LeonardSwiezinski" version="1"}%
%META:FILEATTACHMENT{name="main.gif" attachment="main.gif" attr="h" comment="WebCorpus hadoop flow" date="1333904228" path="main.gif" size="96653" user="Main.LeonardSwiezinski" version="1"}%
%META:TOPICMOVED{by="NicoErbs" date="1337256032" from="Main.UKPWebCorpus" to="Hiwi.UKPWebCorpus"}%

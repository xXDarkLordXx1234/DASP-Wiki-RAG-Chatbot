%META:TOPICINFO{author="LeonardSwiezinski" date="1344770474" format="1.1" reprev="15" version="15"}%
%META:TOPICPARENT{name="Main.WebHome"}%
---+ WebCorpus

---++ About
This project aims to create a system that generates information like n-gram counts, cooccurrence counts, or isolated sentences from a large corpus of webpages for a language of choice. Parallel processing of such tasks can lead to a huge performance benefit over serial processing. [[http://wayback.archive.org/web/*/http://labs.google.com/papers/mapreduce.html][MapReduce]] provides a programming model for parallel processing. We chose [[http://hadoop.apache.org/][hadoop]] as a MapReduce framework. Our system is built as a pipeline of hadoop MapReduce jobs. The raw data is kindly provided by the [[http://wortschatz.uni-leipzig.de/findlinks/][FindLinks]] team at the [[http://uni-leipzig.de][university of leipzig]]. 

---++ Project Subtasks
   * Extract documents from the raw data and provide them for further tasks in a standardized format aligned with metadata such as document URL and crawl date. We will refer to this job as [[#DocumentJob][DocumentJob]].
   * Web crawling usually leads to a lot of noise, so some basic cleanup tasks need to be performed:
      * Deduplication of documents, as a documents might occur multiple times due to recrawling of a document or context variations, like a print page for a document that occurred already as a normal page. ([[#DeduplicationJob][DeduplicationJob]], [[#DeduplicationByHostJob][DeduplicationByHostJob]])
      * Filtering of documents with malformed encodings. ([[#UTF8Job][UTF8Job]])
   * Inner segmentation of documents for further processing:
      * Detection of paragraphs in documents. ([[#DocumentJob][DocumentJob]])
      * Detection of sentences in paragraphs. ([[#SentenceJob][SentenceJob]])
      * Detection of tokens in sentences. ([[#TokenJob][TokenJob]])
   * Filtering of sentences with a language other than the chosen language.
   * Generate corpus of n-grams. ([[#NGramJob][NGramJob]])
   * Generate corpus of cooccurrences. ([[#CooccurrenceJob][CooccurrenceJob]])
   * Extract sentences with clearly detected language in a standardized format. ([[#SentenceExtractJob][SentenceExtractJob]])

---+ Pipeline

---++ Visualization
<img src="%ATTACHURLPATH%/main.gif" alt="main.gif" width="928" height="997" />
 
---++ Hadoop Jobs
All jobs are located in the package webcorpus.hadoopjobs.

---+++ DocumentJob
   * Takes raw input data and retrieves single documents. 
   * Filters documents with missing URL, or content=null or content="null".
   * Wraps URL entries in source metadata (*sm*) with CDATA markup. 
   * Normalizes Encoding to UT8 if other source encoding is given in source metadata. 
   * Adds processing metadata (*pm*) and adds document length as entry "length". 
   * Detects paragraphs by multiple line breaks and wraps them with p-tags. 
   * Normalizes whitespace (replace all line breaks, tabs and multiple whitespaces with single whitespaces) and trim text. 
   * Detects duplicates by same URL, same length and same content in first and last n characters. 
   * Output one document per line. Format (replace # with tab): *URL#sm#pm#document*

---++++ Data example
---+++++ Before
<verbatim>
<source><location>http://document.url/1</location><date>2012-01-01</date><user>Tom</user><original_encoding>utf-8</original_encoding><language>deu</language></source>

              Dies ist eine Testdatei       mit zwei Absätzen.


Zweiter Absatz.


</verbatim>
---+++++ Call
<verbatim>
hadoop jar webcorpus.jar webcorpus.hadoopjobs.DocumentJob webcorpus_data/raw webcorpus_data/document
</verbatim>
---+++++ After
<verbatim>
http://document.url/1#TAB#<source><location><![CDATA[http://document.url/1]]></location><date>2012-01-01</date><user>Tom</user><original_encoding>utf-8</original_encoding><language>deu</language></source>#TAB#<process><length>83</length></process>#TAB#<p>Dies ist eine Testdatei mit zwei Absätzen.</p><p>Zweiter Absatz.</p> 
</verbatim>
The complete document is printed in one line with normalized whitespace. #TAB# is used in this example to illustrate the tab character. 

---+++ DeduplicationJob
   * Create key, by concatenating document length and first and last n characters. 
   * Use BloomFilter, to test, if document already appeared. 
   * If *Conf.dropFilteredItems* is set to false, every item with reoccurring URL will be labeled with _is_duplicate:=[true|false]_.

---+++ DeduplicationByHostJob
   * Same as DeduplicationJob, but looks for items with reoccurring host.

---+++ UTF8Job
   * Labels (_encoding_error:=[true|false]_) or removes documents with defective encoding.
   * Detects defective encoding just by looking for "&#65533;" (unknown glyph).

---+++ SentenceJob
   * Wraps sentences with "s"-Tags.
   * webcorpus_conf/segmentizer.zip 
      * boundariesFile.txt 
      * postList.txt 
      * postRules.txt 
      * preList.txt 
      * preRules.txt
Adding these files as single files to the distributed cache has been problematic (reason unclear yet), but adding them as an archive file finally worked as expected. 

<verbatim>
// Add to the cache with symlink "segmentizer" in public int run(String[] args):
DistributedCache.addCacheArchive(new URI("webcorpus_conf/segmentizer.zip#segmentizer"), conf);
DistributedCache.createSymlink(conf);

// The segmentizer expects an URL for every config file as parameter 
// create a HashMap to store all the URLs in the Mapper
private Map<String, URL> urlMap;

// retrieve from cache in mapper
private void getResources(){
	URL resource = job.getResource("segmentizer");
	File inputPath = new File(resource.getPath());
	for (File f : inputPath.listFiles()) {
		File file = new File(f.getAbsolutePath());
		if(file.exists()){
			try {
				urlMap.put(file.getName(), file.toURI().toURL());
			} catch (MalformedURLException e) {}
		}
	}
}
</verbatim>

---+++ LanguageJob
Sentence based language detection using jlani. 
   * Takes optional parameter [language_name] (ISO 639-1) to set language to look for. 
      * hadoop jar webcorpus.jar webcorpus.hadoopjobs.LanguageJob webcorpus_data/sentence webcorpus_data/language [language_name] 
   * Sentences, where the detected language (lani) matches language_name, will be labeled with: lang=language_name, lani=language_name
   * Where lani does not match language_name for a sequence of sentences, following rules apply:
      * if sequence is at the beginning of a paragraph: lang=lani, lani=lani
      * if sequence is at the end of a paragraph: lang=lani, lani=lani
      * else if sequence is not longer than Conf.maximumUnknownLanguageLength: lang=language_name, lani=lani
      * else: lang=lani, lani=lani
   * if Conf.dropFilteredItems is true, only sentences with lang=language_name will be kept. 
   * jlani expects some files. In order to work with hadoop, they are packed into jlani.zip, which remains in the hdfs (webcorpus_conf/jlani.zip): 
      * blacklist_utf8.txt 
      * mappings.txt 
      * de.ser.gz 
      * en.ser.gz 
      * ... (models for more languages) 
   * The files are loaded from the distributed cache in a similar way, SentenceJob loads files.

---+++ TokenJob
   * Tokenizes sentences and wraps all tokens with t-tag. 
   * Employs Stanford Tokenizer from the Stanford POS-Tagger package. 
   * Writes number of tokens to processing metadata: token_count

---+++ NGramJob
   * Counts n-grams based on tokens. 
   * expects a pameter to set n: 
      * hadoop jar webcorpus.jar webcorpus.hadoopjobs.NGramJob webcorpus_data/token webcorpus_data/ngram [n]

---++++ Data example
---+++++ Before
<verbatim>
<source><location>http://document.url/1</location><date>2012-01-01</date><user>Tom</user><original_encoding>utf-8</original_encoding><language>deu</language></source>
<p><s lani="de" lang="de">Dies ist eine Testdatei mit nur einem Satz .</s></p> 
</verbatim>
---+++++ Call
<verbatim>
hadoop jar webcorpus.jar webcorpus.hadoopjobs.NGramJob webcorpus_data/token webcorpus_data/ngram [n]
</verbatim>
(Replace [n] with the desired int value of n)
---+++++ After
<verbatim>
HELLO
</verbatim>

---+++ CooccurrenceJob
   * Counts cooccurrences based on tokens with distance up to n. 
   * expects a pameter to set n: 
      * hadoop jar webcorpus.jar webcorpus.hadoopjobs.CooccurrenceJob webcorpus_data/token webcorpus_data/cooccurrence [n]

---+ How to run

   * Extract the contents of the project archive to a directory (e.g. "webcorpus")
      * The resulting structure should look like this:
         * webcorpus/
            * example/
               * jobconf.txt
               * run.sh
            * conf/
               * jlani.zip
               * segmentizer.zip
            * webcorpus.jar
   * *> cd webcorpus*
   * Choose a name for your project (e.g. "mycorpus") and create a copy of "example/" with this name.
      * *> mkdir mycorpus*
      * the resulting structure should look like: webcorpus/mycorpus/
   * Create a HDFS-directory for your project
      * *> hadoop dfs -mkdir webcorpus_mycorpus*
   * Create a HDFS-directory called "webcorpus_conf" for configuration resources, required by some of the jobs.
      * *> hadoop dfs -mkdir webcorpus_conf*
   * Copy the contents of webcorpus/conf/ to the previously created directory:
      * *> hadoop dfs -copyFromLocal conf/jlani.zip webcorpus_conf/*
      * *> hadoop dfs -copyFromLocal conf/segmentizer.zip webcorpus_conf/*
   * edit webcorpus/mycorpus/run.sh :
      * change the HDFS-path according to the previously chosen path:
         * *HDFS_DIRECTORY=webcorpus_mycorpus*
      * Optionally change the language setting.
   * Copy raw data to webcorpus_mycorpus/raw/
      * hadoop dfs -copyFromLocal /path/to/raw/data webcorpus_mycorpus/raw/
   * Edit webcorpus/mycorpus/jobconf.txt according to your requirements.
   * *> cd mycorpus*
   * *> sh run.sh*

---++ Shellscript to run the jobs (run.sh)
<verbatim>
HDFS_DIRECTORY=example
JAR=../webcorpus.jar
LANGUAGE=de

# run
hadoop jar ${JAR} webcorpus.hadoopjobs.DocumentJob ${HDFS_DIRECTORY}/raw ${HDFS_DIRECTORY}/document
hadoop jar ${JAR} webcorpus.hadoopjobs.DeduplicationJob ${HDFS_DIRECTORY}/document ${HDFS_DIRECTORY}/deduplication
hadoop jar ${JAR} webcorpus.hadoopjobs.UTF8Job ${HDFS_DIRECTORY}/deduplication ${HDFS_DIRECTORY}/utf8
hadoop jar ${JAR} webcorpus.hadoopjobs.SentenceJob ${HDFS_DIRECTORY}/utf8 ${HDFS_DIRECTORY}/sentence
hadoop jar ${JAR} webcorpus.hadoopjobs.LanguageJob ${HDFS_DIRECTORY}/sentence ${HDFS_DIRECTORY}/language ${LANGUAGE}
hadoop jar ${JAR} webcorpus.hadoopjobs.TokenJob ${HDFS_DIRECTORY}/language ${HDFS_DIRECTORY}/token
hadoop jar ${JAR} webcorpus.hadoopjobs.NGramJob ${HDFS_DIRECTORY}/token ${HDFS_DIRECTORY}/1gram 1
hadoop jar ${JAR} webcorpus.hadoopjobs.NGramJob ${HDFS_DIRECTORY}/token ${HDFS_DIRECTORY}/2gram 2
hadoop jar ${JAR} webcorpus.hadoopjobs.NGramJob ${HDFS_DIRECTORY}/token ${HDFS_DIRECTORY}/3gram 3
hadoop jar ${JAR} webcorpus.hadoopjobs.CooccurrenceJob ${HDFS_DIRECTORY}/token ${HDFS_DIRECTORY}/cooccurrence 3

</verbatim>

---+ Configuration
All configuration is done per project in a dedicated configuration file called "jobconf.txt".

---+ Documented example configuration file
<verbatim>

# Length of tested String (each at start and end)  for deduplication.
# int, (0, MAX_STRING_LENGTH / 2] - default: 1000 
dedupTestLength=1000

# The number of bits in the vector. => |Expected Values| * log_2(1 / FP-Rate)
# int, (0, MAX_INT] - default: 256 
dedupBloomVectorSize=256

# Deduplication BloomFilter: The number of hash function to consider.
# int - default: 1024
dedupBloomNbHash=1024

# Deduplication Bloom Filter hash function
# {murmur, jenkins} - default: jenkins
dedupBloomHashFunction=jenkins

# Maximum length of text, with other language detected than expected.
# int - default: 200
maximumUnknownLanguageLength=200

# Drop filtered documents, intead of labeling them.
# bool - default: false (for testing, use true for production)
dropFilteredItems=true

# Use sequence files instead of text files.
# bool - default: false (for testing, use true for production)
useSequenceFiles=false

# Number of MapTasks
# int - default: 100
numberOfMapTasks=100

# Number of ReduceTasks
# int - default: 100
numberOfReduceTasks=100

# Options for Stanford Tokenizer
# string - default: ""
#tokenizerOptions=

# Separator for multiple keys (e.g. for CooccurrenceJob)
# string - default: @@
keySeparator=@@

# Separator for sentences
# string - default: <s>
sentenceSeparator=<s>
</verbatim>

---+ Data example
FIXME
---++ Example of raw data entry
<verbatim>
<source><location>http://www.arturbecker.de/Prosa/Milchstrasse/Milchstrasse_Leseprobe/milchstrasse_leseprobe.html</location><date>2011-02-02</date><user>Treasurer</user><original_encoding>utf-8</original_encoding><language>deu</language><issue>none</issue></source>
Artur Becker machte mit seiner Lesung aus dem Roman »Onkel Jimmy, die Indianer und ich« in Klagenfurt zwar keinen Preis, gewann dafür aber beim 
Publikum, bei Rezensenten und Lesern Komplizen fürs Leben.
</verbatim>

---++ Same data after sentence splitting
<verbatim>
http://www.arturbecker.de/Prosa/Milchstrasse/Milchstrasse_Leseprobe/milchstrasse_leseprobe.html <source><location><![CDATA[http://www.arturbecker.de/Prosa/Milchstrasse/Milchstrasse_Leseprobe/milchstrasse_leseprobe.html]]></location><date>2011-02-02</date><user>Treasurer</user><original_encoding>utf-8</original_encoding><language>deu</language><issue>none</issue></source> <process><length>5934</length><is_duplicate>false</is_duplicate><is_duplicate>false</is_duplicate><encoding_error>false</encoding_error></process>  <p><s>Artur Becker machte mit seiner Lesung aus dem Roman »Onkel Jimmy, die Indianer und ich« in Klagenfurt zwar keinen Preis, gewann dafür aber beim Publikum, bei Rezensenten und Lesern Komplizen fürs Leben.</s></p>
</verbatim>



%META:FILEATTACHMENT{name="main.png" attachment="main.png" attr="" comment="Hadoop flow" date="1344764603" path="main.png" size="240970" user="LeonardSwiezinski" version="2"}%
%META:FILEATTACHMENT{name="main.gif" attachment="main.gif" attr="" comment="" date="1344764654" path="main.gif" size="106520" user="LeonardSwiezinski" version="2"}%
%META:TOPICMOVED{by="NicoErbs" date="1337256032" from="Main.UKPWebCorpus" to="Hiwi.UKPWebCorpus"}%

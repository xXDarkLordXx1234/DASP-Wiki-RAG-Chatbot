%META:TOPICINFO{author="simpson" comment="reprev" date="1480529578" format="1.1" reprev="1" version="1"}%
%META:TOPICPARENT{name="WebHome"}%
---++ Project Description

See attached document:
   * [[%ATTACHURL%/MelvinLauxTaskDescription.odt][MelvinLauxTaskDescription.odt]]

Supervisor: Edwin Simpson.

---++ Time Tracking

---+++ 2016-11

%EDITTABLE{  sort="on" tableborder="0" cellpadding="1" cellspacing="3" headerrows="2"  footerrows = "1" }%
| *Date* | *Task* | *Hours* | *Notes* |

---+++ 2016-10

%EDITTABLE{  sort="on" tableborder="0" cellpadding="1" cellspacing="3" headerrows="2"  footerrows = "1" }%
| *Date* | *Task* | *Hours* | *Notes* |

---+++ 2016-09

%EDITTABLE{  sort="on" tableborder="0" cellpadding="1" cellspacing="3" headerrows="2"  footerrows = "1" }%
| *Date* | *Task* | *Hours* | *Notes* |

---++ Melvin: things to do from meeting on 27.09.2016

   * Reading up on baselines and state-of-the-art rival methods including those described in confused supervised LDA paper

   * Are there any baselines we have not considered so far?

   * Which method performed best (we should compare against the state-of-the-art)?

   * Is the confused supervised LDA method itself worth implementing and comparing? It may be a complex method, so do the theory and results seem convincing?

   * Implementing alternative baselines:

   * Augment the worker accuracy method to also do bias correction, similar to the approach described in other literature

   * Enable majority vote to work with different thresholds when making a binary choice between outside and not outside (I or B). At the moment, when the crowd is split 50/50, we have an arbitrary choice  one way or another. In some NLP papers, I believe people have used higher thresholds, i.e. only included annotations with > 50% agreement. So if we allow this to be a parameter of the majority voting method, we can test several cases in the experiments. 

   * Invent a simple way to configure or to visualise the \alpha confusion matrix hyper-parameters. This should make it easy to adjust the type of workers generated and the priors of the model when we do inference. Could be done by

   * Setting a small number of parameters manually, such as accuracy (adding values to alpha entries for the correct labels), bias toward missing annotations (adding bias toward O when ground truth is I or B), and preference for long vs. short annotations (adding bias toward B when true label is I). Then, the method fills in the complete matrix automatically. Users can then tweak the resulting matrix if they want to make more detailed changes.

   * Visualisation of the 3-D matrix, e.g. as three 2-D plots, one for each value of the previous annotation

   * Plot ROC curves as an evaluation step

   * Allow the method to be run multiple times, varying the accuracy or bias of the workers

   * Within each iteration, we generate new workers with a specified accuracy or bias

   * At each iteration we increase accuracy or bias

   * When all iterations are complete, we plot the performance metrics for the different methods against the worker accuracy or bias. The idea is to show how each method deals with particularly noisy or biased workers; e.g. at what point does majority voting fail, but another method still work?

   * Can also try with a mixture of good and bad workers, and see what happens when the proportion of bad workers increases

   * Look at any publicly-available real datasets that are mentioned in the papers. If we can do a straight comparison of the results, we may not need to implement so many baselines. 

-- Main.EdwinSimpson - 2016-11-30


%META:FILEATTACHMENT{name="MelvinLauxTaskDescription.odt" attachment="MelvinLauxTaskDescription.odt" attr="" comment="" date="1480529313" path="MelvinLauxTaskDescription.odt" size="25338" user="simpson" version="1"}%

%META:TOPICINFO{author="laux" comment="save topic" date="1482315077" format="1.1" reprev="3" version="3"}%
%META:TOPICPARENT{name="WebHome"}%
---++ Project Description

See attached document:
   * [[%ATTACHURL%/MelvinLauxTaskDescription.odt][MelvinLauxTaskDescription.odt]]

Supervisor: Edwin Simpson.

---++ Time Tracking

---+++ 2016-12

%EDITTABLE{  sort="on" tableborder="0" cellpadding="1" cellspacing="3" headerrows="2"  footerrows = "1" }%
| *Date* | *Task* | *Hours* | *Notes* |

---+++ 2016-11

%EDITTABLE{  sort="on" tableborder="0" cellpadding="1" cellspacing="3" headerrows="2"  footerrows = "1" }%
| *Date* | *Task* | *Hours* | *Notes* |
| 01.11.2016 | dev | 6 | clustering baseline |
| 02.11.2016 | meeting/orga | 2 | |
| 05.11.2016 | dev | 3 | experiment framework |
| 07.11.2016 | orga | 1 | |
| 08.11.2016 | dev | 6 | clustering baseline |
| 09.11.2016 | meeting/orga | 2 | |
| 13.11.2016 | dev | 1 | experient framework |
| 15.11.2016 | dev | 3 | mace baseline |
| 16.11.2016 | meeting/dev | 3 | experiment framework |
| 23.11.2016 | dev | 5 | ibcc baseline |
| 24.11.2016 | meeting/orga | 2 | |
| 27.11.2016 | dev/orga | 2 | ibcc static functions |
| 29.11.2016 | dev | 3 | ibcc test cases |
| 30.11.2016 | meeting/orga | 2 | |
| 01.12.2016 | orga | 2.5 | transfer work hours |

---+++ 2016-10

%EDITTABLE{  sort="on" tableborder="0" cellpadding="1" cellspacing="3" headerrows="2"  footerrows = "1" }%
| *Date* | *Task* | *Hours* | *Notes* |
| 01.10.2016 | reading | 2 | various clustering methods |
| 02.10.2016 | orga | 2 | |
| 03.10.2016 | dev | 6 | experiment framework |
| 04.10.2016 | dev | 3 | clustering baseline |
| 05.10.2016 | dev | 4 | add more plots and metrics to experiment framwork |
| 06.10.2016 | meeting/orga | 5 | |
| 07.10.2016 | dev | 2 | experiment framework |
| 08.10.2016 | orga | 2 | |
| 09.10.2016 | reading | 2 | clustering methods, evaluation metrics |
| 18.10.2016 | dev/orga | 5 | experiment framework, finally move code into gitlab |
| 19.10.2016 | meeting/dev | 3 | |
| 23.10.2016 | dev | 4.5 | experiment framework extension |
| 25.10.2016 | dev | 6 | experiment framework extension and refactoring |
| 26.10.2016 | meeting | 0.5 | |
| 27.10.2016 | dev | 3 | experiment framwork |
| 29.10.2016 | dev | 3 | clustering baseline |
| 30.10.2016 | dev | 3 | experiment framwork |
| 31.10.2016 | dev | 2 | clustering baseline |

---+++ 2016-09

%EDITTABLE{  sort="on" tableborder="0" cellpadding="1" cellspacing="3" headerrows="2"  footerrows = "1" }%
| *Date* | *Task* | *Hours* | *Notes* |
| 28.08.2016 | reading | 5 | reading up on the project (Edwins thesis) |
| 30.08.2016 | meeting | 2 | planning, question clarification, contract formalities |
| 01.09.2016 | reading | 3 | familiarise with ibcc code |
| 03.09.2016 | orga | 3 | set up python environment and required libraries |
| 06.09.2016 | meeting | 1.5 | contract formalities |
| 11.09.2016 | orga | 1 | set google sheets for time tracking |
| 12.09.2016 | dev | 3 | set up project, start with synth data generator |
| 13.09.2016 | dev/meeting | 3 | finish synth data generator, discuss next steps |
| 16.09.2016 | dev | 2 | add dirichlet sampling to data generator |
| 17.09.2016 | dev | 2 | synth data data format update |
| 18.09.2016 | dev | 2 | majority voting baseline |
| 19.09.2016 | dev | 6 | majority voting baseline |
| 20.09.2016 | meeting/dev | 4 | majority voting baseline, next steps |
| 21.09.2016 | orga | 2 | |
| 22.09.2016 | dev | 2.5 | bugfixing |
| 23.09.2016 | reading | 3 | researching alternative baselines |
| 24.09.2016 | dev | 3 | improve data generator by using config file |
| 25.09.2016 | dev | 4 | add variations to majority baselines (thresholds, etc.)) |
| 26.09.2016 | dev | 5 | set up class for systematic experiment |
| 27.09.2016 | meeting/dev | 5 | split config file into data config and experiment config |
| 28.09.2016 | dev | 6 | big code overhaul and refactoring |
| 29.09.2016 | reading | 4 | confused lda |
| 30.09.2016 | dev | 3 | add plots and evaluation to experiment framework |

---++ Melvin: things to do from meeting on 27.09.2016

   * Reading up on baselines and state-of-the-art rival methods including those described in confused supervised LDA paper

   * Are there any baselines we have not considered so far?

   * Which method performed best (we should compare against the state-of-the-art)?

   * Is the confused supervised LDA method itself worth implementing and comparing? It may be a complex method, so do the theory and results seem convincing?

   * Implementing alternative baselines:

   * Augment the worker accuracy method to also do bias correction, similar to the approach described in other literature

   * Enable majority vote to work with different thresholds when making a binary choice between outside and not outside (I or B). At the moment, when the crowd is split 50/50, we have an arbitrary choice  one way or another. In some NLP papers, I believe people have used higher thresholds, i.e. only included annotations with > 50% agreement. So if we allow this to be a parameter of the majority voting method, we can test several cases in the experiments. 

   * Invent a simple way to configure or to visualise the \alpha confusion matrix hyper-parameters. This should make it easy to adjust the type of workers generated and the priors of the model when we do inference. Could be done by

   * Setting a small number of parameters manually, such as accuracy (adding values to alpha entries for the correct labels), bias toward missing annotations (adding bias toward O when ground truth is I or B), and preference for long vs. short annotations (adding bias toward B when true label is I). Then, the method fills in the complete matrix automatically. Users can then tweak the resulting matrix if they want to make more detailed changes.

   * Visualisation of the 3-D matrix, e.g. as three 2-D plots, one for each value of the previous annotation

   * Plot ROC curves as an evaluation step

   * Allow the method to be run multiple times, varying the accuracy or bias of the workers

   * Within each iteration, we generate new workers with a specified accuracy or bias

   * At each iteration we increase accuracy or bias

   * When all iterations are complete, we plot the performance metrics for the different methods against the worker accuracy or bias. The idea is to show how each method deals with particularly noisy or biased workers; e.g. at what point does majority voting fail, but another method still work?

   * Can also try with a mixture of good and bad workers, and see what happens when the proportion of bad workers increases

   * Look at any publicly-available real datasets that are mentioned in the papers. If we can do a straight comparison of the results, we may not need to implement so many baselines. 

-- Main.EdwinSimpson - 2016-11-30


%META:FILEATTACHMENT{name="MelvinLauxTaskDescription.odt" attachment="MelvinLauxTaskDescription.odt" attr="" comment="" date="1480529313" path="MelvinLauxTaskDescription.odt" size="25338" user="simpson" version="1"}%

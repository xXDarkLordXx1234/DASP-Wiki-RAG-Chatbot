%META:TOPICINFO{author="ChristianKirschner" date="1318002104" format="1.1" reprev="35" version="35"}%
%META:TOPICPARENT{name="WebHome"}%
<!--
   * Set ALLOWTOPICVIEW = Main.ChristianKirschner,Main.UkpGroup
   * Set ALLOWTOPICCHANGE = Main.ChristianKirschner,Main.UkpGroup
//-->

---+ Christian Kirschner's Work

---++ Project Plan (tentative)

   * *<span class="green">Part 1</span>*
      * <span class="green">Read task description (CICLing sec. 4; IGI sec. rel. anchoring)</span>
      * <span class="green">Read annotation guidebook</span>
      * <span class="green">Familiarize with task</span>
   * Part 2
      * <span class="green">Look into Wiktionary; structure and API access</span>
      * <span class="green">Which dimensions are necessary/important to consider for a dataset?
         * This will be a core question to be discussed</span>
      * <span class="green">Include Wikisaurus?</span>
      * <span class="green">Develop a well-balanced reference dataset for the English relation anchoring task (maybe also for the German?) [with !ChM]</span>
      * Update the annotation guidebook
   * Part 3
      * Annotate the data [@ChM find additional annotator(s)]
      * Evaluate the dataset [with !ChM]
   * Part 4
      * <span class="green">@ChM Update the anchoring framework</span>
      * <span class="green">Get the framework to work, familiarize with code</span>
      * Run the existing methods with old and new data
      * Analyze errors
         * What do we need to solve?
         * What can be improved?
            * This will be a core question to be discussed
   * Part 5
      * Try and develop heuristics (come up with fancy names for them)
         * existing return relation
         * target gloss contains source word
         * matching domain (@ChM: further think about that)
      * Try and develop new methods, ideas:
         * PPR?
         * Graph-based method (Navid?)
         * PROX?
         * Milne/Witten?
   * Part 6
      * @ChM: get cross-lingual dataset ready
      * Select translation API and translate glosses
      * Experiments on cross-lingual dataset

---++ GoldStandard Generation

   * On the one hand the GoldStandard should build up in a way that weaknesses of disambiguation algorithms can be detected, on the other hand it should be quite representative. Therefore we are going to use the following criteria:
      * Part of speech
      * Relations
      * Number of candidates
      * Labels

---++ Heuristics

   1 Wenn ein Sense eine Relation (z.B. Hyperonym) auf einen anderen Artikel hat und einer der Kandidaten wiederum eine Relation (z.B. Hyponym) zurück zum Ausgangsartikel besitzt, so ist dieser Kandidat höchstwahrscheinlich positiv zu annotieren. Dies funktioniert mit Synonym-Synonym, Antonym-Antonym, Hyperonym-Hyponym und Hyponym-Hyperonym Relationspaaren. Die Heuristik ist bei 11,5% der Paare anwendbar und hat eine Precision von 96,4% (deutscher Datensatz).
   2 Wenn eine Instanz nur einen möglichen Kandidaten hat, wird dieser positiv annotiert. Die Heuristik ist bei 21,6% der Paare anwendbar und hat eine Precision von 96,6% (deutscher Datensatz). 
   3 Wenn ein Kandidat im Gloss das Lemma der Instanz enthält, wird das Paar positiv annotiert. Die Heuristik ist bei 1,5% der Paare anwendbar und hat eine Precision von 94,4% (deutscher Datensatz).

 * Heuristik 1: <br />
     <img src="%ATTACHURLPATH%/heuristik1.png" alt="heuristik1.png" width="400" height="350" />

---++ Results

---+++ Experimente auf dem deutschen Datensatz:

Lesk (set of words), normalisiert auf Werte zwischen 0 und 1, mit Heuristiken 1-3, mit trainiertem Schwellenwert 0.05: 

| *Accuracy* | *Precision* | *Recall* | *F-Measure* | *Gruppe* |
| 0,76923 ( 940/1222) | 0,84735 ( 383/ 452) | 0,64262 ( 383/ 596) | 0,73092 | ALL | 
| 0,86207 (  25/  29) | 0,89286 (  25/  28) | 0,96154 (  25/  26) | 0,92593 | N-1-HAS_SYN | 
| 0,96429 (  27/  28) | 0,96429 (  27/  28) | 1,00000 (  27/  27) | 0,98182 | N-1-HAS_ANT | 
| 1,00000 (  28/  28) | 1,00000 (  28/  28) | 1,00000 (  28/  28) | 1,00000 | N-1-HAS_HYPO/HAS_HYPER | 
| 0,68000 (  51/  75) | 0,83333 (  15/  18) | 0,41667 (  15/  36) | 0,55556 | N-2-HAS_SYN | 
| 0,83962 (  89/ 106) | 0,95652 (  22/  23) | 0,57895 (  22/  38) | 0,72131 | N-2-HAS_ANT | 
| 0,66957 (  77/ 115) | 0,53846 (   7/  13) | 0,17949 (   7/  39) | 0,26923 | N-2-HAS_HYPO/HAS_HYPER | 
| 1,00000 (  28/  28) | 1,00000 (  28/  28) | 1,00000 (  28/  28) | 1,00000 | V-1-HAS_SYN | 
| 0,90323 (  28/  31) | 0,90323 (  28/  31) | 1,00000 (  28/  28) | 0,94915 | V-1-HAS_ANT | 
| 0,96875 (  31/  32) | 0,96875 (  31/  32) | 1,00000 (  31/  31) | 0,98413 | V-1-HAS_HYPO/HAS_HYPER | 
| 0,79808 (  83/ 104) | 0,69565 (  16/  23) | 0,53333 (  16/  30) | 0,60377 | V-2-HAS_SYN | 
| 0,72603 ( 106/ 146) | 0,88235 (  15/  17) | 0,28302 (  15/  53) | 0,42857 | V-2-HAS_ANT | 
| 0,70139 ( 101/ 144) | 0,40000 (  10/  25) | 0,26316 (  10/  38) | 0,31746 | V-2-HAS_HYPO/HAS_HYPER | 
| 1,00000 (  15/  15) | 1,00000 (  15/  15) | 1,00000 (  15/  15) | 1,00000 | A-1-HAS_SYN | 
| 1,00000 (  14/  14) | 1,00000 (  14/  14) | 1,00000 (  14/  14) | 1,00000 | A-1-HAS_ANT | 
| 1,00000 (  14/  14) | 1,00000 (  14/  14) | 1,00000 (  14/  14) | 1,00000 | A-1-HAS_HYPO/HAS_HYPER | 
| 0,60000 (  33/  55) | 0,79167 (  19/  24) | 0,52778 (  19/  36) | 0,63333 | A-2-HAS_SYN | 
| 0,67647 (  23/  34) | 0,80000 (   8/  10) | 0,47059 (   8/  17) | 0,59259 | A-2-HAS_ANT | 
| 0,67442 (  29/  43) | 0,40000 (   6/  15) | 0,54545 (   6/  11) | 0,46154 | A-2-HAS_HYPO/HAS_HYPER | 
| 1,00000 (  14/  14) | 1,00000 (  14/  14) | 1,00000 (  14/  14) | 1,00000 | R-1-HAS_SYN | 
| 1,00000 (  15/  15) | 1,00000 (  15/  15) | 1,00000 (  15/  15) | 1,00000 | R-1-HAS_ANT | 
| 1,00000 (  14/  14) | 1,00000 (  14/  14) | 1,00000 (  14/  14) | 1,00000 | R-1-HAS_HYPO/HAS_HYPER | 
| 0,60526 (  23/  38) | 0,75000 (   6/   8) | 0,31579 (   6/  19) | 0,44444 | R-2-HAS_SYN | 
| 0,77083 (  37/  48) | 0,50000 (   3/   6) | 0,27273 (   3/  11) | 0,35294 | R-2-HAS_ANT | 
| 0,67308 (  35/  52) | 0,33333 (   3/   9) | 0,21429 (   3/  14) | 0,26087 | R-2-HAS_HYPO/HAS_HYPER | 


---+++ Experimente auf dem englischen Datensatz:


---++ Current Schedule

%EDITTABLE{  sort="on" tableborder="0" cellpadding="1" cellspacing="3" headerrows="1"  footerrows = "1" }%
| *Task* | *Date* | *Hours* | *Remarks* |
| - | 28.03.2011 | 0 | Project Plan (!ChM) |
| Part 1 + Meeting | 11.04.2011 | 1 | |
| Datenbank aufsetzen + Workspace anlegen | 11.04.2011 | 0.75 | |
| In Code einlesen | 12.04.2011 | 0.75 | |
| Software testen, Abläufe bei Datenerzeugung nachvollziehen, Wiktionary genauer ansehen | 15.04.2011 | 2.5 | |
| Daten in Datenbank analysieren, Gedanken über Erstellung eines GoldStandards machen | 24.04.2011 | 2.5 | |
| Random und MFS Evaluationsdaten für WordNet-Wiktionary Alignment erstellen | 28.04.2011 | 1.5 | |
| Meeting | 29.04.2011 | 0.5 | |
| Erstellen einer Statistik über Verteilung von POS, Kandidaten, Relationen in englischer Wiktionary und Bewertung der Daten | 01.05.2011 | 2 | |
| Programmaufbau überlegen, mit Umsetzung beginnen | 03.05.2011 | 3 | |
| Statistik über Labels erstellen, analysieren, Programmieren: Code um Samples aus DB zu extrahieren soweit fertig, Textfile wird erstellt, Einbinden in vorgegebenen Code, Erstellung eines ersten (unaligned) GoldStandards, Analyse der Daten und Feststellung von Problemen | 04.05.2011 | 3.5 | |
| Automatische Erstellung einer Statistik über erstellten GoldStandard programmieren | 07.05.2011 | 1.5 | |
| Weitere Datenbank (en_ws) aufsetzen, de und en_ws Datenbanken ansehen und integrieren, Statistik erweitern | 14.05.2011 | 2.5 | |
| Neue Daten importiert, Datensätze generiert | 31.05.2011 | 1 | |
| Deutschen GoldStandard annotiert (Teil 1) | 07.06.2011 | 1.5 | |
| Deutschen GoldStandard annotiert (Teil 2) | 16.06.2011 | 3.5 | |
| Englischen GoldStandard annotiert (Teil 1) | 26.06.2011 | 1.5 | |
| Englischen GoldStandard annotiert (Teil 2) | 27.06.2011 | 3.5 | |
| Englischen GoldStandard annotiert (Teil 3) | 03.08.2011 | 2 | |
| Meeting | 23.08.2011 | 0.5 | |
| Code auschecken und einlesen | 04.09.2011 | 2 | |
| Codeanalyse: Überprüfung der vorhandenen Evaluationsmethode und Feststellung notwendiger Änderungen | 12.09.2011 | 2 | |
| Evaluation und Disambiguierung korrigieren und an unsere Bedürfnisse anpassen: Mehrere positive Annotation pro Instanz, Disambiguierung auf Basis eines Thresholds, Evaluation entsprechend Paaren (nicht Instanzen) | 13.09.2011 | 2,5 | |
| Methoden (z.B. Lesk) testen, Schwachstellen identifizieren | 13.09.2011 | 1,5 | |
| Über Heuristiken nachdenken, eine erste Heuristik implementieren, Nutzen der Heuristik evaluieren | 15.09.2011 | 4 | |
| TreeTagger & StopWords installiert | 18.09.2011 | 0.5 | |
| Heuristiken 2 und 3 implementiert und getestet, TreeTagger weiter eingebunden und getestet | 19.09.2011 | 2 | |
| Schweren Bug bei Kandidatenextraktion gefunden und behoben, Evaluationsdaten für Heuristiken korrigiert | 19.09.2011 | 1 | |
| Schwellenwerttraining + Visualisierung der Ergebnisse implementiert | 05.10.2011 | 1.5 | |
| Schwellenwerttraining besser eingebunden, RelatednessMethod testen, Instanzen mit Folds versehen (für spätere CrossValidation) | 07.10.2011 | 3.5 | |
| *Summe* | | %CALC{"$SUM( $ABOVE() )"}% | |
| *Soll* | | 90 | bis Ende Dezember 2011 |
<nop>

-- Main.ChristianMeyer - 2011-03-28
  

%META:FILEATTACHMENT{name="heuristik1.png" attachment="heuristik1.png" attr="h" comment="Heuristik 1" date="1316115427" path="heuristik1.png" size="36857" user="ChristianKirschner" version="1"}%

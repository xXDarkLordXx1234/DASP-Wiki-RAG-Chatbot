%META:TOPICINFO{author="ChristianKirschner" date="1332338381" format="1.1" reprev="65" version="65"}%
%META:TOPICPARENT{name="WebHome"}%
<!--
   * Set ALLOWTOPICVIEW = Main.ChristianKirschner,Main.UkpGroup
   * Set ALLOWTOPICCHANGE = Main.ChristianKirschner,Main.UkpGroup
//-->

---+ Christian Kirschner's Work

---++ Project Plan (tentative)

   * *<span class="green">Part 1</span>*
      * <span class="green">Read task description (CICLing sec. 4; IGI sec. rel. anchoring)</span>
      * <span class="green">Read annotation guidebook</span>
      * <span class="green">Familiarize with task</span>
   * Part 2
      * <span class="green">Look into Wiktionary; structure and API access</span>
      * <span class="green">Which dimensions are necessary/important to consider for a dataset?
         * This will be a core question to be discussed</span>
      * <span class="green">Include Wikisaurus?</span>
      * <span class="green">Develop a well-balanced reference dataset for the English relation anchoring task (maybe also for the German?) [with !ChM]</span>
      * Update the annotation guidebook
   * Part 3
      * Annotate the data [@ChM find additional annotator(s)]
      * Evaluate the dataset [with !ChM]
   * Part 4
      * <span class="green">@ChM Update the anchoring framework</span>
      * <span class="green">Get the framework to work, familiarize with code</span>
      * Run the existing methods with old and new data
      * Analyze errors
         * What do we need to solve?
         * What can be improved?
            * This will be a core question to be discussed
   * Part 5
      * Try and develop heuristics (come up with fancy names for them)
         * existing return relation
         * target gloss contains source word
         * matching domain (@ChM: further think about that)
      * Try and develop new methods, ideas:
         * PPR?
         * Graph-based method (Navid?)
         * PROX?
         * Milne/Witten?
   * Part 6
      * @ChM: get cross-lingual dataset ready
      * Select translation API and translate glosses
      * Experiments on cross-lingual dataset

---++ GoldStandard Generation

   * On the one hand the GoldStandard should build up in a way that weaknesses of disambiguation algorithms can be detected, on the other hand it should be quite representative. Therefore we are going to use the following criteria:
      * Part of speech
      * Relations
      * Number of candidates
      * Labels

---++ Heuristics

   1 Wenn ein Sense eine Relation (z.B. Hyperonym) auf einen anderen Artikel hat und einer der Kandidaten wiederum eine Relation (z.B. Hyponym) zurück zum Ausgangsartikel besitzt, so ist dieser Kandidat höchstwahrscheinlich positiv zu annotieren. Dies funktioniert mit Synonym-Synonym, Antonym-Antonym, Hyperonym-Hyponym und Hyponym-Hyperonym Relationspaaren. Auf dem deutschen Datensatz ist die Heuristik bei 11,3% der Paare anwendbar und hat eine Precision von 88,4%. Auf dem englischen Datensatz ist die Heuristik bei 3,3% der Paare anwendbar und hat eine Precision von 85,7%. 
   2 Wenn eine Instanz nur einen möglichen Kandidaten hat, wird dieser positiv annotiert. Auf dem deutschen Datensatz ist die Heuristik bei 21,6% der Paare anwendbar und hat eine Precision von 95,1%. Auf dem englischen Datensatz ist die Heuristik bei 12,8% der Paare anwendbar und hat eine Precision von 98,6%. 
   3 Wenn ein Kandidat im Gloss das Lemma der Instanz enthält, wird das Paar positiv annotiert. Auf dem deutschen Datensatz ist die Heuristik bei 1,5% der Paare anwendbar und hat eine Precision von 88,9%. Auf dem englischen Datensatz ist die Heuristik bei 4,6% der Paare anwendbar und hat eine Precision von 81,0%. 

 * Heuristik 1: <br />
     <img src="%ATTACHURLPATH%/heuristik1.png" alt="heuristik1.png" width="200" height="175" />

---++ Boni

   1 Der Score des ersten Kandidaten jeder Instanz erhält einen Bonus indem der berechnete Score verdoppelt wird (+ einer kleinen Konstante 0.01, damit er nicht bei 0 verbleibt). Die FirstCandidateBaseline hat alleine eine Precision von 78,6% (deutscher Datensatz) bzw. 77,53% (englischer Datensatz).

---++ Results
Im folgenden werden die wichtigsten Ergebnisse dargestellt. Alle Werte im Detail sind im Topic [[https://maggie.tk.informatik.tu-darmstadt.de/wiki/bin/view/Hiwi/Experiments][Experiments]] zu finden.

---+++ Übersicht der Ergebnisse auf dem deutschen Datensatz:
| ** | *Accuracy* | *Precision* | *Recall* | *F-Measure* | *Threshold* |
| *PureLesk* | 0,64730 ( 791/1222) | 0,75096 ( 196/ 261) | 0,34875 ( 196/ 562) | 0,47631 | 0.0001 |
| *Random/PureLesk* | 0,45990 ( 562/1222) | 0,45990 ( 562/1222) | 1,00000 ( 562/ 562) | 0,63004 | 0.0 |
| *FirstCand* | 0,79133 ( 967/1222) | 0,78585 ( 422/ 537) | 0,75089 ( 422/ 562) | 0,76797 | - |
| *H123* | 0,76187 ( 931/1222) | 0,91437 ( 299/ 327) | 0,53203 ( 299/ 562) | 0,67267 | - |
| *LeskH123* | 0,76923 ( 940/1222) | 0,80973 ( 366/ 452) | 0,65125 ( 366/ 562) | 0,72189 | 0.005 |
| *LeskB1* | 0,78478 ( 959/1222) | 0,73618 ( 466/ 633) | 0,82918 ( 466/ 562) | 0,77992 | 0.005 |
| *LeskH23B1* |  0,78887 ( 964/1222) | 0,73750 ( 472/ 640) | 0,83986 ( 472/ 562) | *0,78536* | 0.005 |
| *LeskH123B1* | 0,78560 ( 960/1222) | 0,73006 ( 476/ 652) | 0,84698 ( 476/ 562) | 0,78418 | 0.005 |
| *CosH123B1* | 0,78396 ( 958/1222) | 0,72923 ( 474/ 650) | 0,84342 ( 474/ 562) | 0,78218 | 0.005 |

Am besten schneidet die Lesk-Methode in Verbindung mit den Heuristiken 2, 3 und Bonus 1 ab. Heuristik 1 verschlechtert das Ergebnis hier geringfügig, aber nicht signifikant. Die Lesk Methode alleine erreicht einen sehr schlechten F-Measure Wert, allerdings eine akzeptable Precision, sodass die Methode in Verbindung mit den Heuristiken sinnvoll ist. 

*Betrachtung der einzelnen Features (LeskH23B1):*
| ** | *Accuracy* | *Precision* | *Recall* | *F-Measure* |
| *N* | 0,80840 ( 308/ 381) | 0,79293 ( 157/ 198) | 0,83069 ( 157/ 189) | 0,81137 |
| *V* | 0,79588 ( 386/ 485) | 0,70563 ( 163/ 231) | 0,84021 ( 163/ 194) | 0,76706 |
| *A* | 0,74286 ( 130/ 175) | 0,70940 (  83/ 117) | 0,88298 (  83/  94) | 0,78673 |
| *R* | 0,77348 ( 140/ 181) | 0,73404 (  69/  94) | 0,81176 (  69/  85) | 0,77095 |
| *1* | 0,95038 ( 249/ 262) | 0,95402 ( 249/ 261) | 0,99600 ( 249/ 250) | 0,97456 |
| *2* | 0,74479 ( 715/ 960) | 0,58839 ( 223/ 379) | 0,71474 ( 223/ 312) | 0,64544 |
| *HAS_SYN* | 0,74302 ( 266/ 358) | 0,71560 ( 156/ 218) | 0,83871 ( 156/ 186) | 0,77228 |
| *HAS_ANT* | 0,79621 ( 336/ 422) | 0,75622 ( 152/ 201) | 0,80423 ( 152/ 189) | 0,77949 |
| *HAS_HYPO/HAS_HYPER* | 0,81900 ( 362/ 442) | 0,74208 ( 164/ 221) | 0,87701 ( 164/ 187) | 0,80392 |

Bei Instanzen mit nur einem Kandidaten ist kaum noch Verbesserungspotenzial vorhanden (diese werden durch Heuristik 2 grundsätzlich positiv annotiert). Deutliche Schwächen sind allerdings bei den Instanzen mit 2 oder mehr Kandidaten zu erkennen. Insbesondere die Precision ist hier verbesserungswürdig. Eventuell ist eine Begrenzung der Anzahl an Zuweisungen pro Instanz zielführend.

*Ergebnisse mit Begrenzung der Anzahl an Zuweisungen pro Instanz auf die Kandidaten mit den höchsten Scores oberhalb des Schwellenwerts:*
| ** | *Accuracy* | *Precision* | *Recall* | *F-Measure* | *Threshold* |
| *Random* | 0,67840 ( 829/1222) | 0,65736 ( 353/ 537) | 0,62811 ( 353/ 562) | 0,64240 | 0,0 |
| *PureLesk* | 0,62520 ( 764/1222) | 0,55640 ( 513/ 922) | 0,91281 ( 513/ 562) | 0,69137 | 0,0 |
| *FirstCand* | 0,79133 ( 967/1222) | 0,78585 ( 422/ 537) | 0,75089 ( 422/ 562) | 0,76797 | - |
| *H123* | 0,76187 ( 931/1222) | 0,91437 ( 299/ 327) | 0,53203 ( 299/ 562) | 0,67267 | - |
| *LeskH123* | 0,77823 ( 951/1222) | 0,85926 ( 348/ 405) | 0,61922 ( 348/ 562) | 0,71975 | 0,005 |
| *LeskB1*| 0,80933 ( 989/1222) | 0,80633 ( 433/ 537) | 0,77046 ( 433/ 562) | 0,78799 | 0,0 |
| *LeskH23B1* | 0,81178 ( 992/1222) | 0,80855 ( 435/ 538) | 0,77402 ( 435/ 562) | 0,79091 | 0,0 |
| *LeskH123B1* | 0,81342 ( 994/1222) | 0,80699 ( 439/ 544) | 0,78114 ( 439/ 562) | *0,79385* | 0,0 |
| *CosH123B1* | 0,81015 ( 990/1222) | 0,80331 ( 437/ 544) | 0,77758 ( 437/ 562) | 0,79024 | 0,0 |

Die Begrenzung der Zuweisungen bringt in der besten Konfiguration (LeskH123B1) eine deutliche Verbesserung von 78,5% auf 79,4%. Die Betrachtung der einzelnen Features ergibt folgendes Bild:
| ** | *Accuracy* | *Precision* | *Recall* | *F-Measure* |
| *N* | 0,79790 ( 304/ 381) | 0,82558 ( 142/ 172) | 0,75132 ( 142/ 189) | 0,78670 |
| *V* | 0,84330 ( 409/ 485) | 0,80729 ( 155/ 192) | 0,79897 ( 155/ 194) | 0,80311 |
| *A* | 0,80000 ( 140/ 175) | 0,80412 (  78/  97) | 0,82979 (  78/  94) | 0,81675 |
| *R* | 0,77901 ( 141/ 181) | 0,77108 (  64/  83) | 0,75294 (  64/  85) | 0,76190 |
| *1* | 0,95038 ( 249/ 262) | 0,95402 ( 249/ 261) | 0,99600 ( 249/ 250) | 0,97456 |
| *2* | 0,77604 ( 745/ 960) | 0,67138 ( 190/ 283) | 0,60897 ( 190/ 312) | 0,63866 |
| *HAS_SYN* | 0,77095 ( 276/ 358) | 0,78571 ( 143/ 182) | 0,76882 ( 143/ 186) | 0,77717 |
| *HAS_ANT* | 0,81517 ( 344/ 422) | 0,81006 ( 145/ 179) | 0,76720 ( 145/ 189) | 0,78804 |
| *HAS_HYPO/HAS_HYPER* | 0,84615 ( 374/ 442) | 0,82514 ( 151/ 183) | 0,80749 ( 151/ 187) | 0,81622 |

Obwohl mit der Begrenzung der Assignments insgesamt eine Verbesserung der Ergebnisse erreicht wurde, ist der F-Measure Wert bei Instanzen mit 2 oder mehr Zuweisungen nach wie vor verbesserungsfähig. Zwar konnte hier die Precision um knapp 10% gesteigert werden, allerdings ist der Recall gleichzeitig um einen ähnlichen Wert gesunken. Während in der vorherigen Konfiguration also Instanzen mit mehreren Kandidaten zu viele Zuweisungen vorgenommen haben, werden nun zu wenige Kandidaten zugewiesen. Der Versuch die Anzahl der Zuweisungen auf die zwei höchsten Scores zu beschränken brachte keine signifikante Verbesserung. Auffällig ist zudem, dass sich nun nicht mehr Nomen, sondern Adjektive am besten klassifizieren lassen. 

---+++ Übersicht der Ergebnisse auf dem englischen Datensatz:
| ** | *Accuracy* | *Precision* | *Recall* | *F-Measure* | *Threshold* |
| *Random* | 0,28277 ( 481/1701) | 0,28277 ( 481/1701) | 1,00000 ( 481/ 481) | 0,44088 | 0,0 |
| *PureLesk* | 0,76073 (1294/1701) | 0,58259 ( 261/ 448) | 0,54262 ( 261/ 481) | 0,56189 | 0,005 |
| *FirstCand* | 0,83892 (1427/1701) | 0,73258 ( 326/ 445) | 0,67775 ( 326/ 481) | 0,70410 | - |
| *H123* | 0,84303 (1434/1701) | 0,85667 ( 257/ 300) | 0,53430 ( 257/ 481) | 0,65813 | - |
| *LeskH123* | 0,84950 (1445/1701) | 0,77641 ( 316/ 407) | 0,65696 ( 316/ 481) | 0,71171 | 0,17 |
| *LeskB1* | 0,78836 (1341/1701) | 0,58806 ( 404/ 687) | 0,83992 ( 404/ 481) | 0,69178 | 0,005 |
| *LeskH123B1* | 0,86243 (1467/1701) | 0,79475 ( 333/ 419) | 0,69231 ( 333/ 481) | *0,74000* | 0,2 |
| *CosH123B1* | 0,84832 (1443/1701) | 0,74505 ( 339/ 455) | 0,70478 ( 339/ 481) | 0,72436 | 0,045 |



Am besten schneidet die Lesk-Methode in Verbindung mit den Heuristiken 1-3 und dem Bonus 1 ab. Die Lesk Methode alleine führt zu sehr schlechten Ergebnissen, auch die Precision ist hier nicht sonderlich gut. 

*Betrachtung der einzelnen Features (LeskH123B1):*
| ** | *Accuracy* | *Precision* | *Recall* | *F-Measure* |
| *N* | 0,88227 ( 622/ 705) | 0,80838 ( 135/ 167) | 0,72581 ( 135/ 186) | 0,76487 |
| *V* | 0,86971 ( 514/ 591) | 0,74419 (  96/ 129) | 0,68571 (  96/ 140) | 0,71375 |
| *A* | 0,82192 ( 180/ 219) | 0,75342 (  55/  73) | 0,72368 (  55/  76) | 0,73826 |
| *R* | 0,81183 ( 151/ 186) | 0,94000 (  47/  50) | 0,59494 (  47/  79) | 0,72868 |
| *1* | 0,96154 ( 200/ 208) | 0,96154 ( 200/ 208) | 1,00000 ( 200/ 200) | 0,98039 |
| *2* | 0,84863 (1267/1493) | 0,63033 ( 133/ 211) | 0,47331 ( 133/ 281) | 0,54065 |
| *HAS_SYN* | 0,85494 ( 554/ 648) | 0,74742 ( 145/ 194) | 0,76316 ( 145/ 190) | 0,75521 |
| *HAS_ANT* | 0,86358 ( 652/ 755) | 0,84967 ( 130/ 153) | 0,61905 ( 130/ 210) | 0,71625 |
| *HAS_HYPO/HAS_HYPER* | 0,88288 ( 196/ 222) | 0,82979 (  39/  47) | 0,68421 (  39/  57) | 0,75000 |
| *HAS_MERO/HAS_HOLO* | 0,85526 (  65/  76) | 0,76000 (  19/  25) | 0,79167 (  19/  24) | 0,77551 |

Instanzen mit nur einem Kandidaten schneiden bereits optimal ab, bei mehreren Kandidaten sinkt das F-Measure auf einen sehr schlechten Wert von 54%. Der optimale Schwellenwert ist hier mit 0,2 deutlich über 0, worunter insbesondere der Recall leidet. Die Accuracy dagegen ist mit knapp 85% dagegen in einem akzeptablen Bereich. Zwischen den einzelnen Relationsarten ergeben sich kaum Unterschiede, bei den Wortarten schneiden Nomen etwas besser, Verben etwas schlechter ab.

*Ergebnisse mit Begrenzung der Anzahl an Zuweisungen pro Instanz auf die Kandidaten mit den höchsten Scores oberhalb des Schwellenwerts:*
| ** | *Accuracy* | *Precision* | *Recall* | *F-Measure* | *Threshold* |
| *Random* | 0,77778 (1323/1701) | 0,61573 ( 274/ 445) | 0,56965 ( 274/ 481) | 0,59179 | 0,0 |
| *PureLesk* | 0,73251 (1246/1701) | 0,51654 ( 406/ 786) | 0,84407 ( 406/ 481) | 0,64088 | 0,0 |
| *FirstCand* | 0,83892 (1427/1701) | 0,73258 ( 326/ 445) | 0,67775 ( 326/ 481) | 0,70410 | - |
| *H123* | 0,84303 (1434/1701) | 0,85667 ( 257/ 300) | 0,53430 ( 257/ 481) | 0,65813 | - |
| *LeskH123* | 0,85655 (1457/1701) | 0,77494 ( 334/ 431) | 0,69439 ( 334/ 481) | 0,73246 | 0,005 |
| *LeskB1* | 0,87302 (1485/1701) | 0,78867 ( 362/ 459) | 0,75260 ( 362/ 481) | 0,77021 | 0,0 |
| *LeskH123B1* | 0,86831 (1477/1701) | 0,77282 ( 364/ 471) | 0,75676 ( 364/ 481) | 0,76471 | 0,0 |
| *CosH123B1* | 0,86890 (1478/1701) | 0,77564 ( 363/ 468) | 0,75468 ( 363/ 481) | *0,76502* | 0,0 |

Durch die Beschränkung lässt sich das Ergebnis um 2,5% verbessern. Betrachtung der einzelnen Features (LeskH123B1):
| ** | *Accuracy* | *Precision* | *Recall* | *F-Measure* |
| *N* | 0,87660 ( 618/ 705) | 0,77654 ( 139/ 179) | 0,74731 ( 139/ 186) | 0,76164 |
| *V* | 0,86294 ( 510/ 591) | 0,70629 ( 101/ 143) | 0,72143 ( 101/ 140) | 0,71378 |
| *A* | 0,84932 ( 186/ 219) | 0,76543 (  62/  81) | 0,81579 (  62/  76) | 0,78981 |
| *R* | 0,87634 ( 163/ 186) | 0,91176 (  62/  68) | 0,78481 (  62/  79) | 0,84354 |
| *1* | 0,96154 ( 200/ 208) | 0,96154 ( 200/ 208) | 1,00000 ( 200/ 200) | 0,98039 |
| *2* | 0,85532 (1277/1493) | 0,62357 ( 164/ 263) | 0,58363 ( 164/ 281) | 0,60294 |
| *HAS_SYN* | 0,85648 ( 555/ 648) | 0,75130 ( 145/ 193) | 0,76316 ( 145/ 190) | 0,75718 |
| *HAS_ANT* | 0,87152 ( 658/ 755) | 0,78109 ( 157/ 201) | 0,74762 ( 157/ 210) | 0,76399 |
| *HAS_HYPO/HAS_HYPER* | 0,89640 ( 199/ 222) | 0,82692 (  43/  52) | 0,75439 (  43/  57) | 0,78899 |
| *HAS_MERO/HAS_HOLO* | 0,85526 (  65/  76) | 0,76000 (  19/  25) | 0,79167 (  19/  24) | 0,77551 |

Das F-Measure (vor allem durch einen besseren Recall) bei Instanzen mit 2 oder mehr Kandidaten ist nun deutlich besser (+6%), besonders Adverben und Adjektive profitieren von der Beschränkung.

---+++ Fehleranalyse (Deutscher Datensatz):
*False Positives:* (siehe [[%ATTACHURL%/False_Positives_LeskH123B1Limit.xls][False_Positives_LeskH123B1Limit.xls]])
   * Alle False Positives haben gemeinsam, dass mindestens eine Heuristik, der Bonus oder Lesk falsch liegen
   * Bonus und Heuristik 2 betrachten den Inhalt überhaupt nicht. Es wäre wünschenswert diese statistischen Methoden zu ersetzen. Dafür müsste jedoch anstatt der Lesk Methode eine semantische Methode mit einem deutlich höheren Recall verwendet werden. 
   * Man könnte Instanzen nicht mehr automatisch zuweisen, wenn eine Heuristik anschlägt, sondern den Score ähnlich zum Bonussystem erhöhen. Dazu wären jedoch weitere Heuristiken notwendig. Außerdem würde sich die Anzahl der False Negatives vermutlich erhöhen. 

*False Negatives:* (siehe [[%ATTACHURL%/False_Negatives_LeskH123B1.xls][False_Negatives_LeskH123B1.xls]])
   * Alle False Negatives haben gemeinsam, dass keine der 3 Heuristiken und nicht der Bonus anschlägt. Außerdem findet die Lesk Methode keine übereinstimmenden Wörter
   * Ähnlichkeiten zu finden ist z.B. bei Antonymen sehr schwer, da Lesk nur gleiche Wörter, keine gegensätzlichen, erkennt (z.B. gucken vs. horchen oder aufmachen vs. zumachen). Semantische Methoden wären in der Lage dies festzustellen
   * Auch in anderen Fällen fehlt es an semantischen Methoden (z.B. Strauch vs. Strauchwerk oder hingeben vs. Hingabe)
   * Teilweise sind die Beschreibungen sehr kurz
   * Umkehrung von Heuristik 3 ist teilweise hilfreich, führt aber zu vielen False Positives und kommt somit nicht in Frage
   * Teilweise ist der GoldStandard auch fragwürdig bzw. ich würde 0 und 1 als korrekt ansehen

*Fazit:*
   * Es handelt sich um einen sehr schwierigen Task, viele Beispiele sind auch als Mensch nur schwer zu beurteilen
   * Die Lesk Methode hat einen sehr geringen Recall und weist zu viele Schwächen auf. Sie muss durch eine semantische Methode (z.B. PPR) ersetzt werden

---+++ Fehleranalyse (Englischer Datensatz):
*False  Positives:* (siehe [[%ATTACHURL%/False_Positives_LeskH123B1_en.xls][False_Positives_LeskH123B1_en.xls]])
   * Die Heuristiken sind weniger oft anwendbar als beim deutschen Datensatz und sind somit auch für weniger False Positives verantwortlich
   * Oft reicht es bei sehr kurzen Beschreibungen der Senses für eine positive Klassifikation schon aus, wenn ein einziges Wort übereinstimmt (Lesk). Nicht selten ist dieses eine Wort für die Bedeutung des Beispiels nicht sonderlich relevant: 
      * hinder: {{transitive}} To make [[difficult]] to [[accomplish]]; to [[frustrate]], act as obstacle.
      * assist: {{sports}} To make a pass that leads directly towards scoring.
      * Nur das Wort "make" stimmt überein, hat jedoch keine Relevanz.
      * Das kann in manchen Fällen zu sehr vielen FP auf einmal führen (z.B. das Paar "deal with" - "consider" mit 8 FP dadurch, dass auf beiden Seiten das Wort "transitive" vorkommt, das jedoch nicht den Sense selbst beschreibt und somit nicht relevant ist)
   * Im Englischen Datensatz hat eine Instanz oft sehr viel mehr Kandidaten als im Deutschen Datensatz. 
   * Wenn in der Beschreibung der Source das target word vorkommt, kommt dies häufig auch in den Beschreibungen des target words vor was dann jedesmal zu einer positiven Klassifikation führt: 
      * derivative: {{finance}} A [[financial instrument]] whose value depends on the valuation of an [[underlying]] [[asset]]; such as a [[warrant]], an [[option]] etc.
      * warrant: ...
      * 6 verschiedene Kandidaten enthalten in ihrer Beschreibung das Wort "warrant" und werden somit positiv annotiert (Lesk)
   * Manche GoldStandard Annotationen sind abhängig von der "Strenge" des Annotators. In folgendem Beispiel geht es einmal um die Aktion und einmal um die Menge um die etwas erhöht wird:
      * Source: "The action of [[increasing]] or becoming greater."
      * Target: "An amount by which a quantity is increased"
      * Zieht man weitere mögliche Paare hinzu, erkennt man als Annotator (der diese Information hat), dass es ein besseres Target gibt: "{{transitive}} To make (a quantity) larger." Der evaluierende Computer dagegen betrachtet immer nur ein einzelnes Paar.
   * Neben der Lesk Methode entstehen viele False Positives durch Heuristik 3, was der Precision von (nur) 80% geschuldet ist. 

*False  Negatives:* (siehe [[%ATTACHURL%/False_Negatives_LeskH123B1_en.xls][False_Negatives_LeskH123B1_en.xls]])
   * Allen Fehlern hier ist gemeinsam, dass keine der Heuristiken anwendbar ist (Lesk für alle Fehler verantwortlich), es sind im Wesentlichen zwei Punkte zu erkennen, die zu Fehlern führen:
      * Die Beschreibungstexte sind sehr lange, wodurch ein einzelnes Wort nicht mehr für eine positive Annotation ausreicht
      * Es gibt kein einziges übereinstimmendes Wort
   * Zu beachten ist, dass der Schwellenwert mit 0,2 auf dem englischen Datensatz deutlich höher gewählt wurde, als beim deutschen Datensatz, wodurch natürlich auch mehr False Negatives entstehen und mehrere übereinstimmende Wörter für eine Zuweisung notwendig sind
   * Wie auch beim deutschen Datensatz sind gerade Antonyme oft schwer zu erkennen:
      * hinder: {{transitive}} To keep back; to [[delay]] or [[impede]].
      * assist: To [[help]].
      * Einerseits ist die Beschreibung von "assist" sehr kurz, andererseits kann ein nicht semantischer Ansatz kaum erkennen, dass hier eine Verwandtschaft besteht

*Fazit:*
   * Es handelt sich um einen sehr schwierigen Task, viele Beispiele sind auch als Mensch nur schwer zu beurteilen
   * Die Lesk Methode hat einen sehr geringen Recall und weist zu viele Schwächen auf. Sie muss durch eine semantische Methode (z.B. PPR) ersetzt werden

---+++ Multilinguale Experimente (EN_DE) (Deutsche Glosses ins Englische übersetzt):
| ** | *Accuracy* | *Precision* | *Recall* | *F-Measure* | *Threshold* |
| *Random* | 0,28878 ( 175/ 606) | 0,28878 ( 175/ 606) | 1,00000 ( 175/ 175) | 0,44814 | 0,0 |
| *PureLesk* | 0,77228 ( 468/ 606) | 0,65289 (  79/ 121) | 0,45143 (  79/ 175) | 0,53378 | 0,005 |
| *FirstCand* | 0,79043 ( 479/ 606) | 0,61765 ( 126/ 204) | 0,72000 ( 126/ 175) | 0,66491 | - |
| *H123* | 0,78218 ( 474/ 606) | 0,77215 (  61/  79) | 0,34857 (  61/ 175) | 0,48031 | - |
| *H3* | 0,70957 ( 430/ 606) | 0,48387 (  15/  31) | 0,08571 (  15/ 175) | 0,14563 | - |
| *LeskH123* | 0,79373 ( 481/ 606) | 0,66026 ( 103/ 156) | 0,58857 ( 103/ 175) | 0,62236 | 0,075 |
| *LeskB1* | 0,77063 ( 467/ 606) | 0,57031 ( 146/ 256) | 0,83429 ( 146/ 175) | 0,67749 | 0,005 |
| *CosB1* | 0,77393 ( 469/ 606) | 0,57480 ( 146/ 254) | 0,83429 ( 146/ 175) | *0,68065* | 0,01 |
| *LeskH123B1* | 0,75578 ( 458/ 606) | 0,55094 ( 146/ 265) | 0,83429 ( 146/ 175) | 0,66364 | 0,005 |
| *CosH123B1* | 0,75908 ( 460/ 606) | 0,55513 ( 146/ 263) | 0,83429 ( 146/ 175) | 0,66667 | 0,01 |
| *LeskH12B1* | 0,77063 ( 467/ 606) | 0,57031 ( 146/ 256) | 0,83429 ( 146/ 175) | 0,67749 | 0,005 |
| *CosH12B1* | 0,77393 ( 469/ 606) | 0,57480 ( 146/ 254) | 0,83429 ( 146/ 175) | *0,68065* | 0,01 |

Es wird deutlich, dass Heuristik 3 hier eine extrem schlechte Precision hat und somit nicht angewandt werden sollte. Heuristik 1 hat bei diesem Task ohnehin keinen Effekt, Heuristik 2 hat durch den Bonus ebenfalls kaum Einfluss. Die besten Ergebnisse erzielen wir mit dem Cosinus Ansatz zusammen mit dem Bonus (68% F-Measure). Wobei darauf hinzuweisen ist, dass dieser Ansatz die FirstCandidate Baseline nur geringfügig übertrifft. 

*Betrachtung der einzelnen Features (CosB1):*
| ** | *Accuracy* | *Precision* | *Recall* | *F-Measure* |
| *N* | 0,81043 ( 171/ 211) | 0,60241 (  50/  83) | 0,87719 (  50/  57) | 0,71429 |
| *V* | 0,73604 ( 145/ 197) | 0,51163 (  44/  86) | 0,81481 (  44/  54) | 0,62857 |
| *A* | 0,77273 ( 153/ 198) | 0,61176 (  52/  85) | 0,81250 (  52/  64) | 0,69799 |
| *1* | 0,96078 (  49/  51) | 0,96078 (  49/  51) | 1,00000 (  49/  49) | 0,98000 |
| *2* | 0,68627 (  70/ 102) | 0,60317 (  38/  63) | 0,84444 (  38/  45) | 0,70370 |
| *3* | 0,68667 ( 103/ 150) | 0,46875 (  30/  64) | 0,69767 (  30/  43) | 0,56075 |
| *4* | 0,81518 ( 247/ 303) | 0,38158 (  29/  76) | 0,76316 (  29/  38) | 0,50877 |

Je mehr Kandidaten wir haben desto schlechter wird der F-Measure Wert. Bei nur einem Kandidaten ist die Genauigkeit bereits optimal. Bei den Wortarten gibt es insbesondere bei Verben Probleme. Die Precision ist bei allen Features das Hauptproblem (zu viele False Positives). Eine Beschränkung der Anzahl an Zuweisungen pro Instanz könnte helfen:

*Ergebnisse mit Begrenzung der Anzahl an Zuweisungen pro Instanz auf die Kandidaten mit den höchsten Scores oberhalb des Schwellenwerts:*
| ** | *Accuracy* | *Precision* | *Recall* | *F-Measure* | *Threshold* |
| *Random* | 0,67492 ( 409/ 606) | 0,44608 (  91/ 204) | 0,52000 (  91/ 175) | 0,48021 | 0,0 |
| *PureLesk* | 0,79208 ( 480/ 606) | 0,73786 (  76/ 103) | 0,43429 (  76/ 175) | 0,54676 | 0,005 |
| *FirstCand* | 0,79043 ( 479/ 606) | 0,61765 ( 126/ 204) | 0,72000 ( 126/ 175) | 0,66491 | - |
| *H123* | 0,78218 ( 474/ 606) | 0,77215 (  61/  79) | 0,34857 (  61/ 175) | 0,48031 | - |
| *LeskB1*| 0,81023 ( 491/ 606) | 0,64563 ( 133/ 206) | 0,76000 ( 133/ 175) | *0,69816* | 0,0 |
| *CosB1* | 0,80858 ( 490/ 606) | 0,64390 ( 132/ 205) | 0,75429 ( 132/ 175) | 0,69474 | 0,0 |

Durch die Beschränkung ergibt sich eine Verbesserung des F-Measures von fast 2%. Die zuvor recht schwache Precision konnte gesteigert werden. 

Die Betrachtung der einzelnen Features ergibt folgendes Bild:
| ** | *Accuracy* | *Precision* | *Recall* | *F-Measure* |
| *N* | 0,85308 ( 180/ 211) | 0,69118 (  47/  68) | 0,82456 (  47/  57) | 0,75200 |
| *V* |  0,76650 ( 151/ 197) | 0,55882 (  38/  68) | 0,70370 (  38/  54) | 0,62295 |
| *A* | 0,80808 ( 160/ 198) | 0,68571 (  48/  70) | 0,75000 (  48/  64) | 0,71642 |
| *1* | 0,96078 (  49/  51) | 0,96078 (  49/  51) | 1,00000 (  49/  49) | 0,98000 |
| *2* | 0,72549 (  74/ 102) | 0,66667 (  34/  51) | 0,75556 (  34/  45) | 0,70833 |
| *3* |  0,70000 ( 105/ 150) | 0,48077 (  25/  52) | 0,58140 (  25/  43) | 0,52632 | 
| *4* | 0,86799 ( 263/ 303) | 0,48077 (  25/  52) | 0,65789 (  25/  38) | 0,55556 | 

---+++ Multilinguale Experimente (DE_EN) (Englische Glosses ins Deutsche übersetzt):
| ** | *Accuracy* | *Precision* | *Recall* | *F-Measure* | *Threshold* |
| *Random* | 0,29878 ( 196/ 656) | 0,29878 ( 196/ 656) | 1,00000 ( 196/ 196) | 0,46009 | 0,0 |
| *PureLesk* | 0,76524 ( 502/ 656) | 0,66154 (  86/ 130) | 0,43878 (  86/ 196) | 0,52761 | 0,055 |
| *FirstCand* | 0,78659 ( 516/ 656) | 0,63725 ( 130/ 204) | 0,66327 ( 130/ 196) | 0,65000 | - |
| *LeskB1* | 0,77591 ( 509/ 656) | 0,59245 ( 157/ 265) | 0,80102 ( 157/ 196) | 0,68113 | 0,005 |
| *CosB1* | 0,77896 ( 511/ 656) | 0,59696 ( 157/ 263) | 0,80102 ( 157/ 196) | *0,68410* | 0,01 |

Ähnlich wie bei dem anderen Multilingualen Experiment sind die Heuristiken ungeeignet, die besten Werte erreichen wir mit Cosinus + Bonus (68,4% F-Measure). 

*Betrachtung der einzelnen Features (CosB1):*
| ** | *Accuracy* | *Precision* | *Recall* | *F-Measure* |
| *N* | 0,78607 ( 158/ 201) | 0,61538 (  56/  91) | 0,87500 (  56/  64) | 0,72258 |
| *V* | 0,76371 ( 181/ 237) | 0,58427 (  52/  89) | 0,73239 (  52/  71) | 0,65000 |
| *A* | 0,78899 ( 172/ 218) | 0,59036 (  49/  83) | 0,80328 (  49/  61) | 0,68056 | 
| *1* | 0,88235 (  45/  51) | 0,88235 (  45/  51) | 1,00000 (  45/  45) | 0,93750 |
| *2* | 0,70588 (  72/ 102) | 0,61667 (  37/  60) | 0,84091 (  37/  44) | 0,71154 |
| *3* | 0,71895 ( 110/ 153) | 0,58108 (  43/  74) | 0,78182 (  43/  55) | 0,66667 |
| *4* | 0,81143 ( 284/ 350) | 0,41026 (  32/  78) | 0,61538 (  32/  52) | 0,49231 |

Auch hier kommen wir zu ähnlichen Ergebnissen wie oben.

*Ergebnisse mit Begrenzung der Anzahl an Zuweisungen pro Instanz auf die Kandidaten mit den höchsten Scores oberhalb des Schwellenwerts:*
| ** | *Accuracy* | *Precision* | *Recall* | *F-Measure* | *Threshold* |
| *Random* | 
| *PureLesk* | 
| *FirstCand* | 0,78659 ( 516/ 656) | 0,63725 ( 130/ 204) | 0,66327 ( 130/ 196) | 0,65000 | - |
| *H123* | 
| *LeskH123* | 
| *LeskB1*| 
| *LeskH23B1* | 
| *LeskH123B1* | 
| *CosH123B1* | 

Die Betrachtung der einzelnen Features ergibt folgendes Bild:
| ** | *Accuracy* | *Precision* | *Recall* | *F-Measure* |
| *N* | 
| *V* | 
| *A* | 
| *R* | 
| *1* | 
| *2* | 
| *HAS_SYN* | 
| *HAS_ANT* | 
| *HAS_HYPO/HAS_HYPER* | 



---++ Weitere Versuche:
   * Anzahl der positiven Annotationen pro Instanz beschränken
   * Semantische Methoden testen (z.B. PPR)
   * Weitere Relatedness Methoden testen

---++ Current Schedule

%EDITTABLE{  sort="on" tableborder="0" cellpadding="1" cellspacing="3" headerrows="1"  footerrows = "1" }%
| *Task* | *Date* | *Hours* | *Remarks* |
| - | 28.03.2011 | 0 | Project Plan (!ChM) |
| Part 1 + Meeting | 11.04.2011 | 1 | |
| Datenbank aufsetzen + Workspace anlegen | 11.04.2011 | 0.75 | |
| In Code einlesen | 12.04.2011 | 0.75 | |
| Software testen, Abläufe bei Datenerzeugung nachvollziehen, Wiktionary genauer ansehen | 15.04.2011 | 2.5 | |
| Daten in Datenbank analysieren, Gedanken über Erstellung eines GoldStandards machen | 24.04.2011 | 2.5 | |
| Random und MFS Evaluationsdaten für WordNet-Wiktionary Alignment erstellen | 28.04.2011 | 1.5 | |
| Meeting | 29.04.2011 | 0.5 | |
| Erstellen einer Statistik über Verteilung von POS, Kandidaten, Relationen in englischer Wiktionary und Bewertung der Daten | 01.05.2011 | 2 | |
| Programmaufbau überlegen, mit Umsetzung beginnen | 03.05.2011 | 3 | |
| Statistik über Labels erstellen, analysieren, Programmieren: Code um Samples aus DB zu extrahieren soweit fertig, Textfile wird erstellt, Einbinden in vorgegebenen Code, Erstellung eines ersten (unaligned) GoldStandards, Analyse der Daten und Feststellung von Problemen | 04.05.2011 | 3.5 | |
| Automatische Erstellung einer Statistik über erstellten GoldStandard programmieren | 07.05.2011 | 1.5 | |
| Weitere Datenbank (en_ws) aufsetzen, de und en_ws Datenbanken ansehen und integrieren, Statistik erweitern | 14.05.2011 | 2.5 | |
| Neue Daten importiert, Datensätze generiert | 31.05.2011 | 1 | |
| Deutschen GoldStandard annotiert (Teil 1) | 07.06.2011 | 1.5 | |
| Deutschen GoldStandard annotiert (Teil 2) | 16.06.2011 | 3.5 | |
| Englischen GoldStandard annotiert (Teil 1) | 26.06.2011 | 1.5 | |
| Englischen GoldStandard annotiert (Teil 2) | 27.06.2011 | 3.5 | |
| Englischen GoldStandard annotiert (Teil 3) | 03.08.2011 | 2 | |
| Meeting | 23.08.2011 | 0.5 | |
| Code auschecken und einlesen | 04.09.2011 | 2 | |
| Codeanalyse: Überprüfung der vorhandenen Evaluationsmethode und Feststellung notwendiger Änderungen | 12.09.2011 | 2 | |
| Evaluation und Disambiguierung korrigieren und an unsere Bedürfnisse anpassen: Mehrere positive Annotation pro Instanz, Disambiguierung auf Basis eines Thresholds, Evaluation entsprechend Paaren (nicht Instanzen) | 13.09.2011 | 2,5 | |
| Methoden (z.B. Lesk) testen, Schwachstellen identifizieren | 13.09.2011 | 1,5 | |
| Über Heuristiken nachdenken, eine erste Heuristik implementieren, Nutzen der Heuristik evaluieren | 15.09.2011 | 4 | |
| TreeTagger & StopWords installiert | 18.09.2011 | 0.5 | |
| Heuristiken 2 und 3 implementiert und getestet, TreeTagger weiter eingebunden und getestet | 19.09.2011 | 2 | |
| Schweren Bug bei Kandidatenextraktion gefunden und behoben, Evaluationsdaten für Heuristiken korrigiert | 19.09.2011 | 1 | |
| Schwellenwerttraining + Visualisierung der Ergebnisse implementiert | 05.10.2011 | 1.5 | |
| Schwellenwerttraining besser eingebunden, RelatednessMethod testen, Instanzen mit Folds versehen (für spätere CrossValidation) | 07.10.2011 | 3.5 | |
| Experimente mit verschiedenen Methoden durchführen und Ergebnisse festhalten | 08.10.2011 | 1 | |
| Weitere Experimente auf englischem Datensatz durchführen und festhalten, Fehler im GoldStandard identifizieren | 09.10.2011 | 2 | |
| In Bing Translation API einlesen, erste Version programmieren (siehe Attachments) | 09.10.2011 | 2.5 | |
| Tabellen-Schemata für Übersetzungslinks und Vorgehensweise planen | 26.10.2011 | 1 | |
| Berkeley DB Files zum Laufen gebracht und getestet, Tabellenschemata und Vorgehensweise genauer beschreiben | 30.10.2011 | 2.5 | |
| Code für Übersetzung von Sense-Tabellen geschrieben, getestet, überarbeitet und Übersetzung der deutschen sense Tabelle gestartet | 05.11.2011 | 3.5 | |
| wkten1104_sense_translation fertig übersetzen, Probleme mit URLs in zu übersetzendem Text beheben | 06.11.2011 | 2 | |
| Code zum Parsen von Übersetzungslinks schreiben, Testen, Ausführen | 07.11.2011 | 2.5 | |
| Fehler bei Übersetzung beheben, examples nachtragen (übersetzen), mit Übersetzung von wkten1104_sense beginnen | 08.11.2011 | 2 | |
| Code schreiben, um die target_term_ids in den _translation Tabellen aus den Tabellen der Zielsprache herauszusuchen, Ausführen, Tabellen mit korrekten ids exportieren und hochladen | 11.11.2011 | 1.5 | |
| Englische sense Tabelle vollständig übersetzt | 29.11.2011 | 0.5 | |
| Mit ESA befassen, Code so umschreiben, dass multilinguale Experimente möglich, ein erstes Exeriment mit englisch-deutschem Datensatz durchführen | 11.12.2011 | 2 | |
| Experimente mit ESA und auf multilingualen Datensätzen | 21.12.2011 | 2 | |
| Korrigierte Relationtabelle, ExperimentPreparation und Datensätze eingespielt (nur deutsch), Experimente wiederholt, Werte korrigiert, Übersichtstabellen angelegt, Code erweitert, sodass einzelne Features evaluiert werden, einzelne Features evaluiert, Code erweitert, sodass Anzahl der Zuweisungen pro Instanz beschränkt wird, Ergebnisse analysiert | 03.01.2012 | 5 | |
| Zusätzliche WSDEvaluationMethod für Heuristiken erstellt, Voraussetzungen für Fehleranalyse geschaffen: Ergebnisse verschiedener Methoden werden zurück in eine Excel Tabelle geschrieben, dort können Fehlklassifizierungen analysiert und kommentiert werden | 05.01.2012 | 2 | |
| Fehleranalyse | 06.01.2012 | 2.5 | |
| Evaluation des Englischen Datensatzes | 12.01.2012 | 2 | |
| Korrektur und Wiederholung der Experimente auf englischem Datensatz, Fehleranalyse | 20.03.2012 | 4 | |
| *Summe* | | %CALC{"$SUM( $ABOVE() )"}% | |
| *Soll* | | 120 | bis Ende März 2012 |
<nop>

-- Main.ChristianMeyer - 2011-03-28

%META:FILEATTACHMENT{name="heuristik1.png" attachment="heuristik1.png" attr="h" comment="Heuristik 1" date="1316115427" path="heuristik1.png" size="36857" user="ChristianKirschner" version="1"}%
%META:FILEATTACHMENT{name="bing-api-translate-java-0.1.jar" attachment="bing-api-translate-java-0.1.jar" attr="" comment="Erste Version der BingTranslationAPI zum Übersetzen von deutschen/englischen Texten" date="1318185111" path="bing-api-translate-java-0.1.jar" size="6078" user="ChristianKirschner" version="1"}%
%META:FILEATTACHMENT{name="wkten1104_sense_translation.sql" attachment="wkten1104_sense_translation.sql" attr="" comment="Übersetzung der Tabelle wktde1104_sense" date="1320750257" path="wkten1104_sense_translation.sql" size="19659146" user="ChristianKirschner" version="2"}%
%META:FILEATTACHMENT{name="wkten1104_translation.sql" attachment="wkten1104_translation.sql" attr="" comment="geparste Übersetzungslinks (target_term_id bezieht sich auf target)" date="1321027517" path="wkten1104_translation.sql" size="3520345" user="ChristianKirschner" version="2"}%
%META:FILEATTACHMENT{name="wktde1104_translation.sql" attachment="wktde1104_translation.sql" attr="" comment="geparste Übersetzungslinks (target_term_id bezieht sich auf target)" date="1321027587" path="wktde1104_translation.sql" size="2674543" user="ChristianKirschner" version="2"}%
%META:FILEATTACHMENT{name="wktde1104_sense_translation.sql" attachment="wktde1104_sense_translation.sql" attr="" comment="Übersetzung der Tabelle wkten1104_sense" date="1322565057" path="wktde1104_sense_translation.sql" size="73521161" user="ChristianKirschner" version="1"}%
%META:FILEATTACHMENT{name="False_Negatives_LeskH123B1.xls" attachment="False_Negatives_LeskH123B1.xls" attr="" comment="Fehleranalyse: False Negatives" date="1325852429" path="False_Negatives_LeskH123B1.xls" size="51712" user="ChristianKirschner" version="1"}%
%META:FILEATTACHMENT{name="False_Positives_LeskH123B1Limit.xls" attachment="False_Positives_LeskH123B1Limit.xls" attr="" comment="Fehleranalyse: False Positives" date="1325852485" path="False_Positives_LeskH123B1Limit.xls" size="47616" user="ChristianKirschner" version="1"}%
%META:FILEATTACHMENT{name="False_Positives_LeskH123B1_en.xls" attachment="False_Positives_LeskH123B1_en.xls" attr="" comment="Fehleranalyse: False Positives (englisch)" date="1332273327" path="False_Positives_LeskH123B1_en.xls" size="63488" user="ChristianKirschner" version="1"}%
%META:FILEATTACHMENT{name="False_Negatives_LeskH123B1_en.xls" attachment="False_Negatives_LeskH123B1_en.xls" attr="" comment="Fehleranalyse: False Negatives (englisch)" date="1332333756" path="False_Negatives_LeskH123B1_en.xls" size="68096" user="ChristianKirschner" version="1"}%

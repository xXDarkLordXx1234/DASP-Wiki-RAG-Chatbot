%META:TOPICINFO{author="nothvogel" comment="" date="1566318330" format="1.1" reprev="83" version="83"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! %TOPIC%

---++ Arbeitsstunden

<details>
  <summary><p>Historie (click to show)</p></summary>
%EDITTABLE{format="| text,20| text,20| text,20| text,20| text,20|"}%
| *Datum* | *Beginn* | *Ende* | *Täglich (hh:mm)* | *Notiz* |
|  | 10:00 | 16:00 | 06:00 |Quali Test |
|  |  |  | 07:00 |Arg Sim / Funkstandard |
| | | |06:00 |Arg Sim / Funkstandard |
| | | |01:00 |Arg Sim / Funkstandard |
|06.03.2019 | | |02:00 |Arg Sim / Funkstandard |
|07.03.2019 | | |03:30 |Arg Sim / Funkstandard |
|08.03.2019 | | |08:00 |Arg Sim / Funkstandard |
|11.03.2019 | | |03:00 |Hyperparam Optimization |
|12.03.2019 | | |02:30 |Final Model |
|13.03.2019 | | |01:00 |Custom MSE, BERT |
|14.03.2019 | | |00:30 |Meeting, Notizen |
|20.03.2019 | | |01:00 |BERT |
|21.03.2019 | | |02:00 |BERT |
|22.03.2019 | | |02:00 |BERT |
|25.03.2019 | | |04:00 |BERT |
|26.03.2019 | | |02:00 |BERT |
|27.03.2019 | | |04:00 |BERT |
|28.03.2019 | | |02:30 |BERT |
|29.03.2019 | | |02:00 |BERT |
|01.04.2019 | | |05:00 |BERT |
|02.04.2019 | | |07:00 |BERT |
|05.04.2019 | | |02:00 |BERT |
|08.04.2019 | | |03:00 |BERT |
|09.04.2019 | | |01:30 |BERT |
|11.04.2019 | | |01:30 |BERT |
|12.04.2019 | | |01:00 |BERT/Misra |
|16.04.2019 | | |01:30 |BERT/Misra |
|17.04.2019 | | |05:30 |BERT/Misra |
|23.04.2019 | | |04:00 |BERT/Misra |
|25.04.2019 | | |01:00 |BERT/Misra |
|26.04.2019 | | |02:00 |BERT/Misra |
|29.04.2019 | | |03:00 |Misra |
|30.04.2019 | | |02:00 |Misra |
|02.05.2019 | | |01:30 |BERT/Misra |
|03.05.2019 | | |01:00 |BERT/Misra |
|06.05.2019 | | |05:30 |BERT/Misra |
|07.05.2019 | | |01:00 |BERT/Misra |
|08.05.2019 | | |02:30 |BERT/Misra |
|09.05.2019 | | |01:30 |BERT/Misra |
|10.05.2019 | | |02:00 |BERT/Misra |
|13.05.2019 | | |04:30 |BERT/Misra |
|14.05.2019 | | |05:30 |Crowdsourcing |
|15.05.2019 | | |03:30 |Crowdsourcing |
|16.05.2019 | | |03:00 |Crowdsourcing |
|17.05.2019 | | |02:00 |Crowdsourcing |
|20.05.2019 | | |01:30 |Crowdsourcing |
|21.05.2019 | | |01:00 |Crowdsourcing |
|22.05.2019 | | |01:00 |Crowdsourcing |
|23.05.2019 | | |02:30 |Crowdsourcing |
|24.05.2019 | | |00:30 |Crowdsourcing |
|(27.05.2019) | | |02:00 |Crowdsourcing |
|(28.05.2019) | | |01:00 |Crowdsourcing |
|(29.05.2019) | | |00:30 |Crowdsourcing |
|(30.05.2019) | | |01:00 |Crowdsourcing |
|(31.05.2019) | | |01:00 |BERT |
|03.06.2019 | | |02:00 |BERT |
|04.06.2019 | | |01:00 |BERT |
|05.06.2019 | | |02:00 |BERT |
|06.06.2019 | | |02:00 |BERT |
|11.06.2019 | | |01:00 |BERT |
|12.06.2019 | | |01:00 |BERT |
|13.06.2019 | | |01:00 |BERT |
|17.06.2019 | | |03:00 |BERT |
|18.06.2019 | | |01:00 |BERT |
|19.06.2019 | | |01:00 |BERT |
|21.06.2019 | | |01:00 |BERT |
|24.06.2019 | | |02:00 |BERT |
|25.06.2019 | | |02:00 |BERT |
|26.06.2019 | | |03:00 |BERT |
|27.06.2019 | | |04:00 |BERT |
|28.06.2019 | | |02:00 |BERT |
|03.07.2019 | | |01:30 |Clustering |
|04.07.2019 | | |02:00 |Clustering |
|05.07.2019 | | |03:30 |Clustering |
|24.07.2019 | | |02:00 |Clustering |
|25.07.2019 | | |03:00 |Clustering |
|31.07.2019 | | |02:40 |Clustering |


Summe Mai: ~ 40:00
Voll ab 24.05.

Summe Juni: ~30:00; 10h Überstunden vom Februar sind damit ausgeglichen.


Summe Juli: 14:30, mit restlichen Ueberstunden: 30:00
Im August noch nachzuarbeiten: 10h
</details>

---+++ Aktuell
%EDITTABLE{format="| text,20| text,20| text,20| text,20| text,20|"}%
| *Datum* | *Beginn* | *Ende* | *Täglich (hh:mm)* | *Notiz* |
|01.08.2019 | | |01:30 |Clustering |
|02.08.2019 | | |01:00 |Clustering |
|05.08.2019 | | |02:00 |Clustering |
|07.08.2019 | | |00:30 |Clustering |
|08.08.2019 | | |01:30 |Clustering |
|11.08.2019 | | |00:30 |Clustering |
|12.08.2019 | | |02:00 |Clustering |
|14.08.2019 | | |02:30 |Clustering |
|15.08.2019 | | |02:30 |Clustering |
|16.08.2019 | | |01:00 |Documentation |
|18.08.2019 | | |02:00 |Documentation |


Stand: inklusive 18.08.2019

---++ Crowdsourcing

   * Topics:

%EDITTABLE{format="| number| text,20| text,20| number| number|"}%
| *id* | *name* | *description* | *batchNr* | *numOfPairs* |
| 1 | abortion | choosing to end the pregnancy before birth takes place | 1 | 425 |
| 2 | cloning | creating genetically identical copies of humans | 1 | 425 |
| 3 | death penalty | the government killing someone as punishment for a serious crime | 2 | 425 |
| 4 | gun control | restraining people from owning or using guns | 2 | 425 |
| 5 | marijuana legalization | the government allowing recreational use of cannabis | 3 | 425 |
| 6 | minimum wage | increasing least amount of money employees can be paid | 3 | 425 |
| 7 | nuclear energy | using nuclear energy to generate electricity | 4 | 425 |
| 8 | school uniforms | making all students wear a standard set of clothing | 4 | 425 |

   * Pair selection:
      * Misra Predictions labelled as: Top 1%: HS, Top 2-50%: SS, Top 51-100% NS.
      * Pairs for new corpus sampled from these label buckets: 50% HS, 25% SS, 25% NS.
      * Per topic: 350 pairs from stab corpus, 75 one stab + one web argument paired together

   * Batches:

%EDITTABLE{format="| number| number| number| number| number| number| number| number| number| number| number| number| number| text,20| number| "}%
| *number* | *#pairs* | *bwsFactor* | *maxAssignments* | *#HITs* | *#assignments* | *#cheaters* | *$reward* | *$total* | *$perPair* | *bestAgree* | *worstAgree* | *bestWorstAgree* | *topics*                                                     | *run time (hours)* |
| 1        | 850      | 1.75         | 4                | 1638     | 6552           | 0           | 0.20      | 1572.48    | 1.85       |    0.67    |    0.63     |      0.49       | abortion, cloning                       | ~24 |
| 2        | 850      | 1.75         | 4                | 1638     | 6552           | 0           | 0.20      | 1572.48    | 1.85       |    0.58    |    0.57     |      0.41       | death penalty, gun control                       |~20 |
| 3        | 850      | 1.75         | 4                | 1638     | 6552           | 2           | 0.20      | 1572.48    | 1.85       |    0.58    |    0.56     |      0.40       | marijuana legalization, minimum wage                       |~18 |
| 4        | 850      | 1.75         | 4                | 1638     | 6552           | 0           | 0.20      | 1572.48    | 1.85       |    0.59    |    0.58     |      0.42       | nuclear energy, school uniforms                       |~20 |
| 3.1        |       |          |                 |     | ~250           |            |       |  ~60   |        |        |         |             | marijuana legalization, minimum wage                       |~18 |
| *SUM*    | %CALC{"$SUM( $ABOVE() )"}%   |           |                |       |              |        |          | %CALC{"$SUM( $ABOVE() )"}%       |          | | | | ||
| | | | | | | | | | | | | | | |


   * Worker Block List:

| *workerId* | *reason* | *runName* | *rejected* |
| A1JVWK2XKOE3K5 | Bad captchas. Not in line with majority (>30% of votes differing). | batch-1-live | False |
| ASXE94CMR9SVL | Bad captchas. Not in line with majority (>30% of votes differing). | batch-1-live | False |
| AKW57KYG90X61 | Not in line with majority (>30% of votes differing). | batch-1-live | False |
| A2XH3ANYM43OB9 | Not in line with majority (>30% of votes differing). | batch-1-live | False |
| A2VFHTZKUFKG16 | Bad captchas. Not in line with majority (>30% of votes differing). | batch-1-live | False |
| AKBRH661KPW1R | Bad captchas. Not in line with majority (>30% of votes differing). | batch-2-live | False |
| A32DB8CJR9IZ5H | Bad captchas. | batch-2-live | False |
| A218WYOK44NBHQ |  Bad captchas. Not in line with majority (>30% of votes differing). | batch-3-live | False |
| A3N17E39KZOQX0 | Not in line with majority (>30% of votes differing). | batch-3-live | False |
| AITP2LUW8GPB | Not in line with majority (>30% of votes differing). | batch-3-live | False |
| A3A9PARKHSKRWI | Not in line with majority (>30% of votes differing). | batch-3-live | False |
| A38GYK5UQVOCLK | Bad captchas. Not in line with majority (>30% of votes differing). | batch-3-live | False |
| A3FIO5T8LH65DM | Bad captchas. Not in line with majority (>30% of votes differing). | batch-3-live | False |
| A2Z2CVOBYA74B0 | Bad captchas. Not in line with majority (>30% of votes differing). | batch-3-live | False |
| A1Z6XK510SSAMU | Bad captchas. Not in line with majority (>30% of votes differing). | batch-3-live | True |
| AQ8PG3HTV72YT | Bad captchas. Not in line with majority (>30% of votes differing). | batch-3-live | True |

   * How to interpret Quality Control measures from the worker-check script: 
      * Most workers need between 0:30 and 2:00 median time per HIT (m:ss)
      * Workers with a lot of assignments and some experience (e.g. starting from the second batch) can get as low as 15-30s median time per HIT
         * median lower than 15s probably indicates that something is wrong and the worker should be blocked.
      * Error Rate on 'easy' HITs: Is a good measure if the overall agreement is high (> ~70% for best/worst)
         * In these experiments, the majority choice (75% of votes!) was not always correct. 
         * The average worker has up to 25% error rate.
         * Error rate >= 30% means some HITs/other stats should be checked manually.
      * Captchas correct: Most workers are able to solve 95-100% of captchas correctly.
         * Some mix up best and worst instructions and get a high error rate. If this is a repeated mistake, they might not be the best workers and could be blocked anyways.
         * Always check captcha choices by hand!
         * In the table, bad captchas means <85% correct.

   * Captchas correct, Error on Easy HIT, Time needed per Assignment Reference: TODO

---++ Meetings

---+++ neu
   * Flask App für Clustering Web Service:
      * Code für REST Routes, Clustering Algo und Similarity Funktion sind getrennt.
      * Läuft lokal und inzwischen auch auf Lisa.
      * Ein Port muss geöffnet werden! Ich habe keine sudo Rechte und die Firewall blockiert wahrscheinlich den Standard Flask Port (5000). 
      * Input Format aktuell: Json, "arguments": Liste von Argumenten.
      * Output Format aktuell: Json, Liste von Listen von Argumenten. 
      * Clustering Algorithmus wurde so angepasst, dass die Argumente Paarweise in Batches (statt jedes einzeln) predicted werden. Sollte die Performance verbessern.
         * Batch Size kann je nach Speicherbedarf angepasst werden.
         * Predictions werden pro Paar in einem Cache gespeichert. Der wächst potentiell endlos. Evt. irgendwann leeren?
      * Noch nicht implementiert:
         * Docker
         * Auth 
         * Logging, Error Handling
         * Cache leeren, z.B. ab einer bestimmten Anzahl an Einträgen.
         * Maximale Anzahl an Argumenten im Service setzen, um DoS zu verhindern.
   * Darstellung von Clustern: 
      * Erstmal Listen von Argumenten.
      * Dann evt. Word Cloud
   * Threshold evt. auch als Parameter für den Request.
   * Service soll in Docker Container. Evt. nicht von mir
   * Dann im Frontend einbauen. Termin in 2 Wochen für Einweisung Frontend
   * Bis dahin: Service sollte laufen. Dann die Perfomance testen, wie viele Argumente wir in 1 Minute clustern können.
   * Nochmal checken: Sind die alten Projekte noch aktuell? Alles gepusht? Repos sauber machen.
      * Dokumentation: Readme: Description, Setup, Besonderheiten. Was haben wir gemacht? Vorgehen.
      * Repo von der Bachelorarbeit archivieren/zum BERT Projekt linken.
      * Alte TODOs entfernen.
   * Weitere Auswertungen von den BWS Daten. 
   * So weit dokumentieren, dass JD die Plots selbst generieren kann.
   * Auswahl der Paare: Welche Misra Parameter?
   * Code für die Plots sollte auf jeden Fall da sein. Damit JD sie selbst ausführen kann. Clustergrößen, Plots nach BWS.
   * Clustering Ergebnisse (F1 Score) auch noch Google Doc. In das BERT Doc als neuen Tab einfügen.

---+++ 08.08.2019

   * Problem: Teilweise sind die Cluster sehr groß, z.B. split5.
   * Versuchen, das Model mal in die Anwendung zu integrieren, dann kann man es in der Anwendung auswerten.
      * Service, der List of Argument bekommt, und dann direkt Clustering macht. Und dann List of list mit Clustern zurückgeben.
      * Datenformat: JSON. Service Code ist schon vorhanden.
      * Wahrscheinlich 1 GPU verfügbar. Mal testen, wie viel Speicher es braucht. Entweder batch size oder sequence length reduzieren.
      * Keine Rechte für Docker. Entweder lokal auf Laptop testen ohne Cuda oder auf Server ohne docker.
      * Evt. F1 Score mit Score über Threshold -> Im gleichen Cluster? wenn es sinn macht und nicht zu lange rechnet.
   * Für später: Evt. F1 Score bei unbekannten Argumenten oder eine art Qualitätsmaß über die predictete Similarity nehmen. Density?
   * Clustering nochmal mit anderem Seed und JD die Ergebnisse schicken.

---+++ 01.08.2019

   * Clustering ist jetzt deterministisch. Es wird jetzt immer in derselben Reihenfolge durch Sets und Dictionaries iteriert.
   * Damit wir trotzdem noch einen zufälligen Einfluss haben, wird die sortierte Liste geshuffled. Das kann mit dem Seed gesteuert werden.
   * Evt. haben wir leicht verschiedene Predictions von BERT weil (A,B) nicht gleich (B,A).
   * Für 3 Clustering Seeds mit festen BERT Seed nochmal Ergebnisse berechnen.
   * Word Cloud: Nach Möglichkeit auf eine Seite bringen. Oder 10 Bilder in einem Ordner.
   * Visualisierung von Clustern bis Montag, 05.08. noch JD schicken.
   * Plan im August: Integration in ArgumenText. 
      * REST Endpoint: Clustern und evt. Paare auf Similarity überprüfen können.
      * Input: Liste von Argumenten. Output: Cluster (List of list?). Oder 2. Endpoint mit Score.
      * Flask / Docker Container. Flask ist eine Art Binding von HTTP auf python. 
      * Extra Auth erstmal nicht notwendig.
   * Morgen nach der Klausur: Verträge unterzeichnen, Stundenzettel.
   * Vertrag bis 31. Oktober.

---+++ 25.07.2019

   * Cluster weiter auswerten: 
      * Für jedes Cluster eine eigene HTML Datei
      * Word Cloud nach Worthäufigkeit normal und evt. mit Background Corpus, nach Möglichkeit schön darstellen / mehr Aufwand
      * Noch warten: Alles in einem Plot: 2d Darstellung aus Vektorraum aus gesamten Vokabular. Mit 1-hot Endcoding oder so etwas wie word2vec.

   * Determinismus/Seeds: 
      * Evt. wird das Bert Model nicht richtig geladen, oder der Cluster Algo ist random.
      * Oder Probleme mit Float: genauigkeit/rundungsfehler.
      * Evt. Probleme mit Set / Reihenfolge der Argumente nicht deterministisch: Set alphabetisch sortieren?

   * Vorgehen:
      * 1. Woher kommt der non-determinismus? 
      * 2. Visualisierung anpassen
      * 3. Mit verschiedenen Clustering Seeds testen, aber mit demselben BERT Model
      * 4. Evt. noch mit anderem BERT Model, macht aber wahrscheinlich nicht so viel Sinn

   * Clustering erstmal wichtiger als nur stab Daten testen.


---+++ 01.07.2019
   * Clustering Script läuft, aber noch nicht vernünftig.
   * Lookup Table hat bei mir aktuell nicht genug Einträge - Clustering Algorithmus versucht alle Kombinationen von Argumenten, aber ich benutze nur die Predictions der bekannten Paare / wo wir auch einen Gold-Score haben.
   * Doch mit Bert Model Checkpoint testen? Dann wäre es nicht so ein großes Problem. Ansonsten werden die Dateien sehr groß. Ja!
   * Wie funktioniert das dann auf dem Test Set?
      * Clustering Algo testet alle Kombinationen von Argumenten.
      * Aber bei der Evaluation werden nur die bekannten Paare geprüft.
   * Welchen Threshold für das 'true label' benutzen? Im Original sind HS und SS 1. Wenn wir den selben Threshold nehmen, der gerade auch zum Clustern benutzt wird, bekommen wir bei abweichenden Score Ranges wahrscheinlich etwas schlechte Predictions.
      * Erstmal gleich wie beim Clustering lassen. 
   * Visualisierung der entstehenden Cluster?
   * Dokumentation
   * Email mit Zusammenfassung diese Woche noch.
   * Gespeicherte Weights nach den Experimenten wieder löschen!


---+++ 25.06.2019
   * Im Beispielscript werden bereits vorhandene Predictions aus einer TSV Datei benutzt (hat den Vorteil, dass es unabhängig vom BERT Model laufen kann (z.B. auch ohne GPU).)
      * Pro split und Set (Train/Dev/Test) gibt es eine Datei. 
      * Wird im Code in ein `dict` gelesen
   * Cluster wird geschlossen, wenn der mean Score im Cluster kleiner als der Threshold ist?
   * Evaluation: Paar richtig klassifiziert? Wenn Paar similar (Label 1), dann sollen die Argumente A und B im selben Cluster liegen.
   * Threshold:
      * Wird aktuell im Script von 0 bis 1 in 20stel-Schritten getestet.
      * Klassenverteilung zeigt, dass wahrscheinlich nur Werte zwischen 0 und 0.3 bis 0.4 sinnvoll sind (sonst ist similar sehr selten). Je nachdem, wie lange der Algo läuft, trotzdem alles testen!
   * Wie genau "Lernen" wir beim Clustering?
      * Es gibt wie gewohnt train/dev/test Set
      * Parameter, der geändert werden kann: Threshold, sonst nichts.
      * Wir suchen auf dem Dev Set den besten Threshold Wert basierend auf den Predictions, 
      * Testen dann auf dem Test Set. Train Set wird zum Clusteringm nicht benötigt.
      * Predictions nur von 1 Seed verwenden?
         * Für den Anfang mit 1 Seed, aber später mit mehreren.
   * Evaluation: Top 3-5 Argumente aus dem Cluster rausziehen, aber wie die Top Argumente auswählen? Evt. einfach 5 random Argumente rausziehen. Dann für jedes Cluster auflisten.
   * Nochmal gucken, ob Clustering deterministisch oder random ist? Z.B. auch mit welchen Argumenten angefangen wird.
   * Erstmal ist lookup table ok, also muss kein BERT Model live mitlaufen. 


---+++ 18.06.2019
   * Clustering: In dem alten BERT Repo ist eine clustering Klasse. Ähnliche Argumente zusammen clustern. https://git.ukp.informatik.tu-darmstadt.de/daxenberger/unsupervised-bert/blob/master/clustering/hierachical_clustering/run_clustering.py
      * Anpassen: Binär zu Score/Threshold
      * Wie evaluiert man die resultierenden Cluster?
      * Nächster Parameter: Anzahl der Cluster. Zwischen 2 und 10?
      * Wie stellt man die Ergebnisse dar? Z.B. Tabelle mit 1 Spalte pro Cluster. Word Cloud über einem Cluster?
      * Auch hier Cross Topic: 7 Themen trainieren, 1 testen. 
      * Zum Testen des Clustering nur die Paare nehmen, die im Goldstandard sind (und nicht alle Argumente).
      * Test Evaluation: Richtig klassifiziert, wenn Argumente aus ähnlichen Paaren im selben Cluster sind.
   * Schritt 1: Threshold testen: Verteilung plotten (gesamt und pro Thema). Balkendiagramm mit % ähnlich/unähnlich.
   * Schritt 2: Clustering wie oben beschrieben. 
   * Experimente ohne web Argumente: Nebenbei laufen lassen, aber eher nächsten Monat. Clustering wichtiger.

---+++ 13.06.2019
   * Nochmal die besten Configs pro Split mit 2 anderen Seeds testen (auch auf den Test Topics).
      * Für die finalen Test werte: Avg über den seeds + Abweichung 
   * Für intern: Die Top 3 auf den Test Topics testen.
   * Die Repos nochmal besser dokumentieren (Readme, Comments etc.)
   * Die Web Daten müssten wir rauslassen, wenn Ergebnisse fürs Paper reported werden. Achtung: Mean der Scores ist evt. etwas verschoben.
      * Versuchen, das selbe Dev Set zu nehmen (ohne Web Argumente). Wenn der Anteil >10% ist, muss man noch ein Paar Argumente dazu ziehen.
   * Reihenfolge: Erst verschiedene Seeds, dann ohne Web.

   * Erster Run abgebrochen: War doch langsamer als erwartet. Es waren nur 4 statt 6-7 Splits (wie gedacht) fertig.
   * Stattdessen: Neue Hyperparam Suche, zuerst mit grober Parameter Range.
   * Tabelle mit allen durchgeführten Runs: https://docs.google.com/spreadsheets/d/1ubONdAuSj-68geyZqEFASQaC2q6djoHIUy-wUZELYyY/edit?usp=sharing
      * Zu beachten: Getestete Hyperparam Ranges sind auf der 2. Seite (seperates Datenblatt) 
   * Max. Seq. Length 128 statt 64 funktioniert etwas besser: ~0.7 pearson correlation auf dev set.

---+++ 04.06.2019
   * Mehrere Seeds testen!
   * Nochmal Hyperparam Range neu auslegen: Hidden, Learning Rate, Batch Size.
   * Datenanalyse: Score Histogramm (gesamt und über die 8 Themen, unterschieden nach web und stab).
   * Binär Klassenverteilung, wenn man die Scores bei 0, 0.2, 0.4, 0.6 etc. trennt. Tabelle mit Similar/Not Similar/Threshold.
   * Google Spreadsheet aufsetzten und im wiki verlinken:
      * Experiment Runs(nicht jeden einzelnen Parameter, aber für die aktuelle Exp Reihe: Cross Topic: Für jeden Split: Was ist die beste Param Config und die Ranges, Architektur, Welches Embedding (BERT Model Typ)), Eval Results: Pearson + Spearman Correlation + Loss, Dev und Test.
   * Erstmal neue Experimente starten, dann Datenanalyse.
   * Grid Search erstmal grob testen, dann feiner werden. Evt. random search. Zeitfenster: Ca. 1 Woche.

   * Assignments von Rejecteten Workern nachgeholt. Split-Half Reliability etwas besser, aber für so niedriges Agreement sind mehrere Trials (eher 25 als 10) nötig. 
   * Cross Topic BERT: Je nach Split Pearson Correlation von 0.65 bis 0.83 auf dem Dev Set; Experimente laufen noch. Muss noch auf der Test Topic getestet werden.
      * Eventuell sollte der Suchraum angepasst werden: Komplexeres Model oder niedrigere Learning Rate.
      * Random search statt grid search (wie aktuell)?   

---+++ 28.05.2019
   * Fehlende Assignments nachholen, Error Report HITs angucken, Scores + Assignments neu machen, Split Half Reliability nochmal berechnen, BERT cross topic mit 1 Thema.
   * JD Corpus (Scores + arguments) schicken.

---+++ 21.05.2019
   * Batches aufsplitten: Generieren die HITs normal (für beide Themen), aber in 2 getrennten Batches hochladen. Hälfte zufällig samplen. Name ändern, z.b. mit Zahl als Suffix.
   * Split Half Reliability, manual checking (Mal scores generieren lassen, 1 Argument mit anderen Partnern...), Mal Correlation mit BERT Predictions machen.
   * Darf grosszügiger blocken: Wenn Captcha Score zu niedrig ist oder die (Best,Worst) Indices komisch verteilt sind. 
   * 3. Batch kann sofort deployed werden, andere Assignments accepten, Wenn Analyse auf dem 2. Batch schlecht sind, können wir ggf. ein 5. Assignment auf dem 1. Batch machen oder zweifelhafte Worker rauswerfen.
   * Evt. mal Assignment Duration auf 10 Min.
   * Data Cleaning erfolgreich umgesetzt: Paare, die länger als 500 Zeichen sind, HTML Tags enthalten oder eine Edit Distance < 4 haben, werden entfernt.
   * Es ergibt sich eine neue Paarauswahl; im Grunde aber ähnlich verteilt wie vorher.
   * Qualification wurde neu erstellt; Nebenbei wurde ein Bug gefixt, weil versucht wurde, die Qualification vom alten Account zu updaten. Jetzt werden nur Qualifications vom Requester Account selbst benutzt.
   * Batch 1 ist live.
   * Performance beim HIT abrufen von mturk verbessert.
   * Batch 2 hat sehr niedriges Agreement. Worker, die viele HITs gemacht haben, haben auch hohe Fehlerrate. Aber die Test HITs sind nicht eindeutig. 
   * Es kommt mehrmals vor, dass es 2 Argumente mit selbem Aspekt, aber unterschiedlichen Stances gibt. Guidelines anpassen?


---+++ 14.05.2019
   * Es gibt einige Duplikat Argumente im Corpus, zur Reproduzierbarkeit wird nur das 1. (nach Hash) gespeichert. Leicht verschiedene URLs, aber der selbe Satz.
   * Code für die Auswahl der Paare ist fertig und scheint gut zu laufen.
   * Wordnet Fehler treten bei Stab/Gun Control und bei Web/Allen Themen auf.
      * Paare, bei denen eine Exception auftritt, werden ausgelassen (~1-2%)
   * Crowdsourcing: Selbe Parameter wie bei der Bachelorarbeit. 1 Thema pro Batch
   * BERT Predictions können evt. zur Qualitätskontrolle einzelner Worker benutzt werden.
   * TODO: 
      * Data Cleaning? z.B. HTML Tags entfernen. Ja, Argumente mit HTML Tags entfernen!
      * Paare, die Wort für Wort gleich sind, entfernen? Ja, nltk Edit distance, min. dist: 3
      * Expert HITs einbauen?
      * Topic description? Ja!
         * Cloning: human cloning.
      * Mturk Sandbox:  2 Themen auf einmal deployen.
   * Vorgehen: 
      * Zuerst mal versuchen, zu deployen.
      * Dann Data Cleaning, 
      * Topic Beschreibung
      * Qualification Test muss neu erstellt werden!
      * Live deployment

---+++ 07.05.2019

   * Welches Model zum Predicten nehmen: Was auf Misra Daten trainiert wurde
   * Auswahl der Paare: ca. 350 Paare pro Topic aus Stab Corpus, 75 Paare Webforum
   * 50% ziehen wir aus dem HS Bucket => HS Bucket braucht mindestens 175 Paare, sagen wir Top 1% nach Predictions
   * 2-50% Some Similarity, Rest No Similarity
   * 25% SS, 25% NS
   * Verteilung überprüfen mit BERT. Stichprobe, z.B. ein Thema.
   * Mit festen Seeds arbeiten (auch beim Ziehen der Paare)
   * Zum Crowdsourcing selber: Mal nachsehen, wie viele HITs es pro Topic werden. Es kann sein, dass wir 1 Batch pro Topic brauchen. Evt. trotzdem HITs mit verschiedenen Topics mischen.

---+++ 30.04.2019

   * Vorgehen: Histogramme der Predictions per Email schicken, dann überlegen wir, wie die Sätze ausgewählt werden sollen
   * Score-Verteilung überprüfen mit BERT Model
   * Crowdsourcing, evt. muss mir JD neue Keys für AWS schicken.
   * Scripts für Crowdsourcing: Extra Infos müssen noch mit gespeichert werden (stance, urls, hash für jeden Satz).

   * Takelab STS wird jetzt jedes Paar in-memory übergeben.
   * Progress Logs alle 1000 Paare hinzugefügt.
   * Geschwindigkeit: 10 000 Paare in ~ 10 min. mit 10 Workers (frisch gestartet)
      * Möglich, dass die Prozesse bei längerer Laufzeit gedrosselt werden (besonders der Java / Stanford Tagger Prozess).
   * Death Penalty Feature Extraction hat noch Fehler
   * Misra System zu trainieren funktioniert, der beste Estimator wird gespeichert/gepickled (so hat Joy wahrscheinlich predicted)
      * Geringe Anpassungen für die neue Version von sklearn waren nötig. 
      * Potenzieller Bug behoben: Feature Spalten waren nicht alphabetisch sortiert; jetzt ist die Feature Reihenfolge garantiert gleich.
      * Wir können das beste Model nehmen, das jeweils auf Death Penalty, Gun Control und auf allen drei Topics zusammen trainiert wurde.
   * Predictions der Paare (selbst >100k) dauern nur wenige Sekunden.
   * Was genau mit den Predictions speichern?
      * Wahrscheinlich nicht sinnvoll: Sämtliche Features (wo2vec1,wo2vec2,...600 etc.)
      * Notwendig: topicName, argument1, argument2, prediction 
      * Wahrscheinlich sinnvoll: 
         * hash1, hash2 für Reproduzierbarkeit. Wird auch die Quell-URL für jedes Argument benötigt? Ja!
         * stance1, stance2
         * retrievedUrl, archivedUrl

---+++ 23.04.2019
   * Ca. 10% der Paare zufällig ziehen, damit die Feature Extraction schneller geht.
   * Takelab STS von Dateibasiert auf in-memory ändern (wenn in vertretbarem Zeitaufwand machbar)
   * Fortschritt Logs in Feature Extraction einbauen!
   * SVM hat Gamma und C Hyperparameter. Sollte ich aus der `results.csv` übernehmen können (oder nach dem Trainieren der SVM direkt die unbekannten Paare predicten).
   * Für die Zukunft: BERT mit mehreren vorgegebenen Seeds testen; mind. 3 Seeds. Besten Seed auf dem Dev Set dann zum Testen benutzen.
   * Plan: 
      * Features generieren(in absehbarer Zeit, ggf. mit weniger getesteten Paaren)
      * Mit Misra predicten
      * Paare auswählen wie vorher besprochen
      * Mit BERT Sanity Check der Klassenverteilung machen
      * Crowdsourcing
   * Nochmal ein Update via Email diese Woche schicken. 

   * Es müssen sehr viele Paare pro Topic getestet werden: Abortion hat 1 127 251 Paare (nur Stab, noch kein Webforum Corpus); Alle Topics (Paare nur als Text, noch nicht als Features) haben zusammen 3.1 GB Dateigröße als CSV.
   * Resultierende Feature Dateien werden sehr groß, deshalb: Nur Word2Vec und Cosinus wie im Misra Paper, kein GloVe.
   * Code von Joy wurde optimiert: Cache für Sätze, Multiprocess.
   * Trotzdem dauert die Feature Extraction ewig; läuft gerade auf dem lisa Server.
      * Feature Extraction für 1 Paar dauert ca. 1 Sekunde; Läuft mit 10 Prozessen; Also nur für Abortion erwarte ich ca. 24-31h Rechenzeit.
      * Cache für Sätze wird das evt. etwas bescheunigen. Mehr Prozesse erlaubt?

---+++ 11.04.2019
   * 8 Themen für das Paper
   * Pro Topic: Paare samplen mit Misra
   * Nicht einfach bei einem Score Threshold abschneiden.
   * Vorwiegend hohe Predictions, ein Paar aus der Mitte und niedrige
   * Auswahl der Paare: Zufällig ziehen aber mit vorgegebenen Seed. Score wird in Wahrscheinlichkeit, das Paar zu ziehen umgerechnet.
   * JD schickt mir den Corpus

   * Timesheets (Originale) abgeben!
   * Alle möglichen Splits mit 2 Test Topics ergeben fast 150 zu testende Paare. Mit 3 Topics sind es noch mehr.
   * Daher: Einfache Splits, sodass jedes Thema genau einmal getestet wird. Es werden aber nicht alle möglichen Paare beachtet. Das ergibt 6 Splits mit je 3 Test Topics.
   * Ergebnis vom besten BERT Regression Model auf diesen einfachen Splits:
      * selbe Hyperparameter, die auf dem maLSTM Split ermittelt wurden (auf Teilmenge von allen Themen).
      * Trotzdem bekommt das Model jetzt nur einen Teil der Themen als Trainingsdaten. Die Test Topics sind "ungesehen".
   * Von Menschen lesbare Predictions und Plots: [[%ATTACHURLPATH%/bert.regression.best.simple-splits.zip][bert.regression.best.simple-splits.zip]]  
%EDITTABLE{format="| text,20| text,20| text,20| text,20|"}%
| *Split#* | *Test Topic* | *Pearson R* | *Spearman R* |
| 0.  | Fracking | 0.50 | 0.48 |
| 0.  | Gmo | 0.62 | 0.56 |
| 0.  | Electronic voting | 0.46 | 0.46 |
| 1 | Electric cars | 0.65 | 0.63 |
| 1 | Organ donation | 0.68 | 0.69 |
| 1 | Recycling | 0.76 | 0.77 |
| 2 | Offshore drilling | 0.53 | 0.51 |
| 2 | Hydrogen fuel cells | 0.72 | 0.67 |
| 2 | Big data | 0.77 | 0.77 |
| 3 | Hydroelectric dams | 0.72 | 0.72 |
| 3 | Robotic surgery | 0.76 | 0.76 |
| 3 | Net neutrality | 0.67 | 0.66 |
| 4 | Stem cell research | 0.72 | 0.73 |
| 4 | Geoengineering | 0.60 | 0.54 |
| 4 | Internet of things | 0.59 | 0.54 |
| 5 | Nanotechnology | 0.74 | 0.73 |
| 5 | Solar energy | 0.71 | 0.70 |
| 5 | Genetic diagnosis | 0.74 | 0.74 |


---+++ 05.04.2019
   * Cross-topic Experimente: 2 Themen zum Testen rauslassen, Dev Split prozentual von den Trainingsdaten.
   * Einmal mit jetzigen besten Hyperparams. Neue Optimierung für jeden Split.
   * Misra zum laufen bekommen.
   * Wenn misra lauft, dann crowdsourcing.
   * Wahrscheinlich nur 8 Themen, dann aber mehr Paare.

   * BERT Regression Model sieht vielversprechend aus: Bestes Model ohne hidden Layer bzw. das nur eine Linearkombination des BERT Outputs bildet:
      * Global Pearson/Spearman ~ 0.75 auf Dev Set.
   * Wichtig: Evaluation bisher als Correlation über dem kompletten Dev/Test Set (global). In der Bachelor Arbeit/maLSTM wurde aber Avg Correlation pro Topic getestet.
   * Damit die Ergebnisse vergleichbar sind, werden die bisherigen Tests mit `bert.regression` wiederholt. 
      * Evt. macht es aber für das Konferenz-Paper mehr Sinn, einfach die Correlation über dem kompletten Set zu bilden, weil es einfacher von anderen Teams anwendbar ist.
      * Ansonsten kann es Probleme mit der Avg Berechnung geben (Fisher Z-Transformiert oder nicht)
   * Bestes Regression Model mit zusätzlichem hidden Layer: 680 hidden Nodes, Learn Rate = 1.3e-5, Batch Size = 32 .
   * Zum Vergleich: Die maLSTM aus der Bachelor Arbeit hat Spearman pro Topic von 0.65 erreicht.
Correlation der Predictions mit echten Scores auf dem Test Set (maLSTM Split):
| *Modus* | *pearsonR* | *spearmanR* |
| Global | 0.800 | 0.803 |
| Pro Topic | 0.816 | 0.801 |
   * Correlation pro Topic Plot und Predictions (für Menschen lesbar): [[%ATTACHURLPATH%/bert-regression-best.zip][bert-regression-best]]
   * Prediction Histogramm: %IMAGE{"bert.regression.prediction.hist.png" caption="bert-regression-prediction-hist" size="200"}%
   
   * Misra: Immer noch Fehler bei `Rouge/FeatureRouge.py`: 
      * Ich verwende diese Library (wird auch von `pip install rouge` benutzt): https://github.com/pltrdy/rouge
      * Problem beim Code von Joy: rouge Implementierung hat sich geändert (oder ich benutze die falsche).
      * Was im Code wahrscheinlich erwartet wird: Numerischer Rouge Score.
      * Was die Library aktuell ausspuckt: Dictionary mit `{"f": _, "p": _, "r": _}` Einträgen
      * Außerdem ist Rouge Score auf dem Sentence Level nicht implementiert.
      * Welchen Wert soll ich benutzen (f/p/r?)
   * Ansonsten scheint alles (Java- und Python-Teil) zu funktionieren.

---+++ 28.03.2019
   * Wenn überhaupt mehr Tests mit Labels dann nur 1 Stunde investieren
   * Nächster Schritt: BERT Regression Model direkt mit den Scores (statt diskreten Labels)
      * Evaluation: Wieder Correlation: Spearman/Pearson. Vergleich mit maLSTM.
      * Hyperparam Optimierung: Hidden Nodes, Learning Rate, Batch Size.
      * Weitere Varianten testen: 
         * Topic Split fuer train/dev/test benutzen. Z.b. 6 topics train/dev und 2 test.
         * Topic als Feature? Z.B. appenden vorne an jeden Satz.
   * Arg Sim Model für Konferenz (etwa bis Mai):
      * Daten: 8 Themen, die oeffentlich sind. Argumente sind schon annotiert. Thema, Stance (pro/con/no argument).
      * Misra System zur Vorauswahl der Paare. Müsste off the shelf laufen
   * UKP-intern: Dann spaeter ca. 500 Paare pro Topic mit Daten aus ArgumenText.
   
   * BERT mit diskreten Labels (0/1):
   * Fuer Threshold 0 ist die Verteilung der Klassen fast gleich, aber in der Praxis ist dieser Threshold nicht sehr nuetzlich.
   * Daher wird jetzt Threshold 0.25 getestet (ca. 25% der Daten sind damit Label 1 bzw. similar), Score Range von -1 bis 1.
   * Test setup: BERT model Parameter unveraendert. 10 Trainings Epochen. BertAdam Optimizer.
      * Daten: BWS Corpus, Splits von maLSTM. Wichtig: Score range: von 0 bis 1, daher muss threshold im Code gleich 0.625 sein.
      * Hyperparam Range:
         * learnRate = 1 0.1 0.01 0.001 0.0001 2e-5
         * batchSize = 16 32
   * Hohe Learning Rate mit Adam (lr=1) sorgt dafuer, dass nur 1 Klasse predictet wird (scheinbar), da Accuracy genau den Klassenanteilen entspricht.
   * Learning Rates kleiner 0.0001 funktionieren gut. Neue Hyperparam Range:
      * learnRate = 6e-6 8e-6 10e-6 12e-6 14e-6
      * batchSize = 16 32
   * learnRate = 8e-6 funktioniert am Besten unabhängig vom Threshold. 
   * Bester F1 Score der Klasse "1" nach Threshold (auf maLSTM Score Range, also von 0 bis 1) auf dem Dev Set:
      * Labelled Similar: Anteil der Daten mit Label "1" auf dem kompletten Corpus  
%EDITTABLE{format="| text,20| text,20| text,20| text,20| text,20| text,20|"}%
| *Threshold* | *Dev F1* | *Labelled Similar* | *Test F1* | *Test P* | *Test R* |
| 0.5 | 0.79 |47% |0.79 |0.78 |0.80 |
| 0.625 | 0.70 |27% |0.68 |0.66 |0.70 |
| 0.66 | 0.69 |24% |0.64 |0.63 |0.65 |

---+++ 21.03.2019
   * CSV einlesen für BERT Format funktioniert scheinbar.
   * Score Threshold t für Konversion zu 0/1 Labels:
      * t=0.49 ergibt 244 similar Paare (ca. 12% vom Corpus)
      * Statistiken pro Topic:
         * Mean: 13.56
         * Std: 3.84
         * Min: 6
         * Max: 20
   * Threshold 0 und 0.25 mal testen als Threshold. Some Similarity waere in dem Sinne schon noch 1/similar
   * Train/Dev/Test Sets: Das Dev Set nicht anfügen ans Train Set
   * Seed mal konstant setzen
   * Hyperparam Tuning: Batch Sizes, Learning Rate
   * Zugang zu Lisa zum laufen bringen
   * Nächster Schritt: Übergang zu real valued scores.
   
---+++ 14.03.2019
   * Pearson Correlation ist leider ungeeignet als Loss Function (nicht differenzierbar?), Predictions werden NaN
   * Hyperparameter Optimization: 1 hidden LSTM layer, 2 hidden LSTM layer + return_sequences. Bias/initalization. Bidirectional.
   * Bidirectional reduziert Bias auf letzte Wörter im Satz.
   * Bestes Deep Model: (27, 27), bidirectional, initializer: zeros, unitForgetBias=False
   * Custom MSE: Bestraft Fehler härter, je weiter sie von 0.5 (mittlerer Score) entfernt liegen.
      * Zieht die Predictions auf unknown Daten (Funkstandard) weiter auseinander. Trotzdem sind die Predictions nicht besonders gut.
   * Mit den aktuellen Daten / Models ist maLSTM nicht gut genug, um ähnliche Argumente auf komplett unbekannten Themen zu finden.
   * => BERT: Pretrained Transformer, der kontext-abhängige Wort Vektoren liefert.
      * 1. Schritt: Daten Parser anpassen
         * Argumente aus der CSV in das BERT-Format bringen: Sentence1, Sentence2, Topic
         * Scores zu binären Labels konvertieren: 0/1
         * => Threshold?
      * Mittelfristig BERT mit unseren Scores benutzen 

---+++ 07.03.2019
   * Bidirectional LSTM: Prediction von 250k Paaren dauert jetzt etwa doppelt so lange (ca. 5 min 30 sec). Bei selber Trainingsdauer keine Verbesserung der Predictions.
   * Padding umdrehen hilft nichts - jetzt matcht die LSTM nur den Anfang des Satzes
   * Grund fuer das Verhalten (wahrscheinlich): 
      * Beispiele in den Trainingsdaten sind eher kurz und der input bias wurde komplett mit 0 initialisiert 
      * (== nicht genug Daten, um bias fuer laengere Saetze zu lernen)
      * Daher wird der Beginn des Satzes nicht gewichtet.
   * Neuer Test mit Bias = 1 bringt kaum Verbesserung
   * Verschiedene Architekturen: 
      * LSTM mit weniger Nodes overfittet nicht auf die letzten Worte
      * Deep LSTM mit return_sequences=True und z.B. (20,20) Nodes funktioniert auch gut
      * Neues Hyperparameter Tuning mit Correlation als Metrik?
   * Bias im Gold Standard: Training und Dev Set ueberprueft:
      * Es gibt einige Beispiele mit hoher Similarity (>0.9) und gleichem Suffix
      * Aber nicht mehrheitlich, d.h. genug Beispiele mit unterschiedlichem Suffix
      * Mehr Trainingsdaten sammeln?
   * Neue Trainingsdaten:
      * Argumente aus statischem Web Crawl
      * Mit existierendem System (maLSTM) ähnliche Argumente finden
      * Bis dahin: Neue Hyperparameter Optimization!
      * Ergebnisse vor dem nächsten Meeting schicken (Correlation und Beispiele)
      * Predictions auf dem Gold Test Set überprüfen!

---+++ 26.02.2019
   * Predictions liegen selten über 0.6
   * Argumente, die nicht Wort für Wort gleich sind haben bis zu 0.85
   * Threshold schwierig zu setzen: 
      * Bei 0.7 wären es nur etwa 40 Paare
      * Um 0.6 hohe Fehlerquote
   * Pearson Correlation als Metric auf Validation Set scheint vielversprechend:
      * Correlation verbessert sich noch, obwohl der min loss sich nicht grossartig ändert.
   * Problem: Letzten Wörter im Satz werden übergewichtet
      * 0-padding umdrehen?
      * Trainingsdaten gebiased? Also auch immer selbe Worte am Ende
      * mal bidirectional LSTM testen
   * Generell: Irgendwann auch cross-topic ähnliche Paare finden.

---+++ 19.02.2019
   * Funkstandard Argumente: Etwas noisy, quasi-duplikate vorhanden, teilweise andere Sprache (kyrillisch)
   * Paare per brute-force innerhalb der 13 Themen generiert: 250k
   * Embedding Layer in der maLSTM von meiner Bachelor Arbeit ungeeignet für unbekannte Argumente
   * Prediction mit maLSTM trainiert auf arg-sim-bws Corpus: 
      * Extreme (0 und 1) machen Sinn, aber sind relativ uninteressant
      * 0: Normale Argumente mit kyrillischen Zeichen
      * 1: Argumente, die bis auf wenige Zeichen identisch sind
   * Ziel: Paare finden, die nicht genau die gleichen Wörter haben, aber immer noch sehr ähnlich sind.
      * Entweder einen Prediction Threshold setzen oder automatisch suchen mit Substring-Maßen 

-- Main.JohannesDaxenberger - 2019-02-20

%META:FILEATTACHMENT{name="bert-regression-best.zip" attachment="bert-regression-best.zip" attr="" comment="" date="1554244329" size="96173" user="nothvogel" version="1"}%
%META:FILEATTACHMENT{name="bert.regression.prediction.hist.png" attachment="bert.regression.prediction.hist.png" attr="" comment="" date="1554245137" size="18151" user="nothvogel" version="1"}%
%META:FILEATTACHMENT{name="bert.regression.best.simple-splits.zip" attachment="bert.regression.best.simple-splits.zip" attr="" comment="" date="1554739910" size="181715" user="nothvogel" version="1"}%

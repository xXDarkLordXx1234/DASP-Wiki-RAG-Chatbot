%META:TOPICINFO{author="thakur" comment="" date="1576772023" format="1.1" reprev="129" version="129"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! %TOPIC%

---++ Arbeitsstunden

<details>
  <summary><p>Historie (click to show)</p></summary>
%EDITTABLE{format="| text,20| text,20| text,20| text,20| text,20|"}%
| *Datum* | *Beginn* | *Ende* | *Täglich (hh:mm)* | *Notiz* |
|  | 10:00 | 16:00 | 06:00 |Quali Test |
|  |  |  | 07:00 |Arg Sim / Funkstandard |
| | | |06:00 |Arg Sim / Funkstandard |
| | | |01:00 |Arg Sim / Funkstandard |
|06.03.2019 | | |02:00 |Arg Sim / Funkstandard |
|07.03.2019 | | |03:30 |Arg Sim / Funkstandard |
|08.03.2019 | | |08:00 |Arg Sim / Funkstandard |
|11.03.2019 | | |03:00 |Hyperparam Optimization |
|12.03.2019 | | |02:30 |Final Model |
|13.03.2019 | | |01:00 |Custom MSE, BERT |
|14.03.2019 | | |00:30 |Meeting, Notizen |
|20.03.2019 | | |01:00 |BERT |
|21.03.2019 | | |02:00 |BERT |
|22.03.2019 | | |02:00 |BERT |
|25.03.2019 | | |04:00 |BERT |
|26.03.2019 | | |02:00 |BERT |
|27.03.2019 | | |04:00 |BERT |
|28.03.2019 | | |02:30 |BERT |
|29.03.2019 | | |02:00 |BERT |
|01.04.2019 | | |05:00 |BERT |
|02.04.2019 | | |07:00 |BERT |
|05.04.2019 | | |02:00 |BERT |
|08.04.2019 | | |03:00 |BERT |
|09.04.2019 | | |01:30 |BERT |
|11.04.2019 | | |01:30 |BERT |
|12.04.2019 | | |01:00 |BERT/Misra |
|16.04.2019 | | |01:30 |BERT/Misra |
|17.04.2019 | | |05:30 |BERT/Misra |
|23.04.2019 | | |04:00 |BERT/Misra |
|25.04.2019 | | |01:00 |BERT/Misra |
|26.04.2019 | | |02:00 |BERT/Misra |
|29.04.2019 | | |03:00 |Misra |
|30.04.2019 | | |02:00 |Misra |
|02.05.2019 | | |01:30 |BERT/Misra |
|03.05.2019 | | |01:00 |BERT/Misra |
|06.05.2019 | | |05:30 |BERT/Misra |
|07.05.2019 | | |01:00 |BERT/Misra |
|08.05.2019 | | |02:30 |BERT/Misra |
|09.05.2019 | | |01:30 |BERT/Misra |
|10.05.2019 | | |02:00 |BERT/Misra |
|13.05.2019 | | |04:30 |BERT/Misra |
|14.05.2019 | | |05:30 |Crowdsourcing |
|15.05.2019 | | |03:30 |Crowdsourcing |
|16.05.2019 | | |03:00 |Crowdsourcing |
|17.05.2019 | | |02:00 |Crowdsourcing |
|20.05.2019 | | |01:30 |Crowdsourcing |
|21.05.2019 | | |01:00 |Crowdsourcing |
|22.05.2019 | | |01:00 |Crowdsourcing |
|23.05.2019 | | |02:30 |Crowdsourcing |
|24.05.2019 | | |00:30 |Crowdsourcing |
|(27.05.2019) | | |02:00 |Crowdsourcing |
|(28.05.2019) | | |01:00 |Crowdsourcing |
|(29.05.2019) | | |00:30 |Crowdsourcing |
|(30.05.2019) | | |01:00 |Crowdsourcing |
|(31.05.2019) | | |01:00 |BERT |
|03.06.2019 | | |02:00 |BERT |
|04.06.2019 | | |01:00 |BERT |
|05.06.2019 | | |02:00 |BERT |
|06.06.2019 | | |02:00 |BERT |
|11.06.2019 | | |01:00 |BERT |
|12.06.2019 | | |01:00 |BERT |
|13.06.2019 | | |01:00 |BERT |
|17.06.2019 | | |03:00 |BERT |
|18.06.2019 | | |01:00 |BERT |
|19.06.2019 | | |01:00 |BERT |
|21.06.2019 | | |01:00 |BERT |
|24.06.2019 | | |02:00 |BERT |
|25.06.2019 | | |02:00 |BERT |
|26.06.2019 | | |03:00 |BERT |
|27.06.2019 | | |04:00 |BERT |
|28.06.2019 | | |02:00 |BERT |
|03.07.2019 | | |01:30 |Clustering |
|04.07.2019 | | |02:00 |Clustering |
|05.07.2019 | | |03:30 |Clustering |
|24.07.2019 | | |02:00 |Clustering |
|25.07.2019 | | |03:00 |Clustering |
|31.07.2019 | | |02:40 |Clustering |
|01.08.2019 | | |01:30 |Clustering |
|02.08.2019 | | |01:00 |Clustering |
|05.08.2019 | | |02:00 |Clustering |
|07.08.2019 | | |00:30 |Clustering |
|08.08.2019 | | |01:30 |Clustering |
|09.08.2019 | | |00:30 |Clustering |
|12.08.2019 | | |02:00 |Clustering |
|14.08.2019 | | |02:30 |Clustering |
|15.08.2019 | | |02:30 |Clustering |
|16.08.2019 | | |01:00 |Documentation |
|19.08.2019 | | |02:00 |Documentation |
|22.08.2019 | | |02:00 |Clustering |
|28.08.2019 | | |01:00 |Documentation |
|29.08.2019 | | |02:30 |Clustering |
|30.08.2019 | | |02:30 |Clustering, Doc. |
|02.09.2019 | | |01:00 |Frontend |
|03.09.2019 | | |03:00 |Frontend |
|04.09.2019 | | |03:00 |Frontend |
|05.09.2019 | | |01:00 |Frontend |
|06.09.2019 | | |02:00 |Frontend |
|09.09.2019 | | |04:00 |Clustering |
|10.09.2019 | | |02:00 |Clustering |
|11.09.2019 | | |01:00 |Clustering |
|13.09.2019 | | |02:00 |Clustering |
|18.09.2019 | | |02:00 |Clustering |
|19.09.2019 | | |01:00 |Clustering |
|24.09.2019 | | |02:00 |vuejs |
|26.09.2019 | | |04:00 |Clustering |
|30.09.2019 | | |06:00 |Clustering |
|06.10.2019 | | |00:30 |Clustering |
|08.10.2019 | | |01:00 |Clustering |
|09.10.2019 | | |02:00 |Clustering |
|10.10.2019 | | |01:30 |Clustering |
|17.10.2019 | | |01:30 |Clustering |
|18.10.2019 | | |01:00 |Clustering |
|21.10.2019 | | |03:00 |Clustering |
|22.10.2019 | | |02:00 |Clustering |
|23.10.2019 | | |04:00 |Clustering |
|24.10.2019 | | |03:00 |Clustering |
|25.10.2019 | | |04:00 |BERT |
|28.10.2019 | | |01:30 |BERT |
|29.10.2019 | | |02:30 |BERT |
|30.10.2019 | | |03:00 |BERT |
|31.10.2019 | | |01:00 |BERT |

Summe Mai: ~ 40:00
Voll ab 24.05.

Summe Juni: ~30:00; 10h Überstunden vom Februar sind damit ausgeglichen.

Summe Juli: 14:30, mit restlichen Ueberstunden: 30:00

Summe August: 25:00

Summe September: 34:00

Summe Oktober: 31:30

Noch nachzuarbeiten: 10h (Juli) + 15h (August) + 6h (September) + 8h 30 min (Oktober)
= 39:30
Minus 25h Urlaub
== 14:30
</details>

---+++ Aktuell
%EDITTABLE{format="| text,20| text,20| text,20| text,20| text,20|"}%
| *Datum* | *Beginn* | *Ende* | *Täglich (hh:mm)* | *Notiz* |
|01.11.2019 | | |02:30 |BERT |
|06.11.2019 | | |02:20 |BERT |
|11.11.2019 | | |01:20 |BERT |
|12.11.2019 | | |01:40 |BERT |
|15.11.2019 | | |01:10 |Crowdsourcing |
|25.11.2019 | | |01:10 |Crowdsourcing |
|26.11.2019 | | |03:00 |Crowdsourcing |
|28.11.2019 | | |01:30 |Crowdsourcing |
|29.11.2019 | | |01:45 |Crowdsourcing |
|02.12.2019 | | |00:30 |Crowdsourcing |
|03.12.2019 | | |02:00 |Crowdsourcing |

Stand: inklusive 03.12.2019

---++ Crowdsourcing

   * Topics:

%EDITTABLE{format="| text,20| text,20| number| number|"}%
| *name* | *description* | *batchNr* | *numOfPairs* |
| Fracking | forcing fractures in a rock layer to extract oil and natural gas | 1 | 146 |
| Electronic voting | using systems that cast or count votes electronically | 1 | 145 |
| Gmo | genetically modified organisms i.e. crops whose genetic material has been altered | 1 | 149 |
| Recycling | converting waste materials into new materials and objects | 2 | 145 |
| Organ donation | giving an organ away for transplantation or research after death | 2 | 145 |
| Electric cars | vehicles that use electricity to move | 2 | 149 |
| Hydrogen fuel cells | use hydrogen fuel to generate electricity and only emit water as a by-product | 3 | 145 |
| Big data | extracting valuable information from very large datasets | 3 | 148 |
| Offshore drilling | drilling a borehole below the seabed to extract oil or natural gas | 3 | 145 |
| Robotic surgery | using robotic systems to assist in surgery | 4 | 143 |
| Hydroelectric dams | damming a river to generate electricity with water turbines | 4 | 143 |
| Net neutrality | principle that internet providers treat all data equally | 4 | 151 |
| Geoengineering | intervening in the Earth's climate system | 5 | 158 |
| Internet of things | connecting physical devices and everyday objects via the internet to exchange data | 5 | 153 |
| Stem cell research | using embryonic or adult-derived stem cells for medical research | 5 | 145 |
| Solar energy | using the energy of the sun to generate electricity or heat | 6 | 156 |
| Nanotechnology | manipulating matter on a atomic or molecular scale | 6 | 157 |
| Genetic diagnosis | testing if a person is likely to develop genetic disorders or diseases | 6 | 152 |

   * Pair selection:
      * BERT Predictions labelled as: Top 1%: HS, Top 2-20%: SS, Top 21-100% NS.
      * Pairs for new corpus sampled from these label buckets: 25% HS, 25% SS, 50% NS.
      * Per topic: about 150 pairs

   * Batches:

%EDITTABLE{format="| number| number| number| number| number| number| number| number| number| number| number| number| number| text,20| number| "}%
| *number* | *#pairs* | *bwsFactor* | *maxAssignments* | *#HITs* | *#assignments* | *#cheaters* | *$reward* | *$total* | *$perPair* | *bestAgree* | *worstAgree* | *bestWorstAgree* | *topics*                                                     | *run time (hours)* |
| 1        | 440      | 1.75         | 4                | 855     | 3420           |      1      | 0.20      |     | 1.85       |    0.70    |    0.65     |      0.51       | Fracking, Electronic voting, Gmo                       | < 24 |
| 1.1        |       |          |                 | 230     | 230           |     2      | 0.20      |     |        |        |         |             | Fracking, Electronic voting, Gmo                       | < 24 |
| 1.2        |       |          |                 |   191   |      191      |     0      | 0.20      |     |        | 0.72       | 0.66        | 0.53            | Fracking, Electronic voting, Gmo                       | < 24 |
| 2        | 439      | 1.75         | 4                | 851     | 3404           |      2      | 0.20      |     | 1.85       |    0.66    |    0.61     |      0.46       | Recycling, Organ Donation, Electric Cars                       | < 12 |
|2.1 | | | | | | |0.20 | | |0.68 |0.62 |0.48 |Recycling, Organ Donation, Electric Cars |<12 |
|3 |438 |1.75 |4 |849 |3396 |1 |0.20 | | |0.62 |0.60 |0.44 |Hydrogen Fuel Cells, Big Data, Offshore Drilling |<48 |
|3.1 | | | |227 |227 | |0.20 | | |0.64 |0.61 |0.46 |Hydrogen Fuel Cells, Big Data, Offshore Drilling |< 2 |
|4 |437 |1.75 |4 |851 |3404 |0 |0.20 | | |0.71 |0.63 |0.50 |Robotic Surgery, Hydroelectric Dams. Net Neutrality |< 5 |
|5 |456 |1.75 |4 |883 |3532 | |0.20 | | |0.64 |0.63 |0.46 |Geoengineering, Internet of Things, Stem Cell Research |< 5 |
|6 |465 |1.75 |4 |897 |3588 | |0.20 | | |0.67 |0.63 |0.48 |Solar energy, Nanotechnology, Genetic Diagnosis |< 5 |
| *SUM*    | %CALC{"$SUM( $ABOVE() )"}%   |           |                |       |              |        |          | %CALC{"$SUM( $ABOVE() )"}%       |          | | | | ||


   * Worker Block List:

%EDITTABLE{format="| text,20| text,20| text,12| text,6|"}%
| *workerId* | *reason* | *runName* | *rejected* |
| A2Z8V4Q3XIU7HC | Extremely high error rate on easy HITs (>70% of votes differing). | batch-1-live | True |
| AWLT46A9EWTK8 | Not in line with majority (>30% of votes differing) or bad captcha performance. | batch-1-live | False |
| A1ET2J1PIP0RGO | Not in line with majority (>30% of votes differing) or bad captcha performance. | batch-1-live | False |
| A1MFQBQRNANOD6 | Not in line with majority (>30% of votes differing) or bad captcha performance. | batch-1-live | False |
| A2074VMS950Q3V | Not in line with majority (>30% of votes differing) or bad captcha performance. | batch-1-live | False |
| A16WMISN53AP9Q | Not in line with majority (>30% of votes differing) or bad captcha performance. | batch-1-live | False |
| A26PKIZV08W9H6 | Not in line with majority (>30% of votes differing) and weird answer patterns. | batch-1-live | True |
| A4IPFWI1G83KJ | Not in line with majority (>30% of votes differing) and weird answer patterns. | batch-1-live | True |
| AQ92LNNNAUJA0 | Not in line with majority (>30% of votes differing). | batch-1-live | False |
| A9189NV85IC2Y | Not in line with majority (>30% of votes differing). Low time per HIT. | batch-2-live | False |
| A1LOD3LNX7FUPJ | Not in line with majority (>30% of votes differing). | batch-2-live | True |
| A1AS4S7BOJPOEY | Not in line with majority (>30% of votes differing). | batch-2-live | True |
|A20JVRNN0C4H12 |Not in line with majority (>30% of votes differing) and weird answer patterns. |batch-3-live |True |
|A31UT0B85K82RA |Not in line with majority (>30% of votes differing) and weird answer patterns. |batch-3-live |False |
|A2AH0FADJ9PCE5 |Not in line with majority (>30% of votes differing) and weird answer patterns. |batch-3-live |False |
|A2959L16M8OES7 |Not in line with majority (>30% of votes differing) and weird answer patterns. |batch-3-live |False |
|A2NXHJ905WLGAA |Not in line with majority (>30% of votes differing) and weird answer patterns. |batch-4-live |False |
|AW4AHMQ7EN4QI |Not in line with majority (>30% of votes differing) and weird answer patterns. |batch-4-live |False |
|A36CQIFWEDEECU |Not in line with majority (>30% of votes differing) and weird answer patterns. |batch-4-live |False |
|A38Z5T2J8MFBL1 |Not in line with majority (>30% of votes differing) and weird answer patterns. |batch-5-live |False |
|A3C2X1L5PVNNLV |Not in line with majority (>30% of votes differing) and weird answer patterns. |batch-5-live |False |
|A2541C8MY0BYV3 |Not in line with majority (>30% of votes differing) and weird answer patterns. |batch-5-live |False |
|A35CL66BEUSJ5Z |Not in line with majority (>30% of votes differing) and weird answer patterns |batch-6-live |False |


<details>
  <summary><p>Stab/Web Corpus (click to show)</p></summary>
   * Topics:

%EDITTABLE{format="| number| text,20| text,20| number| number|"}%
| *id* | *name* | *description* | *batchNr* | *numOfPairs* |
| 1 | abortion | choosing to end the pregnancy before birth takes place | 1 | 425 |
| 2 | cloning | creating genetically identical copies of humans | 1 | 425 |
| 3 | death penalty | the government killing someone as punishment for a serious crime | 2 | 425 |
| 4 | gun control | restraining people from owning or using guns | 2 | 425 |
| 5 | marijuana legalization | the government allowing recreational use of cannabis | 3 | 425 |
| 6 | minimum wage | increasing least amount of money employees can be paid | 3 | 425 |
| 7 | nuclear energy | using nuclear energy to generate electricity | 4 | 425 |
| 8 | school uniforms | making all students wear a standard set of clothing | 4 | 425 |

   * Pair selection:
      * Misra Predictions labelled as: Top 1%: HS, Top 2-50%: SS, Top 51-100% NS.
      * Pairs for new corpus sampled from these label buckets: 50% HS, 25% SS, 25% NS.
      * Per topic: 350 pairs from stab corpus, 75 one stab + one web argument paired together

   * Batches:

%EDITTABLE{format="| number| number| number| number| number| number| number| number| number| number| number| number| number| text,20| number| "}%
| *number* | *#pairs* | *bwsFactor* | *maxAssignments* | *#HITs* | *#assignments* | *#cheaters* | *$reward* | *$total* | *$perPair* | *bestAgree* | *worstAgree* | *bestWorstAgree* | *topics*                                                     | *run time (hours)* |
| 1        | 850      | 1.75         | 4                | 1638     | 6552           | 0           | 0.20      | 1572.48    | 1.85       |    0.67    |    0.63     |      0.49       | abortion, cloning                       | ~24 |
| 2        | 850      | 1.75         | 4                | 1638     | 6552           | 0           | 0.20      | 1572.48    | 1.85       |    0.58    |    0.57     |      0.41       | death penalty, gun control                       |~20 |
| 3        | 850      | 1.75         | 4                | 1638     | 6552           | 2           | 0.20      | 1572.48    | 1.85       |    0.58    |    0.56     |      0.40       | marijuana legalization, minimum wage                       |~18 |
| 4        | 850      | 1.75         | 4                | 1638     | 6552           | 0           | 0.20      | 1572.48    | 1.85       |    0.59    |    0.58     |      0.42       | nuclear energy, school uniforms                       |~20 |
| 3.1        |       |          |                 |     | ~250           |            |       |  ~60   |        |        |         |             | marijuana legalization, minimum wage                       |~18 |
| *SUM*    | %CALC{"$SUM( $ABOVE() )"}%   |           |                |       |              |        |          | %CALC{"$SUM( $ABOVE() )"}%       |          | | | | ||
| | | | | | | | | | | | | | | |


   * Worker Block List:

| *workerId* | *reason* | *runName* | *rejected* |
| A1JVWK2XKOE3K5 | Bad captchas. Not in line with majority (>30% of votes differing). | batch-1-live | False |
| ASXE94CMR9SVL | Bad captchas. Not in line with majority (>30% of votes differing). | batch-1-live | False |
| AKW57KYG90X61 | Not in line with majority (>30% of votes differing). | batch-1-live | False |
| A2XH3ANYM43OB9 | Not in line with majority (>30% of votes differing). | batch-1-live | False |
| A2VFHTZKUFKG16 | Bad captchas. Not in line with majority (>30% of votes differing). | batch-1-live | False |
| AKBRH661KPW1R | Bad captchas. Not in line with majority (>30% of votes differing). | batch-2-live | False |
| A32DB8CJR9IZ5H | Bad captchas. | batch-2-live | False |
| A218WYOK44NBHQ |  Bad captchas. Not in line with majority (>30% of votes differing). | batch-3-live | False |
| A3N17E39KZOQX0 | Not in line with majority (>30% of votes differing). | batch-3-live | False |
| AITP2LUW8GPB | Not in line with majority (>30% of votes differing). | batch-3-live | False |
| A3A9PARKHSKRWI | Not in line with majority (>30% of votes differing). | batch-3-live | False |
| A38GYK5UQVOCLK | Bad captchas. Not in line with majority (>30% of votes differing). | batch-3-live | False |
| A3FIO5T8LH65DM | Bad captchas. Not in line with majority (>30% of votes differing). | batch-3-live | False |
| A2Z2CVOBYA74B0 | Bad captchas. Not in line with majority (>30% of votes differing). | batch-3-live | False |
| A1Z6XK510SSAMU | Bad captchas. Not in line with majority (>30% of votes differing). | batch-3-live | True |
| AQ8PG3HTV72YT | Bad captchas. Not in line with majority (>30% of votes differing). | batch-3-live | True |

</details>

   * How to interpret Quality Control measures from the worker-check script: 
      * Most workers need between 0:30 and 2:00 median time per HIT (m:ss)
      * Workers with a lot of assignments and some experience (e.g. starting from the second batch) can get as low as 15-30s median time per HIT
         * median lower than 15s probably indicates that something is wrong and the worker should be blocked.
      * Error Rate on 'easy' HITs: Is a good measure if the overall agreement is high (> ~70% for best/worst)
         * In these experiments, the majority choice (75% of votes!) was not always correct. 
         * The average worker has up to 25% error rate.
         * Error rate >= 30% means some HITs/other stats should be checked manually.
      * Captchas correct: Most workers are able to solve 95-100% of captchas correctly.
         * Some mix up best and worst instructions and get a high error rate. If this is a repeated mistake, they might not be the best workers and could be blocked anyways.
         * Always check captcha choices by hand!
         * In the table, bad captchas means <85% correct.

   * Captchas correct, Error on Easy HIT, Time needed per Assignment Reference: TODO

---++ Meetings

---+++ neu
   * Evt. noch meinen clustering-module in den development Branch mergen.
   * Hits Approved 5 000 HITs.
   * Batches nur abends hochladen.
   * Falls nächster Batch wieder so schlecht, mit Whitelist Qualification. Können wir uns hoffentlich sparen.
   * Auswahl der neuen Paare, um Joy Corpus aufzufüllen:
      * Input Distribution: 
         * 1% HS
         * 19% SS
         * 80% NS
      * Target Output Distribution: [[https://wiki.ukp.informatik.tu-darmstadt.de/pub/Hiwi/PhilippNothvogelHiwi/label%20distribution.png][Plot zur Kontrolle]]
         * 25% HS
         * 25% SS
         * 50% NS
      * Paare mit HTML Code oder zu geringer Edit Distance wurden wieder entfernt.
      * Themen aus der Bachelor Arbeit können auf 260 Paare pro Thema aufgefüllt werden:
         * Wir annotieren insgesamt 2675 Paare neu
         * Bei 1.85$ pro Paar kostet das ca. 4950$

---+++ 15.11.2019
   * BERT Model ist bereit, neue Paare zu predicten.
   * Neue Paare enthalten sicher keine 1:1 Duplikate, aber evt. welche mit geringer Edit Distance.
   * Ziel Output Distribution: 50 HS, 50 SS, 100 NS
   * Ziel: Etwa gleich große Themen. Wir rechnen mit 1.85$ pro Paar, etwa 165 Paare Pro Thema, 5000€ / 5000$
   * Alle Themen aus der Bachelor Arbeit nehmen.
   * BERT Ergebnisse: Nochmal abs. Zahl der Testpaare angucken pro Split
   * Paare Ziehen, JD nochmal die Verteilung schicken, dann auf Sandbox deployen.
   * Batches sollen nicht zu groß sein, also nicht mehr als 1000-2000 HITs

---+++ 31.10.2019
   * Clustering-Module ist fertig zum testen.
   * Todo:
      * Neue Paare suchen 
         * Mit BERT Model, trainiert auf allen Daten, die wir haben. Neue Themen und Daten von der Bachelor Arbeit.
         * Evt. Cross-Val auf kleiner gleich 5 Splits.
         * Neue Hyperparam Optimierung, aber eher grob.
   * Ergebnisse der BERT Experimente: [[https://docs.google.com/spreadsheets/d/1NM3Ogblft3IkRKJbAJY3HO612qL_ky4nWhBowr3GLxk/edit?usp=sharing][Google Tabelle]]
   * Weiter optimieren oder die Top3 Configs testen?
   * Wie viele Paare pro Topic annotieren lassen? Der alte Datensatz hat 18 Topics mit je 90 bis 110 Paaren.
   * Mögliche Paare pro Topic:
      * Dieses Mal eher Größenordnung 10,000 bis 40,000
      * Minimum: Electronic voting: 6555
   * Urlaubsbescheinigung für neuen Arbeitgeber?  
   * Restliche Stunden? 35h nachzuarbeiten aus vorherigen Monaten. Dann noch restliche Stunden aus dem Oktober.

   * Test-Ergebnisse auf allen Splits berechnen.
   * Mit Model, das auf demselben Thema trainiert wurde, die Scores für die Paare predicten.
   * Verteilung auf allen möglichen Paaren ansehen. Dann entscheiden, wie gezogen wird. Wollen etwa Normalverteilung.
   * Wahrscheinlich etwa 200 pro Thema neu annotieren. 4-5k$
   * Urlaubsanspruch mal ausrechnen. 

---+++ 22.10.2019
   * Problem: Argumente mit selbem Text aber unterschiedlichen Quellen
      * Duplikate werden beim Clustern entfernt. 
      * So verschwinden scheinbar Argumente im Ergebnis.
      * Wie mit den Duplikaten im Input umgehen?
      * Erstmal so lassen / soll schon vorher in der Pipeline gefiltert werden.
   * Permissions werden jetzt geprüft / Argumente über Query ID gesucht.
   * Auf Query Change wird reagiert.
   * Todo: X Clustertitel scrollbar, unterschiedliche Input Methoden?
      * X Meta-Infos ausgeben: X Argumente in Y Clusters, Z omitted.
      * X Scrollbare Titel, sodass es in eine Zeile passt.
      * X Input Format etwas kompakter machen, evt. mit Slider.
      * X Wenn fertig, JD bescheid geben.
   * Mehr Daten: Themen von der Bachelor Arbeit auffüllen.
      * Argumente aus Classifier Trainingsdaten (sind zum Großteil wirklich Argumente)
      * Dann neue Paare suchen

---+++ 17.10.2019
   * Cluster werden jetzt im Service sortiert und können nach min Size gefiltert werden.
   * Meta-Daten: Anzahl der gefilterten Cluster oder Argumente zurückgeben? Wahrscheinlich ist #2 besser.
   * Server sind bis auf weiteres immer noch offline.
   * Todo:
      * X Permission / Argumente über Query holen.
      * Evt. Clustering direkt mit BERT Embeddings.
      * Evt. neues BERT Model trainieren auf mehr Daten... aus meiner Bachelor Arbeit oder noch neue annotieren.
      * Wichtig: Version vom Frontend, die man Testen kann. Dann Model selber optimieren. 
      * Für neue Daten: 
         * Man könnte sich über die API neue Argumente holen. Zu bestimmten Themen und bestimmte Anzahl.
         * Mit bestem Modell aktuell klassifizieren mit ähnlichen Kriterien wie beim letzten Mal.
         * Evt. ca. 300 Argumente für ca. 10 Themen, die in der Bachelor Arbeit noch nicht abgedeckt waren.

---+++ 10.10.2019
   * Sortierung der Cluster ändern: Nach Größe sortieren?
      * Sortierung im Service oder im Frontend.
      * Eher im Service sortieren, default nach Größe absteigend.
      * Als Meta-Info ins Cluster-Objekt: Clustergröße.
      * Größe auch in Klammern in den Tabs anzeigen.
   * Cluster mit 1 Argument bringen eigentlich nichts:
      * Parameter an Cluster Service, z.B. minSize, durch den Cluster ausgeschlossen werden.
      * Sollte auch im Frontend einstellbar sein.
   * Evt. Tabs durch zusätzliches Markup klarer trennen. 
      * Oder Tabs scrollbar machen, siehe letztes Meeting.
      * Oder Dropdown.
   * Wenn der Service keine Cluster zurückgibt, also leerer Result, sollte es eine Anzeige im Frontend geben (no Clusters found).
   * Caching der Argument IDs: Scheinbar wird der State im Local Storage (auf dem Client-Rechner) gespeichert.
      * Besser kein Caching im State, oder wir müssen den Cache leeren, wenn er zu groß wird.  
   * Aber: Caching im Speicher / im Cluster Component möglich
      * Erstmal so lassen wie es ist.
   * GetArgumentsById ist langsam (wahrscheinlich kein Index auf argument_id)
      * Evt. Query mitschicken, alle Argumente für die Query holen, dann nur die gesuchten IDs heraussuchen.
      * War nicht mehr so besonders langsam.
   * Tabs sehen aus wie Pagination: Cluster-Titel?
   * Num Of Args Begrenzung nochmal im Backend vom Frontend prüfen.
   * Argument Limit: Evt. im Service auch nochmal runter setzen. 500 Argumente dauern ewig / geben Timeout.
   * Permissions prüfen / ob User Zugriff auf Query hat und dann Argumente nach Query Id, dann nach argument ID holen.
   * Evt. noch umsetzen: https://github.com/UKPLab/sentence-transformers

---+++ 04.10.2019
   * Threshold ist jetzt einstellbar und Request kommt vom Frontend an den Clustering Service.
   * Argumente sind leider wirklich im globalen State paginiert. Also immer nur 5 pro Stance mit allen Informationen
      * Die getArguments Action sollte ich aber auch benutzen können, um z.B. 100 Argumente zu bekommen.
   * Es können jetzt testweise diese 10 Argumente geclustert und angezeigt werden.
   * Todo: 
      * Auf Query change reagieren
         * Auf welche Events reagieren? Nur Query Change? Ja, erstmal nur Query Change
         * Evt. noch auf Zeitraum Änderung reagieren.
         * Bei Änderung Cluster null setzen oder anzeigen, dass das Ergebnis alt ist und man neu laden muss, z.B. mit Reload Button
         * Events sollten existieren.
      * Weniger als 10 Argumente im Ergebnis? Nochmal den Cluster Service prüfen. 
      * Cluster mit Tabs und Argumente paginiert darstellen? 
         * Titel der Cluster? Z.B. im Vergleich seltene Worte/ngrams 
         * Evt. wie bei Pagination bei zu vielen Tabs eine Scrollmöglichkeit hinzufügen
      * Mehr Argumente als Input vom Backend holen
         * Kann im globalen State oder im Cluster Component gecached werden
         * Wann neu laden?
         * Evt. nur mit IDs arbeiten und nur die Argumente vom aktuell angezeigten Cluster laden.
         * Neue Route im Backend: Z.b. 100 Argument IDs nach bestimmten Sortierkriterien.
         * Dann komplette Argument Infos vom Backend holen per ID. Mal versuchen, ob es mit Lokalem Cache klappt.
      * Queuing System Requests: Json Data wird doppelt encoded?
      * State: Vuex Tutorial ansehen
         * State wird wahrscheinlich nicht richtig zuruckgesetzt. z.b. bei graph 

---+++ 26.09.2019
   * Clusters auch im globalen State speichern? Oder nur im Component?
   * Bedienelemente (Threshold einstellung, Button zum Clustern) bei Query Eingabe oder im Component?
      * Einstellungen für Clustering soll auch im Cluster Modul bleiben.
      * Einstellbar sollten sein: Threshold, evt. später Algo, welche Argumente geclustert werden
      * Erstmal die ersten N Argumente: Pro/Con/Gemischt
      * Evt. später Aktionen aus anderen Komponenten starten.
      * Evt. später noch Sort By
   * Argumente erstmal als getabbte Liste darstellen?
      * Evt. Liste noch paginieren 
   * Wie bekomme ich Liste aller Argumente? über state.arguments.arguments sind sie scheinbar nach pro/con aufgeteilt.
      * Globales Feld ist scheinbar paginated.
      * Wenn die Query geladen wird, sollte ich alle Argumente sehen.
      * 5 elemente sind eventuell nur Anzeigefehler von Console
   * Clustering Service war auf lisa nicht erreichbar. 

---+++ 19.09.2019
   * Neue Query lädt immer noch keine Argumente
      * Existierende Query auf meinen User in DB kopieren funktioniert leider nicht
      * Wird für Clustering Frontend gebraucht

---+++ 13.09.2019
   * Query erstellen funktioniert jetzt.
      * Aber: Liefert aktuell keine Ergebnisse. Welches Model, welcher Corpus, strict search? Wo es auf jeden Fall Ergebnisse gibt?
      * Fehlen mir immer noch Rechte?
      * Strict Search true, ohne Zeitraum.
      * Queuing System hat scheinbar einen Fehler.
   * Frontend Flask Server: 
      * Wie prüfe ich, ob ein User Zugriff auf die Argumente haben darf, die er clustern will?
      * Muss ich auch die zugehörige Query prüfen?
      * Über Rechte muss ich mir keine Gedanken machen erstmal
   * Clustering Speed:
      * Dauer skaliert scheinbar quadratisch mit Anzahl der Argumente
      * 100 Argumente dauern ca. 25s, 200 Argumente dann ca. 100s
      * Clustering ist scheinbar CPU-bound; könnte wahrscheinlich mit multiprocessing deutlich schneller gemacht werden. Muss noch nicht super optimiert sein.
      * Erhöhung der GPU Batch Size bringt nur sehr kleine Verbesserung.
   * Sklearn Agglomerative Clustering getestet: Keine wirkliche Veränderung bei der Geschwindigkeit, selbst implementiert sogar noch etwas schneller.
      * Es könnten so aber andere Clustering Algorithmen getestet werden.
   * API Format: Wo werden die IDs der Argumente benötigt? Kann z.B. im Backend oder im Clustering Service vom Satz auf ID gemappt werden. 
      * in backend frontend mappen
   * JD schreibt mir, wenn das Queuing System gefixt ist. 
   * GEvent Worker kann ich mal lokal testen. 

---+++ 05.09.2019
   * dashboard-ui lokal aufgesetzt
      * Requirements vom flask server sind veraltet; flask_jwt_extended==3.20.0 muss gesetzt werden, da sonst eine andere Flask Version (1.x statt 0.12) genutzt wird.
      * Readme ist leider veraltet. Port für lokales Dashboard stimmt nicht mehr.
      * Beide Probleme oben wurden gefixt.
   * Test-Cluster Modul ist erstellt und kann angezeigt werden
   * ArgumenText Account wurde aktiviert, aber ich kann keine Query erstellen. Erschwert das Testen.
      * Dev Setup: Frontend + Flask Backend + redis lokal, Datenbank und Queuing auf bart.
      * Sollte redis nicht auch auf bart zeigen? Fehlen mir andere Umgebungsvariablen / Einstellungen?
   * user_index_association und query_index_association Einträge anlegen für meinen User. Regelt auf welche Corpora und Models ich Zugriff habe.
   * Muss rebasen auf die development branch. Wahrscheinlich dev branch in meine Branch mergen.
   * Erst mal Backend Anbindung machen, dann Frontend Design. Was besser passt.

---+++ 29.08.2019
   * Single GPU wird in der Docker Config gesetzt.
   * Cache wird jetzt geleert, wenn es mehr als 100 000 000 Einträge gibt.
   * Threshold als Parameter vom Request hinzugefügt.

---+++ 22.08.2019
   * Torch Versions Konflikt: Requirements immer auf dem Server erstellen lassen, nicht auf dem Laptop! Berichtigen und Benjamin nochmal schreiben.
      * Auch nochmal auf single GPU hinweisen / wie es vorher gemacht wurde. 
   * Cache: Maximalgröße setzen und dann leeren lassen. Anzahl der Argumente, die auf einmal geclustert werden können, auch begrenzen.
   * Threshold auch als Parameter vom Request.
   * Bei Änderungen vom Clustering: In einer neuen Branch arbeiten.
   * Dokumentation: 
      * Readme mindestens auf Root Ebene.
      * Auswahl der Paare: Prozess sollte nochmal dargestellt sein.
      * JD sollte die Plots am Besten selbst erstellen können.
      * Ergebnisse: Evt. nochmal in der UKP Cloud hochladen.

---+++ 15.08.2019
   * Flask App für Clustering Web Service:
      * Code für REST Routes, Clustering Algo und Similarity Funktion sind getrennt.
      * Läuft lokal und inzwischen auch auf Lisa.
      * Ein Port muss geöffnet werden! Ich habe keine sudo Rechte und die Firewall blockiert wahrscheinlich den Standard Flask Port (5000). 
      * Input Format aktuell: Json, "arguments": Liste von Argumenten.
      * Output Format aktuell: Json, Liste von Listen von Argumenten. 
      * Clustering Algorithmus wurde so angepasst, dass die Argumente Paarweise in Batches (statt jedes einzeln) predicted werden. Sollte die Performance verbessern.
         * Batch Size kann je nach Speicherbedarf angepasst werden.
         * Predictions werden pro Paar in einem Cache gespeichert. Der wächst potentiell endlos. Evt. irgendwann leeren?
      * Noch nicht implementiert:
         * Docker
         * Auth 
         * Logging, Error Handling
         * Cache leeren, z.B. ab einer bestimmten Anzahl an Einträgen.
         * Maximale Anzahl an Argumenten im Service setzen, um DoS zu verhindern.
   * Darstellung von Clustern: 
      * Erstmal Listen von Argumenten.
      * Dann evt. Word Cloud
   * Threshold evt. auch als Parameter für den Request.
   * Service soll in Docker Container. Evt. nicht von mir
   * Dann im Frontend einbauen. Termin in 2 Wochen für Einweisung Frontend
   * Bis dahin: Service sollte laufen. Dann die Perfomance testen, wie viele Argumente wir in 1 Minute clustern können.
   * Nochmal checken: Sind die alten Projekte noch aktuell? Alles gepusht? Repos sauber machen.
      * Dokumentation: Readme: Description, Setup, Besonderheiten. Was haben wir gemacht? Vorgehen.
      * Repo von der Bachelorarbeit archivieren/zum BERT Projekt linken.
      * Alte TODOs entfernen.
   * Weitere Auswertungen von den BWS Daten. 
   * So weit dokumentieren, dass JD die Plots selbst generieren kann.
   * Auswahl der Paare: Welche Misra Parameter?
   * Code für die Plots sollte auf jeden Fall da sein. Damit JD sie selbst ausführen kann. Clustergrößen, Plots nach BWS.
   * Clustering Ergebnisse (F1 Score) auch noch Google Doc. In das BERT Doc als neuen Tab einfügen.

---+++ 08.08.2019

   * Problem: Teilweise sind die Cluster sehr groß, z.B. split5.
   * Versuchen, das Model mal in die Anwendung zu integrieren, dann kann man es in der Anwendung auswerten.
      * Service, der List of Argument bekommt, und dann direkt Clustering macht. Und dann List of list mit Clustern zurückgeben.
      * Datenformat: JSON. Service Code ist schon vorhanden.
      * Wahrscheinlich 1 GPU verfügbar. Mal testen, wie viel Speicher es braucht. Entweder batch size oder sequence length reduzieren.
      * Keine Rechte für Docker. Entweder lokal auf Laptop testen ohne Cuda oder auf Server ohne docker.
      * Evt. F1 Score mit Score über Threshold -> Im gleichen Cluster? wenn es sinn macht und nicht zu lange rechnet.
   * Für später: Evt. F1 Score bei unbekannten Argumenten oder eine art Qualitätsmaß über die predictete Similarity nehmen. Density?
   * Clustering nochmal mit anderem Seed und JD die Ergebnisse schicken.

---+++ 01.08.2019

   * Clustering ist jetzt deterministisch. Es wird jetzt immer in derselben Reihenfolge durch Sets und Dictionaries iteriert.
   * Damit wir trotzdem noch einen zufälligen Einfluss haben, wird die sortierte Liste geshuffled. Das kann mit dem Seed gesteuert werden.
   * Evt. haben wir leicht verschiedene Predictions von BERT weil (A,B) nicht gleich (B,A).
   * Für 3 Clustering Seeds mit festen BERT Seed nochmal Ergebnisse berechnen.
   * Word Cloud: Nach Möglichkeit auf eine Seite bringen. Oder 10 Bilder in einem Ordner.
   * Visualisierung von Clustern bis Montag, 05.08. noch JD schicken.
   * Plan im August: Integration in ArgumenText. 
      * REST Endpoint: Clustern und evt. Paare auf Similarity überprüfen können.
      * Input: Liste von Argumenten. Output: Cluster (List of list?). Oder 2. Endpoint mit Score.
      * Flask / Docker Container. Flask ist eine Art Binding von HTTP auf python. 
      * Extra Auth erstmal nicht notwendig.
   * Morgen nach der Klausur: Verträge unterzeichnen, Stundenzettel.
   * Vertrag bis 31. Oktober.

---+++ 25.07.2019

   * Cluster weiter auswerten: 
      * Für jedes Cluster eine eigene HTML Datei
      * Word Cloud nach Worthäufigkeit normal und evt. mit Background Corpus, nach Möglichkeit schön darstellen / mehr Aufwand
      * Noch warten: Alles in einem Plot: 2d Darstellung aus Vektorraum aus gesamten Vokabular. Mit 1-hot Endcoding oder so etwas wie word2vec.

   * Determinismus/Seeds: 
      * Evt. wird das Bert Model nicht richtig geladen, oder der Cluster Algo ist random.
      * Oder Probleme mit Float: genauigkeit/rundungsfehler.
      * Evt. Probleme mit Set / Reihenfolge der Argumente nicht deterministisch: Set alphabetisch sortieren?

   * Vorgehen:
      * 1. Woher kommt der non-determinismus? 
      * 2. Visualisierung anpassen
      * 3. Mit verschiedenen Clustering Seeds testen, aber mit demselben BERT Model
      * 4. Evt. noch mit anderem BERT Model, macht aber wahrscheinlich nicht so viel Sinn

   * Clustering erstmal wichtiger als nur stab Daten testen.


---+++ 01.07.2019
   * Clustering Script läuft, aber noch nicht vernünftig.
   * Lookup Table hat bei mir aktuell nicht genug Einträge - Clustering Algorithmus versucht alle Kombinationen von Argumenten, aber ich benutze nur die Predictions der bekannten Paare / wo wir auch einen Gold-Score haben.
   * Doch mit Bert Model Checkpoint testen? Dann wäre es nicht so ein großes Problem. Ansonsten werden die Dateien sehr groß. Ja!
   * Wie funktioniert das dann auf dem Test Set?
      * Clustering Algo testet alle Kombinationen von Argumenten.
      * Aber bei der Evaluation werden nur die bekannten Paare geprüft.
   * Welchen Threshold für das 'true label' benutzen? Im Original sind HS und SS 1. Wenn wir den selben Threshold nehmen, der gerade auch zum Clustern benutzt wird, bekommen wir bei abweichenden Score Ranges wahrscheinlich etwas schlechte Predictions.
      * Erstmal gleich wie beim Clustering lassen. 
   * Visualisierung der entstehenden Cluster?
   * Dokumentation
   * Email mit Zusammenfassung diese Woche noch.
   * Gespeicherte Weights nach den Experimenten wieder löschen!


---+++ 25.06.2019
   * Im Beispielscript werden bereits vorhandene Predictions aus einer TSV Datei benutzt (hat den Vorteil, dass es unabhängig vom BERT Model laufen kann (z.B. auch ohne GPU).)
      * Pro split und Set (Train/Dev/Test) gibt es eine Datei. 
      * Wird im Code in ein `dict` gelesen
   * Cluster wird geschlossen, wenn der mean Score im Cluster kleiner als der Threshold ist?
   * Evaluation: Paar richtig klassifiziert? Wenn Paar similar (Label 1), dann sollen die Argumente A und B im selben Cluster liegen.
   * Threshold:
      * Wird aktuell im Script von 0 bis 1 in 20stel-Schritten getestet.
      * Klassenverteilung zeigt, dass wahrscheinlich nur Werte zwischen 0 und 0.3 bis 0.4 sinnvoll sind (sonst ist similar sehr selten). Je nachdem, wie lange der Algo läuft, trotzdem alles testen!
   * Wie genau "Lernen" wir beim Clustering?
      * Es gibt wie gewohnt train/dev/test Set
      * Parameter, der geändert werden kann: Threshold, sonst nichts.
      * Wir suchen auf dem Dev Set den besten Threshold Wert basierend auf den Predictions, 
      * Testen dann auf dem Test Set. Train Set wird zum Clusteringm nicht benötigt.
      * Predictions nur von 1 Seed verwenden?
         * Für den Anfang mit 1 Seed, aber später mit mehreren.
   * Evaluation: Top 3-5 Argumente aus dem Cluster rausziehen, aber wie die Top Argumente auswählen? Evt. einfach 5 random Argumente rausziehen. Dann für jedes Cluster auflisten.
   * Nochmal gucken, ob Clustering deterministisch oder random ist? Z.B. auch mit welchen Argumenten angefangen wird.
   * Erstmal ist lookup table ok, also muss kein BERT Model live mitlaufen. 


---+++ 18.06.2019
   * Clustering: In dem alten BERT Repo ist eine clustering Klasse. Ähnliche Argumente zusammen clustern. https://git.ukp.informatik.tu-darmstadt.de/daxenberger/unsupervised-bert/blob/master/clustering/hierachical_clustering/run_clustering.py
      * Anpassen: Binär zu Score/Threshold
      * Wie evaluiert man die resultierenden Cluster?
      * Nächster Parameter: Anzahl der Cluster. Zwischen 2 und 10?
      * Wie stellt man die Ergebnisse dar? Z.B. Tabelle mit 1 Spalte pro Cluster. Word Cloud über einem Cluster?
      * Auch hier Cross Topic: 7 Themen trainieren, 1 testen. 
      * Zum Testen des Clustering nur die Paare nehmen, die im Goldstandard sind (und nicht alle Argumente).
      * Test Evaluation: Richtig klassifiziert, wenn Argumente aus ähnlichen Paaren im selben Cluster sind.
   * Schritt 1: Threshold testen: Verteilung plotten (gesamt und pro Thema). Balkendiagramm mit % ähnlich/unähnlich.
   * Schritt 2: Clustering wie oben beschrieben. 
   * Experimente ohne web Argumente: Nebenbei laufen lassen, aber eher nächsten Monat. Clustering wichtiger.

---+++ 13.06.2019
   * Nochmal die besten Configs pro Split mit 2 anderen Seeds testen (auch auf den Test Topics).
      * Für die finalen Test werte: Avg über den seeds + Abweichung 
   * Für intern: Die Top 3 auf den Test Topics testen.
   * Die Repos nochmal besser dokumentieren (Readme, Comments etc.)
   * Die Web Daten müssten wir rauslassen, wenn Ergebnisse fürs Paper reported werden. Achtung: Mean der Scores ist evt. etwas verschoben.
      * Versuchen, das selbe Dev Set zu nehmen (ohne Web Argumente). Wenn der Anteil >10% ist, muss man noch ein Paar Argumente dazu ziehen.
   * Reihenfolge: Erst verschiedene Seeds, dann ohne Web.

   * Erster Run abgebrochen: War doch langsamer als erwartet. Es waren nur 4 statt 6-7 Splits (wie gedacht) fertig.
   * Stattdessen: Neue Hyperparam Suche, zuerst mit grober Parameter Range.
   * Tabelle mit allen durchgeführten Runs: https://docs.google.com/spreadsheets/d/1ubONdAuSj-68geyZqEFASQaC2q6djoHIUy-wUZELYyY/edit?usp=sharing
      * Zu beachten: Getestete Hyperparam Ranges sind auf der 2. Seite (seperates Datenblatt) 
   * Max. Seq. Length 128 statt 64 funktioniert etwas besser: ~0.7 pearson correlation auf dev set.

---+++ 04.06.2019
   * Mehrere Seeds testen!
   * Nochmal Hyperparam Range neu auslegen: Hidden, Learning Rate, Batch Size.
   * Datenanalyse: Score Histogramm (gesamt und über die 8 Themen, unterschieden nach web und stab).
   * Binär Klassenverteilung, wenn man die Scores bei 0, 0.2, 0.4, 0.6 etc. trennt. Tabelle mit Similar/Not Similar/Threshold.
   * Google Spreadsheet aufsetzten und im wiki verlinken:
      * Experiment Runs(nicht jeden einzelnen Parameter, aber für die aktuelle Exp Reihe: Cross Topic: Für jeden Split: Was ist die beste Param Config und die Ranges, Architektur, Welches Embedding (BERT Model Typ)), Eval Results: Pearson + Spearman Correlation + Loss, Dev und Test.
   * Erstmal neue Experimente starten, dann Datenanalyse.
   * Grid Search erstmal grob testen, dann feiner werden. Evt. random search. Zeitfenster: Ca. 1 Woche.

   * Assignments von Rejecteten Workern nachgeholt. Split-Half Reliability etwas besser, aber für so niedriges Agreement sind mehrere Trials (eher 25 als 10) nötig. 
   * Cross Topic BERT: Je nach Split Pearson Correlation von 0.65 bis 0.83 auf dem Dev Set; Experimente laufen noch. Muss noch auf der Test Topic getestet werden.
      * Eventuell sollte der Suchraum angepasst werden: Komplexeres Model oder niedrigere Learning Rate.
      * Random search statt grid search (wie aktuell)?   

---+++ 28.05.2019
   * Fehlende Assignments nachholen, Error Report HITs angucken, Scores + Assignments neu machen, Split Half Reliability nochmal berechnen, BERT cross topic mit 1 Thema.
   * JD Corpus (Scores + arguments) schicken.

---+++ 21.05.2019
   * Batches aufsplitten: Generieren die HITs normal (für beide Themen), aber in 2 getrennten Batches hochladen. Hälfte zufällig samplen. Name ändern, z.b. mit Zahl als Suffix.
   * Split Half Reliability, manual checking (Mal scores generieren lassen, 1 Argument mit anderen Partnern...), Mal Correlation mit BERT Predictions machen.
   * Darf grosszügiger blocken: Wenn Captcha Score zu niedrig ist oder die (Best,Worst) Indices komisch verteilt sind. 
   * 3. Batch kann sofort deployed werden, andere Assignments accepten, Wenn Analyse auf dem 2. Batch schlecht sind, können wir ggf. ein 5. Assignment auf dem 1. Batch machen oder zweifelhafte Worker rauswerfen.
   * Evt. mal Assignment Duration auf 10 Min.
   * Data Cleaning erfolgreich umgesetzt: Paare, die länger als 500 Zeichen sind, HTML Tags enthalten oder eine Edit Distance < 4 haben, werden entfernt.
   * Es ergibt sich eine neue Paarauswahl; im Grunde aber ähnlich verteilt wie vorher.
   * Qualification wurde neu erstellt; Nebenbei wurde ein Bug gefixt, weil versucht wurde, die Qualification vom alten Account zu updaten. Jetzt werden nur Qualifications vom Requester Account selbst benutzt.
   * Batch 1 ist live.
   * Performance beim HIT abrufen von mturk verbessert.
   * Batch 2 hat sehr niedriges Agreement. Worker, die viele HITs gemacht haben, haben auch hohe Fehlerrate. Aber die Test HITs sind nicht eindeutig. 
   * Es kommt mehrmals vor, dass es 2 Argumente mit selbem Aspekt, aber unterschiedlichen Stances gibt. Guidelines anpassen?


---+++ 14.05.2019
   * Es gibt einige Duplikat Argumente im Corpus, zur Reproduzierbarkeit wird nur das 1. (nach Hash) gespeichert. Leicht verschiedene URLs, aber der selbe Satz.
   * Code für die Auswahl der Paare ist fertig und scheint gut zu laufen.
   * Wordnet Fehler treten bei Stab/Gun Control und bei Web/Allen Themen auf.
      * Paare, bei denen eine Exception auftritt, werden ausgelassen (~1-2%)
   * Crowdsourcing: Selbe Parameter wie bei der Bachelorarbeit. 1 Thema pro Batch
   * BERT Predictions können evt. zur Qualitätskontrolle einzelner Worker benutzt werden.
   * TODO: 
      * Data Cleaning? z.B. HTML Tags entfernen. Ja, Argumente mit HTML Tags entfernen!
      * Paare, die Wort für Wort gleich sind, entfernen? Ja, nltk Edit distance, min. dist: 3
      * Expert HITs einbauen?
      * Topic description? Ja!
         * Cloning: human cloning.
      * Mturk Sandbox:  2 Themen auf einmal deployen.
   * Vorgehen: 
      * Zuerst mal versuchen, zu deployen.
      * Dann Data Cleaning, 
      * Topic Beschreibung
      * Qualification Test muss neu erstellt werden!
      * Live deployment

---+++ 07.05.2019

   * Welches Model zum Predicten nehmen: Was auf Misra Daten trainiert wurde
   * Auswahl der Paare: ca. 350 Paare pro Topic aus Stab Corpus, 75 Paare Webforum
   * 50% ziehen wir aus dem HS Bucket => HS Bucket braucht mindestens 175 Paare, sagen wir Top 1% nach Predictions
   * 2-50% Some Similarity, Rest No Similarity
   * 25% SS, 25% NS
   * Verteilung überprüfen mit BERT. Stichprobe, z.B. ein Thema.
   * Mit festen Seeds arbeiten (auch beim Ziehen der Paare)
   * Zum Crowdsourcing selber: Mal nachsehen, wie viele HITs es pro Topic werden. Es kann sein, dass wir 1 Batch pro Topic brauchen. Evt. trotzdem HITs mit verschiedenen Topics mischen.

---+++ 30.04.2019

   * Vorgehen: Histogramme der Predictions per Email schicken, dann überlegen wir, wie die Sätze ausgewählt werden sollen
   * Score-Verteilung überprüfen mit BERT Model
   * Crowdsourcing, evt. muss mir JD neue Keys für AWS schicken.
   * Scripts für Crowdsourcing: Extra Infos müssen noch mit gespeichert werden (stance, urls, hash für jeden Satz).

   * Takelab STS wird jetzt jedes Paar in-memory übergeben.
   * Progress Logs alle 1000 Paare hinzugefügt.
   * Geschwindigkeit: 10 000 Paare in ~ 10 min. mit 10 Workers (frisch gestartet)
      * Möglich, dass die Prozesse bei längerer Laufzeit gedrosselt werden (besonders der Java / Stanford Tagger Prozess).
   * Death Penalty Feature Extraction hat noch Fehler
   * Misra System zu trainieren funktioniert, der beste Estimator wird gespeichert/gepickled (so hat Joy wahrscheinlich predicted)
      * Geringe Anpassungen für die neue Version von sklearn waren nötig. 
      * Potenzieller Bug behoben: Feature Spalten waren nicht alphabetisch sortiert; jetzt ist die Feature Reihenfolge garantiert gleich.
      * Wir können das beste Model nehmen, das jeweils auf Death Penalty, Gun Control und auf allen drei Topics zusammen trainiert wurde.
   * Predictions der Paare (selbst >100k) dauern nur wenige Sekunden.
   * Was genau mit den Predictions speichern?
      * Wahrscheinlich nicht sinnvoll: Sämtliche Features (wo2vec1,wo2vec2,...600 etc.)
      * Notwendig: topicName, argument1, argument2, prediction 
      * Wahrscheinlich sinnvoll: 
         * hash1, hash2 für Reproduzierbarkeit. Wird auch die Quell-URL für jedes Argument benötigt? Ja!
         * stance1, stance2
         * retrievedUrl, archivedUrl

---+++ 23.04.2019
   * Ca. 10% der Paare zufällig ziehen, damit die Feature Extraction schneller geht.
   * Takelab STS von Dateibasiert auf in-memory ändern (wenn in vertretbarem Zeitaufwand machbar)
   * Fortschritt Logs in Feature Extraction einbauen!
   * SVM hat Gamma und C Hyperparameter. Sollte ich aus der `results.csv` übernehmen können (oder nach dem Trainieren der SVM direkt die unbekannten Paare predicten).
   * Für die Zukunft: BERT mit mehreren vorgegebenen Seeds testen; mind. 3 Seeds. Besten Seed auf dem Dev Set dann zum Testen benutzen.
   * Plan: 
      * Features generieren(in absehbarer Zeit, ggf. mit weniger getesteten Paaren)
      * Mit Misra predicten
      * Paare auswählen wie vorher besprochen
      * Mit BERT Sanity Check der Klassenverteilung machen
      * Crowdsourcing
   * Nochmal ein Update via Email diese Woche schicken. 

   * Es müssen sehr viele Paare pro Topic getestet werden: Abortion hat 1 127 251 Paare (nur Stab, noch kein Webforum Corpus); Alle Topics (Paare nur als Text, noch nicht als Features) haben zusammen 3.1 GB Dateigröße als CSV.
   * Resultierende Feature Dateien werden sehr groß, deshalb: Nur Word2Vec und Cosinus wie im Misra Paper, kein GloVe.
   * Code von Joy wurde optimiert: Cache für Sätze, Multiprocess.
   * Trotzdem dauert die Feature Extraction ewig; läuft gerade auf dem lisa Server.
      * Feature Extraction für 1 Paar dauert ca. 1 Sekunde; Läuft mit 10 Prozessen; Also nur für Abortion erwarte ich ca. 24-31h Rechenzeit.
      * Cache für Sätze wird das evt. etwas bescheunigen. Mehr Prozesse erlaubt?

---+++ 11.04.2019
   * 8 Themen für das Paper
   * Pro Topic: Paare samplen mit Misra
   * Nicht einfach bei einem Score Threshold abschneiden.
   * Vorwiegend hohe Predictions, ein Paar aus der Mitte und niedrige
   * Auswahl der Paare: Zufällig ziehen aber mit vorgegebenen Seed. Score wird in Wahrscheinlichkeit, das Paar zu ziehen umgerechnet.
   * JD schickt mir den Corpus

   * Timesheets (Originale) abgeben!
   * Alle möglichen Splits mit 2 Test Topics ergeben fast 150 zu testende Paare. Mit 3 Topics sind es noch mehr.
   * Daher: Einfache Splits, sodass jedes Thema genau einmal getestet wird. Es werden aber nicht alle möglichen Paare beachtet. Das ergibt 6 Splits mit je 3 Test Topics.
   * Ergebnis vom besten BERT Regression Model auf diesen einfachen Splits:
      * selbe Hyperparameter, die auf dem maLSTM Split ermittelt wurden (auf Teilmenge von allen Themen).
      * Trotzdem bekommt das Model jetzt nur einen Teil der Themen als Trainingsdaten. Die Test Topics sind "ungesehen".
   * Von Menschen lesbare Predictions und Plots: [[%ATTACHURLPATH%/bert.regression.best.simple-splits.zip][bert.regression.best.simple-splits.zip]]  
%EDITTABLE{format="| text,20| text,20| text,20| text,20|"}%
| *Split#* | *Test Topic* | *Pearson R* | *Spearman R* |
| 0.  | Fracking | 0.50 | 0.48 |
| 0.  | Gmo | 0.62 | 0.56 |
| 0.  | Electronic voting | 0.46 | 0.46 |
| 1 | Electric cars | 0.65 | 0.63 |
| 1 | Organ donation | 0.68 | 0.69 |
| 1 | Recycling | 0.76 | 0.77 |
| 2 | Offshore drilling | 0.53 | 0.51 |
| 2 | Hydrogen fuel cells | 0.72 | 0.67 |
| 2 | Big data | 0.77 | 0.77 |
| 3 | Hydroelectric dams | 0.72 | 0.72 |
| 3 | Robotic surgery | 0.76 | 0.76 |
| 3 | Net neutrality | 0.67 | 0.66 |
| 4 | Stem cell research | 0.72 | 0.73 |
| 4 | Geoengineering | 0.60 | 0.54 |
| 4 | Internet of things | 0.59 | 0.54 |
| 5 | Nanotechnology | 0.74 | 0.73 |
| 5 | Solar energy | 0.71 | 0.70 |
| 5 | Genetic diagnosis | 0.74 | 0.74 |


---+++ 05.04.2019
   * Cross-topic Experimente: 2 Themen zum Testen rauslassen, Dev Split prozentual von den Trainingsdaten.
   * Einmal mit jetzigen besten Hyperparams. Neue Optimierung für jeden Split.
   * Misra zum laufen bekommen.
   * Wenn misra lauft, dann crowdsourcing.
   * Wahrscheinlich nur 8 Themen, dann aber mehr Paare.

   * BERT Regression Model sieht vielversprechend aus: Bestes Model ohne hidden Layer bzw. das nur eine Linearkombination des BERT Outputs bildet:
      * Global Pearson/Spearman ~ 0.75 auf Dev Set.
   * Wichtig: Evaluation bisher als Correlation über dem kompletten Dev/Test Set (global). In der Bachelor Arbeit/maLSTM wurde aber Avg Correlation pro Topic getestet.
   * Damit die Ergebnisse vergleichbar sind, werden die bisherigen Tests mit `bert.regression` wiederholt. 
      * Evt. macht es aber für das Konferenz-Paper mehr Sinn, einfach die Correlation über dem kompletten Set zu bilden, weil es einfacher von anderen Teams anwendbar ist.
      * Ansonsten kann es Probleme mit der Avg Berechnung geben (Fisher Z-Transformiert oder nicht)
   * Bestes Regression Model mit zusätzlichem hidden Layer: 680 hidden Nodes, Learn Rate = 1.3e-5, Batch Size = 32 .
   * Zum Vergleich: Die maLSTM aus der Bachelor Arbeit hat Spearman pro Topic von 0.65 erreicht.
Correlation der Predictions mit echten Scores auf dem Test Set (maLSTM Split):
| *Modus* | *pearsonR* | *spearmanR* |
| Global | 0.800 | 0.803 |
| Pro Topic | 0.816 | 0.801 |
   * Correlation pro Topic Plot und Predictions (für Menschen lesbar): [[%ATTACHURLPATH%/bert-regression-best.zip][bert-regression-best]]
   * Prediction Histogramm: %IMAGE{"bert.regression.prediction.hist.png" caption="bert-regression-prediction-hist" size="200"}%
   
   * Misra: Immer noch Fehler bei `Rouge/FeatureRouge.py`: 
      * Ich verwende diese Library (wird auch von `pip install rouge` benutzt): https://github.com/pltrdy/rouge
      * Problem beim Code von Joy: rouge Implementierung hat sich geändert (oder ich benutze die falsche).
      * Was im Code wahrscheinlich erwartet wird: Numerischer Rouge Score.
      * Was die Library aktuell ausspuckt: Dictionary mit `{"f": _, "p": _, "r": _}` Einträgen
      * Außerdem ist Rouge Score auf dem Sentence Level nicht implementiert.
      * Welchen Wert soll ich benutzen (f/p/r?)
   * Ansonsten scheint alles (Java- und Python-Teil) zu funktionieren.

---+++ 28.03.2019
   * Wenn überhaupt mehr Tests mit Labels dann nur 1 Stunde investieren
   * Nächster Schritt: BERT Regression Model direkt mit den Scores (statt diskreten Labels)
      * Evaluation: Wieder Correlation: Spearman/Pearson. Vergleich mit maLSTM.
      * Hyperparam Optimierung: Hidden Nodes, Learning Rate, Batch Size.
      * Weitere Varianten testen: 
         * Topic Split fuer train/dev/test benutzen. Z.b. 6 topics train/dev und 2 test.
         * Topic als Feature? Z.B. appenden vorne an jeden Satz.
   * Arg Sim Model für Konferenz (etwa bis Mai):
      * Daten: 8 Themen, die oeffentlich sind. Argumente sind schon annotiert. Thema, Stance (pro/con/no argument).
      * Misra System zur Vorauswahl der Paare. Müsste off the shelf laufen
   * UKP-intern: Dann spaeter ca. 500 Paare pro Topic mit Daten aus ArgumenText.
   
   * BERT mit diskreten Labels (0/1):
   * Fuer Threshold 0 ist die Verteilung der Klassen fast gleich, aber in der Praxis ist dieser Threshold nicht sehr nuetzlich.
   * Daher wird jetzt Threshold 0.25 getestet (ca. 25% der Daten sind damit Label 1 bzw. similar), Score Range von -1 bis 1.
   * Test setup: BERT model Parameter unveraendert. 10 Trainings Epochen. BertAdam Optimizer.
      * Daten: BWS Corpus, Splits von maLSTM. Wichtig: Score range: von 0 bis 1, daher muss threshold im Code gleich 0.625 sein.
      * Hyperparam Range:
         * learnRate = 1 0.1 0.01 0.001 0.0001 2e-5
         * batchSize = 16 32
   * Hohe Learning Rate mit Adam (lr=1) sorgt dafuer, dass nur 1 Klasse predictet wird (scheinbar), da Accuracy genau den Klassenanteilen entspricht.
   * Learning Rates kleiner 0.0001 funktionieren gut. Neue Hyperparam Range:
      * learnRate = 6e-6 8e-6 10e-6 12e-6 14e-6
      * batchSize = 16 32
   * learnRate = 8e-6 funktioniert am Besten unabhängig vom Threshold. 
   * Bester F1 Score der Klasse "1" nach Threshold (auf maLSTM Score Range, also von 0 bis 1) auf dem Dev Set:
      * Labelled Similar: Anteil der Daten mit Label "1" auf dem kompletten Corpus  
%EDITTABLE{format="| text,20| text,20| text,20| text,20| text,20| text,20|"}%
| *Threshold* | *Dev F1* | *Labelled Similar* | *Test F1* | *Test P* | *Test R* |
| 0.5 | 0.79 |47% |0.79 |0.78 |0.80 |
| 0.625 | 0.70 |27% |0.68 |0.66 |0.70 |
| 0.66 | 0.69 |24% |0.64 |0.63 |0.65 |

---+++ 21.03.2019
   * CSV einlesen für BERT Format funktioniert scheinbar.
   * Score Threshold t für Konversion zu 0/1 Labels:
      * t=0.49 ergibt 244 similar Paare (ca. 12% vom Corpus)
      * Statistiken pro Topic:
         * Mean: 13.56
         * Std: 3.84
         * Min: 6
         * Max: 20
   * Threshold 0 und 0.25 mal testen als Threshold. Some Similarity waere in dem Sinne schon noch 1/similar
   * Train/Dev/Test Sets: Das Dev Set nicht anfügen ans Train Set
   * Seed mal konstant setzen
   * Hyperparam Tuning: Batch Sizes, Learning Rate
   * Zugang zu Lisa zum laufen bringen
   * Nächster Schritt: Übergang zu real valued scores.
   
---+++ 14.03.2019
   * Pearson Correlation ist leider ungeeignet als Loss Function (nicht differenzierbar?), Predictions werden NaN
   * Hyperparameter Optimization: 1 hidden LSTM layer, 2 hidden LSTM layer + return_sequences. Bias/initalization. Bidirectional.
   * Bidirectional reduziert Bias auf letzte Wörter im Satz.
   * Bestes Deep Model: (27, 27), bidirectional, initializer: zeros, unitForgetBias=False
   * Custom MSE: Bestraft Fehler härter, je weiter sie von 0.5 (mittlerer Score) entfernt liegen.
      * Zieht die Predictions auf unknown Daten (Funkstandard) weiter auseinander. Trotzdem sind die Predictions nicht besonders gut.
   * Mit den aktuellen Daten / Models ist maLSTM nicht gut genug, um ähnliche Argumente auf komplett unbekannten Themen zu finden.
   * => BERT: Pretrained Transformer, der kontext-abhängige Wort Vektoren liefert.
      * 1. Schritt: Daten Parser anpassen
         * Argumente aus der CSV in das BERT-Format bringen: Sentence1, Sentence2, Topic
         * Scores zu binären Labels konvertieren: 0/1
         * => Threshold?
      * Mittelfristig BERT mit unseren Scores benutzen 

---+++ 07.03.2019
   * Bidirectional LSTM: Prediction von 250k Paaren dauert jetzt etwa doppelt so lange (ca. 5 min 30 sec). Bei selber Trainingsdauer keine Verbesserung der Predictions.
   * Padding umdrehen hilft nichts - jetzt matcht die LSTM nur den Anfang des Satzes
   * Grund fuer das Verhalten (wahrscheinlich): 
      * Beispiele in den Trainingsdaten sind eher kurz und der input bias wurde komplett mit 0 initialisiert 
      * (== nicht genug Daten, um bias fuer laengere Saetze zu lernen)
      * Daher wird der Beginn des Satzes nicht gewichtet.
   * Neuer Test mit Bias = 1 bringt kaum Verbesserung
   * Verschiedene Architekturen: 
      * LSTM mit weniger Nodes overfittet nicht auf die letzten Worte
      * Deep LSTM mit return_sequences=True und z.B. (20,20) Nodes funktioniert auch gut
      * Neues Hyperparameter Tuning mit Correlation als Metrik?
   * Bias im Gold Standard: Training und Dev Set ueberprueft:
      * Es gibt einige Beispiele mit hoher Similarity (>0.9) und gleichem Suffix
      * Aber nicht mehrheitlich, d.h. genug Beispiele mit unterschiedlichem Suffix
      * Mehr Trainingsdaten sammeln?
   * Neue Trainingsdaten:
      * Argumente aus statischem Web Crawl
      * Mit existierendem System (maLSTM) ähnliche Argumente finden
      * Bis dahin: Neue Hyperparameter Optimization!
      * Ergebnisse vor dem nächsten Meeting schicken (Correlation und Beispiele)
      * Predictions auf dem Gold Test Set überprüfen!

---+++ 26.02.2019
   * Predictions liegen selten über 0.6
   * Argumente, die nicht Wort für Wort gleich sind haben bis zu 0.85
   * Threshold schwierig zu setzen: 
      * Bei 0.7 wären es nur etwa 40 Paare
      * Um 0.6 hohe Fehlerquote
   * Pearson Correlation als Metric auf Validation Set scheint vielversprechend:
      * Correlation verbessert sich noch, obwohl der min loss sich nicht grossartig ändert.
   * Problem: Letzten Wörter im Satz werden übergewichtet
      * 0-padding umdrehen?
      * Trainingsdaten gebiased? Also auch immer selbe Worte am Ende
      * mal bidirectional LSTM testen
   * Generell: Irgendwann auch cross-topic ähnliche Paare finden.

---+++ 19.02.2019
   * Funkstandard Argumente: Etwas noisy, quasi-duplikate vorhanden, teilweise andere Sprache (kyrillisch)
   * Paare per brute-force innerhalb der 13 Themen generiert: 250k
   * Embedding Layer in der maLSTM von meiner Bachelor Arbeit ungeeignet für unbekannte Argumente
   * Prediction mit maLSTM trainiert auf arg-sim-bws Corpus: 
      * Extreme (0 und 1) machen Sinn, aber sind relativ uninteressant
      * 0: Normale Argumente mit kyrillischen Zeichen
      * 1: Argumente, die bis auf wenige Zeichen identisch sind
   * Ziel: Paare finden, die nicht genau die gleichen Wörter haben, aber immer noch sehr ähnlich sind.
      * Entweder einen Prediction Threshold setzen oder automatisch suchen mit Substring-Maßen 

-- Main.JohannesDaxenberger - 2019-02-20

%META:FILEATTACHMENT{name="bert-regression-best.zip" attachment="bert-regression-best.zip" attr="" comment="" date="1554244329" size="96173" user="nothvogel" version="1"}%
%META:FILEATTACHMENT{name="bert.regression.prediction.hist.png" attachment="bert.regression.prediction.hist.png" attr="" comment="" date="1554245137" size="18151" user="nothvogel" version="1"}%
%META:FILEATTACHMENT{name="bert.regression.best.simple-splits.zip" attachment="bert.regression.best.simple-splits.zip" attr="" comment="" date="1554739910" size="181715" user="nothvogel" version="1"}%
%META:FILEATTACHMENT{name="label distribution.png" attachment="label distribution.png" attr="" comment="" date="1574764832" size="10925" user="nothvogel" version="1"}%

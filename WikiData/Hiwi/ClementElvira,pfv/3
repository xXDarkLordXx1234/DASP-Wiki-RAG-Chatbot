%META:TOPICINFO{author="elvira" comment="reprev" date="1406900664" format="1.1" reprev="1" version="3"}%
%META:TOPICPARENT{name="WebHome"}%
-- Main.ClementElvira - 2014-06-20
---+ Project management

---++ Project specification

TBD
---++ Tasks & WPs

---+ Weekly report

---++Week 1

   * First days at Ukp, configuren eclipe, getting started

---++Week 2

   * Exploring dkpro and weka
   * Implemeting first readers (acl and wikipedia dataset)
   * Thinking about how to do summarization with dipro

---++Week 3

   * Starting implementing a summarizer with dkpro
   * Adapting acl reader
   * Adding Rouge evaluation through java

---++Week 4

   * Creating first baseline summarizer (Selecting the ten first sentences)
   * Adding result parsing
   *  TODO: add result

---++Week 5

   * Adding a second Baseline summarizer (the idea was to use weka to do something more sophiticated): It computed the Bhattacharyya distance between the ngram distribution of each sentence and the ngram distribution of each text. Sentence are then rejected or accepted in terms of a treshold, computed thanks to the golden summary.
      * *TODO:* add result
      *  *Comments* : the Bhattacharyya distance between two distribution p and q is <latex> D_{bat}(p, q) = - \ln \left( \sum_{x \in X} \sqrt{p(x) * q(x)} \right) </latex>
  
   * Working on a first summarizer from literature:  [[http://www.csie.ntnu.edu.tw/~g96470318/A_trainable_document_summarizer_.pdf][Kupiec, Pederson, Chen, A document trainable Summarizer]]

---++Week 6
   
   * Improving Kupiec Perderson Chen classifier
      * Adding features: Based on POS tagging (verb, adjective), and improving StopWord dataset
      * Using Stemming during pre-processing step
      * Linking golden summaries to body texts trough cosinus similarity (instead of Leverson distance)

*Confusion matrix:* (trained on 1/4 of the acl dataset, computed on the validation set)
|    | *P* | *N* |
| *T* | 158 | 1172 |
| *F* | 144 | 26419 | 

*Rouge Evalution:* rouge C ~0.25<br>
*Comment:* Because the number of feature is increasing, it is less and less relevant to approximate the corelation matrix by the identity matrix, which contradicts the main asumption of the naive bayes classifier.

   * Implementing a new algorithm based on Random Forest (not tested yet)
*Comment:* This classifier may be interesting because of the automatic selection of relevant feature and the "boosting".

---++Week 7

   * Improving ROUGE results for a subset of ACL to 0.29 (on validation set) thanks to Random forest
   * Implementing Wikipedia dataset Reader
   * First experiments on this datasets:
      * *baseline: * 0.17
      * *Modified version of Pederson Chen Kupiec: * 0.16

---++Week 8

   * Rewriting various baselines summarizer:
      * TFIDF based (select 10 sentences with the highest TFIDF score)
      * a summarizer that ponderate sentences positions by TFIDF score
      * POS tagging Patern (compute ngram distribution on part of speech tagging on the summary, and look for ngram)

---++Week 9

   * Preparation of the presentation
   * Improving Kupiec Pederson Chen Summarizer using annotation, solving bugs etc...
   * Experimenting with short dataset (because of the curse of time computation)
   *  Results available here:

---++Week 10

   * Implementing a MEAD based summarizer (http://www.summarization.com/mead/)
   * rewritting some module, integrating ROUGE evaluation into batch Report
   * Experimenting with MEAD summarizer
   * Setting up environment for experiment on server

---++ Plan

   * First task, Experimenting:
      * (75%) Implementing a 100% dkpro based summarizer (in term of features)
      * Integrating MEAD feature
      * Integrating POS ngram Patern features
      * Integrating Kupiec ... features
      * test various combination of these features, ml technics
 
   * Second task, improving
      * Spend time on one of the baseline summarizer (most probably POS ngram)
      * Improve summarizer by doing error analysis
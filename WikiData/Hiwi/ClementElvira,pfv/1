%META:TOPICINFO{author="elvira" comment="reprev" date="1403276745" format="1.1" reprev="1" version="1"}%
%META:TOPICPARENT{name="WebHome"}%
-- Main.ClementElvira - 2014-06-20
---+ Project management

---++ Project specification

TBD
---++ Tasks & WPs

---+ Weekly report

---++Week 1

   * First days at Ukp, configuren eclipe, getting started

---++Week 2

   * Exploring dkpro and weka
   * Implemeting first readers (acl and wikipedia dataset)
   * Thinking about how to do summarization with dipro

---++Week 3

   * Starting implementing a summarizer with dkpro
   * Adapting acl reader
   * Adding Rouge evaluation through java

---++Week 4

   * Creating first baseline summarizer (Selecting the ten first sentences)
   * Adding result parsing
   *  TODO: add result

---++Week 5

   * Adding a second Baseline summarizer (the idea was to use weka to do something more sophiticated): It computed the Bhattacharyya distance between the ngram distribution of each sentence and the ngram distribution of each text. Sentence are then rejected or accepted in terms of a treshold, computed thanks to the golden summary.
      * *TODO:* add result
      *  *Comments* : the Bhattacharyya distance between two distribution p and q is <latex> D_{bat}(p, q) = - \ln \left( \sum_{x \in X} \sqrt{p(x) * q(x)} \right) </latex>
  
   * Working on a first summarizer from literature:  [[http://www.csie.ntnu.edu.tw/~g96470318/A_trainable_document_summarizer_.pdf][Kupiec, Pederson, Chen, A document trainable Summarizer]]

---++Week 6
   
   * Improving Kupiec Perderson Chen classifier
      * Adding features: Based on POS tagging (verb, adjective), and improving StopWord dataset
      * Using Stemming during pre-processing step
      * Linking golden summaries to body texts trough cosinus similarity (instead of Leverson distance)

*Confusion matrix:* (trained on 1/4 of the acl dataset, computed on the validation set)
|    | *P* | *N* |
| *T* | 158 | 1172 |
| *F* | 144 | 26419 | 

*Rouge Evalution:* rouge C ~0.25<br>
*Comment:* Because the number of feature is increasing, it is less and less relevant to approximate the corelation matrix by the identity matrix, which contradicts the main asumption of the naive bayes classifier.

   * Implementing a new algorithm based on Random Forest (not tested yet)
*Comment:* This classifier may be interesting because of the automatic selection of relevant feature and the "boosting".
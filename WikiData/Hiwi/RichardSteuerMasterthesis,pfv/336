%META:TOPICINFO{author="steuer" comment="reprev" date="1363015917" format="1.1" reprev="336" version="336"}%
%META:TOPICPARENT{name="RichardSteuer"}%
---++ Richard Steuer<br /><br />Master Thesis Student March 2013 - September 2013.
<div style="float:right; background: solid white; border: solid grey 1px; padding: 0.5em">
<b>Quick links</b><br/>
[[https://scruffy.ukp.informatik.tu-darmstadt.de/artifactory/webapp/archivesearch.html?4][Artifactory class search]]<br/>
[[https://bugzilla.ukp.informatik.tu-darmstadt.de//][Bugzilla UKP]]
</div>

---+++++ Thesis Overview

*"Using Contextualized Lexical Expansion for Information Retrieval"* <br/>
Supervisors: Prof. Dr. Chris Biemann, Dr. Jungi Kim

---++Meetings
   * (in) what we did within the meeting
   * (after) what I did after the meeting
   * (before) notes for me, before the meeting

---+++ 11.03.2013 (15th meeting)
   * (in) talked about the result of the !WordNet "evaluation", Chris' conclusion: "It confirms out intuition, that !WordNet is useless for expansion."
   * (after) %RED% TODO: %ENDCOLOR% read the thesis: "A Comparison of Lexical Expansion Methodologies to Improve Medical Question and Answering Systems" ([[http://gradworks.umi.com/15/12/1512012.html][online]])
   * (in) talked about the written thesis, went through the TOC, the extent, the registration
   * (in) talked about the word count to come and using the UKP cluster
   * (in) talked about index side expansion and using multiple fields as the last and very important unknown
   * (in) talked about assigning weights to TWSI terms
   * (after) %RED% TODO: %ENDCOLOR% get multiple fields to work, one field for all the expansion terms
   * (after) %RED% TODO: %ENDCOLOR% send results to Jungi: run with and without multiple fields

---+++ 11.02.2013 (14th meeting)
   * (before) idea: maybe conduct an analysis between T and TD, which terms help
   * Chris consoled me: "Don't get frustrated. IR is hard."
   * (in) Chris told us the distinction between NN and NNP in DT to be in the works
   * (in) Chris told us to be on vacation (and hence not available) from 16th to 27th
   * (in) Jungi had a brilliant idea for hacking a workaround to get Terrier query expansion to work: Since we have the expansion output (terms and weight), I could use this and post-process the terms + weight
   * (in) talked about now using several fields on the index side, one for all the original terms and one for all expansion terms and assigning a different weight for the expansion field
      * (in) the next analysis ("Do this until you find a trend.") should only regard the ones with a large impact (i.e. either bridging a large rank gap or consider results only in the top ranks)
   * (in) looked at the evaluation together and concluded two assumptions: 1. is_a relations and synonyms help. 2. co-hyponyms do not help.
   * (in) Chris offered the possibility of using money from his crowdsourcing seminar to me to conduct a crowdsourced re-ranking relevance task
   * (after) worked with the !WordNet libraries JAWS and JWI, managed to output semantic relations like synonymy and co-hyponymy
   * (after) conducted a new analysis and automatically evaluated and classified the results using !WordNet
      * wrote down a pair (original term, expansion term) and the measure (e.g. 1/rank_orig - 1/rank_withexpansions) and write down e.g. the path UDD (for up-down-down)
      * applied the JWI API and wrote a function with !WordNet telling the relation between two terms
      * done this for integer "rank difference" and reciprocal rank value
   * (after) %RED% TODO: %ENDCOLOR% (high) ran experiments with the expansion field on the index side (in order to assign a different weight to all the expansion terms)
   * (after) tried to tackle the issue with "u.n." and related terms; created new Token annotation type (to be able to set a value instead of using !coveredText), switched all the Token types, ran experiments, but MAP decreased, so switched back
   * (after) using the new DT db from now on (that distinguishes between NN and NNP)
   * (after) added a new parameter _numberOfExpansionsPerDoc_ to limit expansions per query/doc
   * (after) %RED% TODO: %ENDCOLOR% (low) idea: conduct a word frequency count (based on the whole doc collection) and only expand the more specific (i.e. rare) one, so that in e.g. "solar energy" only "solar" is expanded
   * (after) %RED% TODO: %ENDCOLOR% (low) idea: use the DT without NNP for the title, and the other one for the rest (D, N)

---+++ 05.02.2013 (13th meeting, just with Jungi)
   * (in) first, Jungi showed to me how to get TF out of the index (it's the indexReader inside of !TerrierSearcher.java)
   * (in) showed the qe bug to Jungi, worked on it for a while, couldn't fix it
   * (in) talked about query term weighting with the circumflex, I have to test it myself and ask Richard, if it's available in !DkPro IR
      * tested it myself, and it works perfectly!

---+++ 04.02.2013 (12th meeting)
   * (in) handle multi-word expressions: leave it that way (of splitting them)
   * (in) talked about weighting the query terms: Jungi confirmed that the circumflex is the way he uses it
   * (in) Chris told me to just use the train query collection from now on (for my experiments)
   * (in) Chris complimented me on my progress ("Keep up the speed!"), I replied: "I've changed. :)"
   * (in) Chris explained that another analysis of the results to be the next most important thing: Are there two classes of expansions (the ones that help and the ones that don't)? conduct that new analysis: At what terms does the worsening happen? And how often? At (in)frequent terms?
      * use maybe 3 and 0 as expansion quantity (both sides), and all POS (goal: not pursue the path of least worsening); "bring some examples"
   * (in) talked about that function of Terrier, how often a query term is present in the index and each file
   * (in) Chris advised not to follow the path of least worsening; once a system is upon baseline, we can improve on that
   * (in) mentioned the possibility of using the DT counts as a variable for weighting (division, square root, sqrt twice)
   * (after) found out that there are 150 train queries (instead of 91), debugged it and solved this important issue: trec_eval wasn't evaluating all the search results, because in the gold standard file, they use query IDs without leading zero ("41" instead of "041", as it is handled throughout the rest of the pipeline and even the topic files); changed the output before writing the search results
   * (after) found out that the !IndexTerm annotation has a weight feature, applied it, extended the !IndexTermAnnotation class, set weights in the DT and a default (1.0) for all other query terms; now it works perfectly! the query term weights are taken into account
      * ran experiments on T test and train with the "division weighting method", got insignificant improvements
      * implemented weighting methods: none (1.0), division, sqrt, sqrttwice; tested a little bit; obersavtions: division improves significantly over none, sqrt can't beat that
   * (after) integrated the run_id into the evaluation output; doing so, I found a bug in the !SearchResultRecord class, e-mailed to REC, fixed it
   * (after) html visualization: added the query annotation and document frequency for every word in it
   * (after) conducted new query analysis, looked at all the 150 queries, created page QueryAnalysis20130209
   * (after) tried abbreviations annotator with seven fixed abbreviations, didn't improve anything
      * (after) doing so, removed "us" and "un" from the stopwords file, which did improve results
---+++ 28.01.2013 (11th meeting)
   * (in) because I asked, talked about the DT not being the main resource we are trying to apply, rather expansions in general
   * (in) first talked about getting and using abbreviation lists, rather generate the list from the very same document collection
   * (in) one goal of the thesis could be to achieve the same results with T as with TD
   * (in) Chris showed a typical thesis structure to me ("This is a random thesis."), mentioning software used; I should describe document collection and TREC evaluation as well
   * (in) talked about DT expansions in general and expansion modes to try: DT-expand the query, the index, and both; try TWSI (UIMA annotator on [[http://www.ukp.tu-darmstadt.de/software/twsi-sense-substituter/][website]])
   * (in) went to Martin Riedl afterwards, explained to me how to employ the DT framework
   * (after) tried the other two Terrier query expansion models (Bo1, Bo2); results are still the same (i.e. query expansion is not working)
   * (after) spend a day debugging, why qe is not working; I can't figure it out, the reduction of the !ResultSet happens within the org.terrier !Manager class and the !QueryExpansion class
   * (after) enhanced !AnnotationRemover to take the class name to work with (as parameter)
   * (after) made choosing T,D,N a UIMA and Java command line parameter
   * (after) plugged in the old DTExpander query-side and ran experiments with it on frink; tried 1, 5 and 10 expansions; all precision results worsened (recall improved); the more expansion terms, the worse
   * (after) had to redo the experiments again, because I had found out that there is no NNP in the DT; results even worsened (because more terms got expansions)
   * <s> TODO: (medium) find out how to assign weights to !IndexTerm annotation (therefore, for expansion terms)</s> Jungi confirmed to use the circumflex in Terrier
   * (after) got the new DT data to work, wrote an annotator, installed a !POS mapping, ran experiments query side; results worsened in the same way
   * (after) implemented POS parameter to !DTAnnotator; ran query-side experiments dependent on POS, same results (but: most little worsening with verbs only, even an insignificant increase with 2 expansion terms on Test data with T only)
   * (after) ran experiments with expansions on index side as well, and both index and query side, results so far:
      * could not increase map, but muss less worsening with verbs only
      * could slightly increase !P@5  with verbs only (train data)
      * full list here: EvaluationResults20130204
   * (after) plugged in the TWSI sense annotator, encountered (and solved) problems:
      * was hard to find out how to distribute the config and data (model, etc.) files, because some of the relative paths are hard-coded into the classes
      * used an old uimafit API, had to rewrite some parts of the code
      * contrary to the Javadoc, it additionally relies on Sentence annotations (which I didn't have)
   * <s>TODO: (high) try annotator: TWSI</s> done after 04.02.2013, results worsened in the same way, even a little bit more
   * <s>TODO: (high) try annotator: abbreviations</s> done after 04.02.2013

---+++ 10.01.2013 (10th meeting)
   * (in) Chris joked that after the thesis, I'd have to know the IR book by heart (Manning//Raghavan/Sch√ºtze: Introduction to Information Retrieval)
   * (in) decided to focus on the *query evaluation* next: take 10 docs and one query, find out why query expansion improves performance; compare evaluation with and without query expansion
   * (in) talked about DT expansion: test it with T, TD, TDN
   * (in) decided to continue the meetings on mondays
   * (after) ran new experiments, just used TD (title, description); performance just slightly decreased
   * (after) searching pipeline: made choosing train/test a Java command line parameter
   * <s>TODO: (low) make choosing T, D, N a !UIMA parameter</s> done after 28.01.2013
   * (after) wrote an !UnderscoreSplitReAnnotator to re-annotate terms containing underscore (Token and Lemma);in the test data (queries) this makes 2,58% of all terms; created new index; slightly improved results
      * (after) enhanced that annotator to also re-annotate terms containing hyphens (e.g. " habit-forming") and added an annotator removing all terms only consisting characters that are neither letter nor number; slightly improved results again
   * (after) discovered that query expansion is not working (results with and without are the same), although console output says so; adding the PARAM_ENABLE_ORQUERY_WITHINFIELDS and debugging; crashes because of an empty query string (added by query expansion), e.g. by file ENtopic.C331.wsd (test data set)
   * %RED% TODO: %ENDCOLOR% (medium) fix query expansion; doesen't seem to work, results with and without aren't different
   * (after) discovered a Bug while processing terms containing dots ("U.N.", "e.g."); wrote an annotator to post-remove them
   * (after) wrote a tool to easily visualize the retrieval results, the query and the evaluation on one page; deployed it to compute server and ran it there with all the 160 test queries (and the corresponding search and evaluation results); created section [[https://maggie.tk.informatik.tu-darmstadt.de/wiki/bin/view/Hiwi/IrExperiments#4_1_Evaluation_browsing][Evaluation_browsing]] in the documentation
   * (after) manually examined the first ten queries with their results, drew error classes
      * (after) created an expansion annotator and manually added expansion terms; evaluated another 20 queries to examine how expansion should be done
   * %RED% TODO: %ENDCOLOR% (high) write the "Related Work" chapter (two sentences per paper)
   * (after) read the papers and started writing (!LaTex):
      * "UKP: Computing Semantic Textual Similarity byCombining Multiple Content Similarity Measures", 2012
      * "Using Distributional Similarity for Lexical Expansion in Knowledge-based Word Sense Disambiguation", 2012
      * "Text: Now in 2D! A Framework for Lexical Expansion with Contextual Similarity", 2012

---+++ 20.12.2012 (9th meeting, just with Jungi)
   * (in) asked Jungi, and decided not to touch the Synset type for now
   * (in) asked Jungi about *IndexTermGenerator*.PARAM_TOKEN_WEIGHT (and others): since we now use *IndexTermAnnotator* (due to API changes), which does not have these weight parameters; answer: Extend *IndexTermAnnotator* and add parameters for each (Token, Lemma, Stem)
   * (in) asked Jungi if we'll have many parameters and thus should use !DkPro Lab; answer is no
   * (in) asked Jungi: we should take the second index (i.e. index token, lemma and stem)
   * (in) discussed weights, Jungi told me to use 1/3 for each (Token, Lemma, Stem); and we'll not use weighting for headline/text
   * (in) Jungi explained the structure of the topics to me (regarding evaluation): They consist of a Title (T), a Description (D) and a Narrative (N). For evaluation purposes, we should try all the combinations: T, TD, TDN (currently, we're using TD)
   * (in) since the overall evaluation scores are still poor (compared to the paper), we took a look at the paper again and found out that they use a slightly different model ("DFR_BM25") and query expansion ("KL") ([[http://www.cs.buap.mx/~dpinto/research/CICLing07_1/Pinto06c/node2.html][What is KL?]])
   * (in) Jungi advised to compare how the DT expansion performs with and without the query expansion later
   * (after) ran the search again with DFR_BM25 and KL; only the !TokenLemmaStem variant improved (slightly), the two others showed slightly decreased performance (see BaselineIR)
   * (after) ran experiments, tried the three combinations (T, TD, TDN); T performed poorly, while TDN improved the results again
   * (after) Contacted REC, he said not to use the !IndexTermGenerator class; simulating the weight parameters could be done by using fields and assigning weights to them
   * (after) finally found out how to select the Terrier fields to search on, and how to assign weights to them; played with it, ran experiments, but no improvements at all (maybe it's because of the missing orquery option)
   * <s> TODO: find out how the lemma is created in the CLEF09 corpus</s> couldn't find it out, Jungi said it's !WordNet (because of the synset scores)
   * (after) had a massive struggle with "TerrierSearcher.PARAM_ENABLE_ORQUERY_WITHINFIELDS, false" (as they used in the paper, necessary for query expansion) causing a !BufferUnderflowException; after many hours, I finally found out it's because of an empty query term (added by the lexical expansion); fixed the empty term problem by modifying the org.terrier source code, but this didn't solve the problem; contacted REC, who confirmed by observation that all !IndexTerms are search for within all fields ... which makes we wonder about the point of using fields in the first place ...
   * (after) mea culpa: discovered that I was always using the training data for results comparison; used the test data, which boosted the results again
   * (after) the test data set combined with TDN (title, description, narrative) threw an error; while debugging, I found out it's because of the term "1:0" (a soccer game), Terrier interprets this as "look for the term '0' in the field '1'" ...
   * (after) another error occured; it turned out there are some single terms "-" in the topics, which is a special character for Terrier as well (i.e., minus); removed those term entries manually from two files (ENtopic.C277.wsd, ENtopic.C295.wsd)
   * (after) could solve another !BufferUnderflowException by debugging, it's because of a special character in the test data set; this resulted in being able to process the N (narrative) as well; boosted the results again!

---+++ 06.12.2012 (8th meeting)
   * (in) agreed on staying with the CLEF09 corpus, but just take one WSDed system/corpus
   * (in) talked about next step goals (within 4 weeks): read papers and write "Previous and related work", find out weighting of headline/text, resurrect "Synset" type
   * (after) adapted the XML *topics* reader to just read one corpus (NUS or UBC), ran and evaluated it; the evaluation results with the entire topics collection is the same, so I'll just take one topics corpus from now on
   * (after) did the same with the *document* collections; results are the same, again; I'll take one collection from now on
   * (after) checked with the debugger, if the *lemma text* is indexed correctly (because I was confused); yes, it is
   * (after) found out: Synset score is currently not used; you simply could include it by adding an !IndexTerm annotation in terms of the *Synset text value* (e.g. _"06838825n"_ )
   * (after) resurrected the Synset type (including max score annotation)
   * (after) added stemming to the document and topics pipeline
   * (after) while debugging, I discovered and fixed not having the same stopword removal processing in both the pipelines
   * (after) while evaluating, I found and reported a bug in the !StopWordRemover annotator
   * (after) ran and evaluated several indexing combinations, see BaselineIR; all approaches improved significantly just by applying stopwords correctly
   * (after) found out: there is no weighting of headline/text
   * <s>TODO: write a tool to easily visualize and browse the query evaluation</s> done after 10.01.2013

---+++ 29.11.2012 (7th meeting)
   * (in) talked about the next step goal: extracting the documents by id after the retrieval
   * (in) Chris recommended the following papers for lexical expansion:
      * "Text: Now in 2D! A Framework for Lexical Expansion with Contextual Similarity", 2012
      * "Using Distributional Similarity for Lexical Expansion in Knowledge-based Word Sense Disambiguation", 2012
      * "UKP: Computing Semantic Textual Similarity byCombining Multiple Content Similarity Measures", 2012
   * (after) took a look at the terrier web interface, thought about extracting documents, looked at queries with 10 ten bad results, started an analysis, created page QueryAnalysis
   * <s>TODO: read the papers (later step)</s> done after 10.01.2013
   * (after) created page WebBasedTerrier
  
---+++ 27.11.2012 (6th meeting, just with Jungi, consulted Richard [rec])
   * (in) sat down with Jungi to get help with the "cannot process topic" error; we thought it was an encoding error
   * (in) after spending one hour on it, we consulted Richard (REC) for another hour; he kindly took the time, ran the debugger and fixed the problem (solved by stopword removing round brackets and quotation marks), casually fixed the entire project dependencies
   * (after) merged and unpacked all the files (that took several hours)
   * (after) indexed all the 300k documents (took two hours on frink)
      * commented and refactored the old XML reader code (from the paper)
      * validated all the 300k XML files using _xmllint_ (result: OK); compared if all the file names are the same in both directories (result: OK)
      * FOUND IT: the file "LA042494-0045.wsd" (in both the NUS and the UBC collection) is causing the problems, it is exceptionally small
      * ran experiment on frink (for the used commands see IrExperiments)
---+++ 23.11.2012 (5th meeting, just with Jungi)
   * (in) Jungi presented his changes to me: he included "my" readers (which I had transformed from the old code) into one of his recent DKPro IR experiments
   * (in) the pipelines are working now, including evaluation (see IrExperiments); he shared everything by SVN
   * (in) Jungi said to not use DKPro Lab, because we don't have so many parameters
   * (in) instead of the class _IndexTermGenerator_ we now use _IndexTermAnnotator_
   * (after) understood and adapted new software package, kept it runnable, created the page IrExperiments

---+++ 22.11.2012 (4th meeting, just with Jungi)
   * (in) went through the code together, fixed many bugs
   * (in) got the document indexing pipeline running
   * (in) got stuck at the other pipeline (which does the topics and the search) by org.terrier source code error
   * (in) Richard (REC) dropped by, pointed us to DKPro Lab
   * EXTRA: read "A Lightweight Framework for Reproducible Parameter Sweeping in Information Retrieval", 2011

---+++ 08.11.2012 (3rd meeting, just with Jungi)
   * (in) went through the framework and overall technical aspects with Jungi
   * (after) set up SVN folder and shared with Jungi
   * (after) rewrote dkrpro pipelines, readers and annotators (topics and docs) from the package into new uima layout

---+++ 25.10.2012 (2nd meeting)
   * (after) downloaded and installed the software/framework to use, 
   * <s>TODO: manually evaluate the retrieval performance and examine "error classes"</s> done after 10.01.2013
   * %RED% TODO: %ENDCOLOR% read "Retrieving with good sense", 2000
   * EXTRA: read "Language model information retrieval with document expansion", 2006
   * EXTRA: read "Flexible UIMA Components for Information Retrieval Research", 2008

---+++ 11.10.2012 (1st meeting)
   * (in) discussed ideas, suggested papers
   * (in) Jungi said: "If you give me one week", he'd fix the framework (they used in the paper of 2009)
   * (after) I read the following papers:
      * "CLEF 2009 Ad Hoc Track Overview: Robust-WSD Task", 2009
      * "Combining Probabilistic and Translation-Based Models for Information Retrieval Based on Word Sense Annotation", 2009
      * "Word Sense Disambiguation and Information Retrieval", 1994
      * "Word Sense Disambiguation Improves Information Retrieval", 2012
      * "Enriching Document Representation via Translation for Improved Monolingual Information Retrieval", 2011
      * "A Markov Random Field Model for Term Dependencies", 2005
      * "Latent concept expansion using markov random fields", 2007
      
---++See also
   * [[https://wiki.ukp.informatik.tu-darmstadt.de/bin/view/DKPro/HowToMaven][How To Maven]]
   * [[https://wiki.ukp.informatik.tu-darmstadt.de/bin/view/Hiwi/DkproRepositoryForHiwis][How To SVN]]
   * [[https://wiki.ukp.informatik.tu-darmstadt.de/bin/view/DKPro/DKProIrProduct][DKPro IR]] (out-dated)
   * [[https://wiki.ukp.informatik.tu-darmstadt.de/bin/view/DKPro/TheLab][DKPro Lab]]
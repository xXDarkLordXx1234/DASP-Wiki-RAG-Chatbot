%META:TOPICINFO{author="steuer" comment="save topic" date="1374133514" format="1.1" reprev="194" version="203"}%
%META:TOPICPARENT{name="RichardSteuerMasterthesis"}%
This topic describes how to perform the experiments with the IR framework that I use in [[https://wiki.ukp.informatik.tu-darmstadt.de/bin/view/Hiwi/RichardSteuerMasterthesis][my thesis]].

<div style="float:right; background: solid white; border: solid grey 1px; padding: 0.5em">
<b>Quick links</b><br/>
BaselineIR<br/>
QueryAnalysis<br/>
TerrierStandalone<br/>
</div>

%TOC%

---++ Experiments

---+++ 1. Obtain the code

   * SVN directory: https://scruffy.ukp.informatik.tu-darmstadt.de/svn/dkpro_students/steuer/de.tudarmstadt.ukp.ir.masterthesis.steuer.first/
   * the main methods are in the  current working package called *pipelines*

---+++ 2. Indexing the documents

---++++ 2.1 The document collection

   * as documents, we use the CLEF-WSD dataset (Robust WSD task), see _"CLEF 2009 Ad Hoc Track Overview: Robust-WSD Task", 2009_ and http://ixa2.si.ehu.es/clirwsd/ ([[http://ixa2.si.ehu.es/clirwsd/index.php?option=com_content&task=view&id=19&Itemid=35][just the data]]) . The full dataset is comprised of:
      * GH95_NUS: 56.472 files (5,1G in total)
      * LAT94_NUS: 110.245 files (13G in total)
      * GH95_UBC: 56.472 files (5,4G in total)
      * LAT94_UBC: 110.245 files (14G in total)

   * HINT: to quickly get the number of files in a large directory: _find dirname -maxdepth 1 -type f | wc -l_
   * we use this collection because it is lemmatized and POS-tagged
   * See example here: XmlFileExample
   * get all the lemmas of all the files: _for i in *.wsd; do grep " LEMA=" $i | sed 's/.*LEMA=\"\(.*\)\" POS.*/\L\1/' | sed s/_/\\n/g | sed s/-/\\n/g ; done_

---++++ 2.2 Indexing

   * navigate to the class *IndexDocument* and put the command line parameters to eclipse (see next section)
   * run the program, the index will be created at the given location name ([[http://ir.dcs.gla.ac.uk/wiki/Terrier/IndexStructures Index Structure]])
   * HINT: indexing the whole document collection took two hours on frink (1G index) (or just 40min for one document collection)
   * %RED% WARNING: %ENDCOLOR% the XML reader (in the version I took from the paper) expects all the file names in directory 1 to be found in directory 2 as well. That is, dir1 <= dir2, or dir2 >= dir1, respectively.

---++++ 2.3 Maven export indexing to compute server
   * go to local workspace project and do: _mvn -DskipTests=true -Djava.memory.max=8000m -Ppackage-deploy_
   * scp the directory  to the compute server
   * on the compute server deploy directory: _bin/run -Dmainclass=pipelines.IndexDocument -Dargline="--inputdir1 /home/steuer/RobustWSD/allNUSdocs --outputdir /home/steuer/out/docIndexNUS_Lemma"_
   * *parameters* are as follows (order doesn't matter):
      * 1. -inputdir1: the directory of the document to be indexed (required!)
      * 2. -outputdir: the directory that will contain the index (required!)
      * 3. -nrOfExpPerTerm: limit to the expansion annotator (>0) per query/document *term* (optional, default: 0)
      * 4. -nrOfExpPerDoc: limit to the expansion annotator (>0) per *query/document* (optional, default: 0, i.e. no limit)
      * 5. -selectDtTerm: "leastfrequent" or "byweight" (optional, default: byweight)
      * 6. -types: e.g. "Token", "Lemma", "Stem", "TokenLemmaStem", ... (optional, default: !Lemma)
      * 7. -usemultiplefields: "true" or "false" (optional, default: false)
      * 8. -createfields: e.g. "token:TokenLemma,expansion:Expansion" (optional, default: "token:Lemma")

---+++++ 2.3.1 Transforming the XML documents to the tagged Terrier format

   * since we're using the stand-alone Terrier package, you only need to set the input directory (for the index documents to be transformed) and the output directory to put the new text documents:
   * _./bin/run -Dmainclass=pipelines.IndexDocument -Dargline="--inputdir1 /home/steuer/RobustWSD/allNUSdocs --outputdir /home/steuer/out/terrierTagged_Lemma/ --nrOfExpPerTerm 1 --resource delex"_
   * NOTE: to trigger expansions, e.g. use the _nrOfExpPerTerm_ parameter (the value has no effect)
   * NOTE: since there are 166k documents, this may take a while (took me 77min on Frink, without expansions)
   * see also: Hiwi.TerrierStandalone

---+++ 3. Read the topics and perform the search

We have a *train* (150 queries: C041-C140, C201-C250) and a *test* data set (160 queries: C141-C200, C251-C350). The queries consist of title (T), description (D) and narrative (N).
   * Train NUS: 7.909 word forms (tokens): [[http://www.ims.uni-stuttgart.de/projekte/CorpusWorkbench/CQP-HTMLDemo/PennTreebankTS.html][Tagset description]]

| *NN* | *IN* | *NNS* | *NNP* | *DT* | *JJ* | *.* | *VBP* | *CC* | *VB* | *VBN* | *VBG* | *RB* | *,* | *MD* | *VBZ* | *TO* | *CD* | *WDT* | *VBD* | *WP* | *PRP* | *PRP$* |
| 1168 | 1099 | 915 | 828 | 826 | 559 | 419 | 289 | 266 | 209 | 155 | 152 | 142 | 134 | 122 | 87 | 85 | 84 | 71 | 67 | 41 | 30 | 29 |

   * Test NUS: 8.440 word forms (tokens):

| *NN* | *IN* | *NNS* | *DT* | *NNP* | *JJ* | *.* | *CC* | *VBP* | *VB* | *VBN* | *RB* | *MD* | *VBG* | *,* | *WDT* | *TO* | *VBD* | *VBZ* | *CD* | *WP* | *PRP$* | *NNPS* |
|1364 | 1197 | 992 | 774 | 755 | 559 | 441 | 357 | 311 | 248 | 191 | 156 | 148 | 142 | 137 | 93 | 91 | 88 | 86 | 69 | 49 | 31 | 30 | 

---++++ 3.1 Overview

   * the topic and judgement files can be obtained via http://ixa2.si.ehu.es/clirwsd/index.php?option=com_content&view=article&id=19&Itemid=35
   * navigate to the class *SearchTopic* and put the command line parameters to eclipse (see next section)
   * the link to the topic files directories can be set within the source code (e.g. "src/main/resources/train_topics/...")
   * the given file will contain the output in the *TREC output format*, e.g.: 207    1    GH950102-000036    0.0    7.625813961029053    null
   * The columns are as follows (source: http://old-site.clsp.jhu.edu/ws2002/preworkshop/labs/resnik/lab.html):
      * 1. qid (integer query ID)
      * 2. iter
      * 3. docno (document number, a string)
      * 4. rank (integer starting from 0)
      * 5. sim
      * 6. run_id (identifying the system name, optional)

   * See topic file example here: TopicFileExample

---++++ 3.1 Maven export searching to compute server
   * go to local workspace project and do: _mvn -DskipTests=true -Djava.memory.max=8000m -Ppackage-deploy_ (takes me 4 minutes)
   * scp the directory  to the compute server
   * go to the deploy directory on the server and do: _ln -s ~/src src_ (for resource files: train/test queries, stopword file)
   * also, link the DT libraries: _cd lib; ln -s ~/jobimtext_lib/; cd .._
   * on the compute server deploy directory: _bin/run -Dmainclass=pipelines.SearchTopic -Dargline="--inputdir /home/steuer/out/docIndexNUS_Lemma --outputdir /home/steuer/out/ --dataset train --nrOfExpPerTerm 0 --pos NN"_
   * *parameters* are as follows (order doesn't matter):
      *  1. -inputdir: the directory of the documents index (required!)
      *  2. -outputdir: the directory, in which the search output is written to (required!)
      *  3. -queryparts: "T", "TD" or "TDN" (optional, default: TD)
      *  4. -dataset: "train" or "test" (optional, default: train)
      *  5. -nrOfExpPerTerm: limit to the expansion annotator (>0) per query/document *term* (optional, default: 0)
      *  6. -nrOfExpPerDoc: limit to the expansion annotator (>0) per *query/document* (optional, default: 0)
      *  7. -selectDtTerm: "leastfrequent" or "byweight" (optional, default: byweight)
      *  8. -pos: "NN" etc., "V" for V*, "N" for N*, "all" for all (optional, default: all)
      *  9. -types: e.g. "Token", "Lemma", "Stem", "TokenLemmaStem", ... (optional, default: !Lemma)
      * 10. -usemultiplefields: "true" or "false" (optional, default: false)
      * 11. -enableorquerywithinfields: "true" or "false (optional, default: true)
      * 12. -fieldweights: e.g. "1.0,0.3" (optional, default: "1.0,1.0")
      * 13. -termsinallfields: "true" or "false" (optional, default: "true")
      * 14. -createfields: e.g. "token:TokenLemma,expansion:Expansion" (optional, default: "token:Lemma")
      * 15. -limitsearchresults: e.g. "1000" (optional, default: 1000)

---+++++ 3.1.1 Transforming the XML documents into single line queries

   * since we're using the stand-alone Terrier package, we need to output the query annotations into a single file containing one line for earch query (this can be done locally, there only 150 queries)
   * parameter: _-inputdir /tmp -outputdir /tmp/topics/ -nrOfExpPerTerm 1_
   * NOTE: to trigger expansions, e.g. use the nrOfExpPerTerm parameter (the value has no effect)
   * see also Hiwi.TerrierStandalone

---+++ 4. Evaluation

   * Having this output file, you now can evaluate your system: There, we use the tool *[[http://trec.nist.gov/trec_eval/][trec_eval]]* ([[http://old-site.clsp.jhu.edu/ws02/preworkshop/labs/resnik/lab.html][Info]]), e.g.:
   * _"trec_eval ah-robust-english-train-clef2008.txt smallCorpus-result"_ , whereby
      * _ah-robust-english-train-clef2008.txt_ is a judgement file, you may obtain it by the link above
      * _smallCorpus-result_ is the output file given to the *SearchTopic* class
   *  the resulting output has many variables, some are:
      * num_q (number of queries)
      * num_ret (Total number of documents retrieved over all queries)
      * num_rel (Total number of relevant documents over all queries)
      * num_rel_ret (Total number of relevant documents retrieved over all queries)
      * map (Mean Average Precision)
      * gm_ap (Average Precision. Geometric Mean, q_score=log(MAX(map,.00001)))
      * R-prec (R-Precision (Precision after R (= num-rel for topic) documents retrieved))
      * bpref (Binary Preference, top R judged nonrel)
      * recip_rank (Reciprical rank of top relevant document)
      * ircl_prn.X.XX (Interpolated Recall - Precision Averages at X.XX recall)
      * P5, P10, ...
   * *-q* option: In addition to summary evaluation, give *evaluation for each query* 

---++++ 4.1 Evaluation browsing

   * I wrote a tool to display the evaluation and search results on HTML pages to browse them.
   * For *search results* and evaluation generation:
      * run the !SearchTopic pipeline to write the query annotations to a file (so called _systemqueries_)
      * run trec_eval on the retrieval output to get the evaluation results
      * _./bin/run -Dmainclass=utils.visualization.GenerateHTMLDocs -Dargline="/home/steuer/out/TokenLemma.TD.0.1_0_0_0.train.output /home/steuer/out/treceval.TokenLemma.TD /home/steuer/out/docIndexNUS_TokenLemmaExpansion_fields /home/steuer/htmleval/htmleval_20130526/""_
      * The command line parameters are:
         * 1. the search output (by the *SearchTopic* pipeline)
         * 2. the *trec_eval -q* output (run on 1.)
         * 3. the path to the document index (to provide term frequencies, etc.)
         * 4. the directory of the HTML files output (where to put it?)
      * The calculation on all the queries took me half a minute on frink.
   * For the *index documents* HTML generation:
      * _./bin/run -Dmainclass=utils.visualization.GenerateHTMLDocs -Dargline="/home/steuer/RobustWSD/allNUSdocs/ "_
         * parameter: the directory to the document files
      * you only need to do this once (because the documents won't change)
      * Processing all the 166k documents took me one hour on frink.
   * In the search results, only the first 1000 results are displayed, otherwise the files would get too big and usage would be inconvenient.
---++See also

 * [[https://wiki.ukp.informatik.tu-darmstadt.de/bin/view/DKPro/HowToRunExperimentsOnComputeServer HowToRunExperimentsOnComputeServer]]<br/>
 * WebBasedTerrier (own topic)<br/>
 * QueryAnalysis (own topic)<br/>
 * BaselineIR (own topic)<br/>
 * DatasetNotesPOS (own topic)<br/>
 * DtAnnotator (own topic)<br/>
%META:TOPICINFO{author="IvanGalkin" date="1237473377" format="1.1" reprev="2" version="2"}%
%META:TOPICPARENT{name="IvanGalkinWork"}%
---+ *Reduzierung der Bearbeitungsebenen*
---++ Problemstellung:

Die sehr lange Arbeitszeit des Programms dient als ein Zeichen für ineffiziente Algorithmen der Datenverarbeitung und die Notwendigkeit, diese Algorithmen zu optimieren.

---++ Analyse:

Nach der ausführlichen Analyse des Programmablaufs kann man die Bearbeitung einer Dump-Datei als mehrere Bearbeitungsebenen darstellen. Da der größte Zeitaufwand offensichtlich mit dem pages-meta-history-Dump verbunden ist, werden hier nur seine Bearbeitungsschritte beschrieben.

<img width="500" height="400" alt="" src="%ATTACHURL%/layersBefore.png" />

Dabei spielen die dargestellten Schichten folgende Rollen:
   * *Archivdatei* - die einzige Möglichkeit, die Dumps runter zu laden und auf der Festplatte zu speichern, weil die entpackte Information mehrere Terabyte groß sein kann
   * *XML-Darstellung* - wird durch das Entpacken der Archivdatei mithilfe der Klasse =UniversatDecompressor= erstellt.
   * *SQL-Darstellung* -  wird durch den Einsatz der Bibliothek =MWDumper= bzw. der Klasse =XMLInputStream= aus der XML-Darstellung konvertiert.
   * *textuelle Darstellung* - separiert verschiedene Tabellen aus der SQL-Darstellung von einander und stellt die Daten tabellenartig dar (CSV-ähnlich, wobei der Tabulator als Separater benutzt wird). Diese Darstellung wird mithilfe der Klasse =SQLInputStream= erstellt
 
Je nach Bearbeitungsschritt (s. DataMachineReview oder ParallelismImpossibility) werden verschiedene Parser eingesetzt, um die notwendigen Werte aus der textuellen Darstellung zu extrahieren.

Die Bearbeitungsschritte von dem pages-meta-history-Dump kann man folgendermaßen skizzieren:
   1 processRevision
      1 komplettes Entpacken der Dump-Datei  (für deutsche Wikipedia zurzeit > 1 Terabyte)
      1 *komplettes* Umwandeln der XML-Darstellung in die SQL Darstellung (> 1 Terabyte)
      1 Umwandeln der "revision"-Tabelle mit allen ihren *neun Spalten*
      1 Parsing der textuellen Darstellung und Auslesen von *drei* ihren Feldern
   1 processPage
      1 komplettes Entpacken der Dump-Datei
      1 *komplettes* Umwandeln der XML-Darstellung in die SQL Darstellung (> 1 Terabyte)
      1 Umwandeln der "page"-Tabelle mit allen ihren *11 Spalten*
      1 Parsing der textuellen Darstellung und Auslesen von *vier* ihren Feldern
   1 processText
      1 komplettes Entpacken der Dump-Datei
      1 *komplettes* Umwandeln der XML-Darstellung in die SQL Darstellung (> 1 Terabyte)
      1 Umwandeln der "text"-Tabelle mit allen ihren *3 Spalten* 
      1 Parsing der textuellen Darstellung und Auslesen von *zwei* ihren Feldern

Die Notwendigkeit, der aufeinander folgenden Bearbeitung, ist in ParallelismImpossibility bewiesen. Aus diesem Grund bewerten wir den Aufwand, der mit dem Entpacken verbunden ist, als unvermeidbar.

Eine weitere Quelle des zeitlichen Aufwandes ist die Tatsache, dass trotz der relativ kleinen Menge der notwendigen Informationen auf jedem Schritt der ganze Dump unnötigerweise in SQL umgewandelt und danach mehrmals gefiltert werden soll. Es ist einfach zu sehen, dass durch komplette Konvertierung XML2SQL schon 3 Terabyte der überflüssigen Informationen entstehen. Die endgültige textuelle Darstellung enthält auch viel unnützlicher Daten, was wieder sehr viel Rechenkapazität und Zeit braucht.

---++ Lösungsvorschlag:

Es ist grundsätzlich unnötig, die XML-Datei ins SQL-Format zu konvertieren. Die verwendete Bibliothek MWDumper bietet die Möglichkeit, die Informationen, die aus XML-Darstellung bekommen werden, nach beliebiger Art und Weise darzustellen. Diese Darstellung ist mit dem bis jetzt verwendeten SQL-Format nicht beschränkt und lässt sich mithilfe der Ableitungen der Klasse =org.mediawiki.importer.DumpWriter= beeinflussen.

Momentan wird für die XML2SQL-Konvertierung folgender Quellcode verwendet:


=SqlStream= stellt eine ZielStream dar und wird mit =OutputStream= initializiert
<verbatim>SqlStream output = new SqlFileStream(someOutputStream);</verbatim>
=DumpWriter= ist ein Interface, das für Transformationen der Dump-Information zuständig ist. In diesem Fall wird es als Output-Format SQL 1.5 verwendet
<verbatim>DumpWriter sqlWriter = new SqlWriter15(new MySQLTraits(), output);</verbatim>
=xmlReader= liest den Dump, transformiert ihn auf Grund vom =DumpWriter= und schreibt den in =SqlStream=
<verbatim>xmlReader = new XmlDumpReader(someInputStream, sqlWriter);</verbatim>
Starten
<verbatim>xmlReader.readDump();</verbatim>

Für unsere Lösung sollen wir die Klasse =SqlWriter15= mit unseren eigenen Klassen ersetzen, die das Interface =DumpWriter= implementieren. So könnte man drei verschiedene DumpWriter schreiben, die sich jeweils an den konkreten Schritten der Dumpbearbeitung orientieren (processRevision, processPage oder processText) und *nur* die notwendige Daten zurückliefern.

Durch solche Modifikationen kann man erhebliche Arbeitszeit sparen, indem mehrere komplette Dump-Konvertierungen mit einer gezielten Konvertierung ersetzt werden:

<img width="500" height="400" alt="" src="%ATTACHURL%/layersAfter.png" />

%META:FILEATTACHMENT{name="layersBefore.jpg" attachment="layersBefore.jpg" attr="" comment="" date="1237473128" path="layersBefore.odg" size="11790" stream="layersBefore.odg" tmpFilename="/var/tmp/CGItemp27958" user="IvanGalkin" version="3"}%
%META:FILEATTACHMENT{name="layersAfter.png" attachment="layersAfter.png" attr="" comment="" date="1237473171" path="layersAfter.png" size="28809" stream="layersAfter.png" tmpFilename="/var/tmp/CGItemp24614" user="IvanGalkin" version="1"}%
%META:FILEATTACHMENT{name="layersBefore.png" attachment="layersBefore.png" attr="" comment="" date="1237473263" path="layersBefore.png" size="25896" stream="layersBefore.png" tmpFilename="/var/tmp/CGItemp24573" user="IvanGalkin" version="1"}%

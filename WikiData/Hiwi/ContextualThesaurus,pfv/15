%META:TOPICINFO{author="RichardSteuer" date="1334568861" format="1.1" version="15"}%
%META:TOPICPARENT{name="RichardSteuer"}%
---+ Contextual Thesaurus

This page documents our work on the next level of the DistributionalThesaurus (DT), the *Contextual Thesaurus*.

The implementation to this research is based on a local UIMA pipeline (i.e. the cluster is not used here). It relies on the output of the *ContextParses* step, see the corresponding paragraph at the DT page.

Furthermore, the Berkeley Database (BDB) is used:

   * the complete DT (simsort-out) is put into one BDB
   * the results of the !FeatureCount step are put into another
   * the output of !AggrPerFtWithParams (see Level 3) is index into one

The goal of this second version of the DT is to examine whether or not a _local_ _context_ could help with automatic similarity (or disambiguation) calculation.

The current results are very promising.

-- Main.RichardSteuer - 2012-01-29

%TOC%

---++ Level 1

I'd like to illustrate this by example. Consider the following sentence:

   * _I started to work it out._
The sentence has the following dependency relations (according to the [[http://nlp.stanford.edu/software/lex-parser.shtml][Stanford Parser]]):

   * nsubj(started-2, I-1)
   * root(ROOT-0, started-2)
   * aux(work-4, to-3)
   * *xcomp(started-2, work-4)*
   * dobj(work-4, it-5)
   * prt(work-4, out-6)
Remember, we read all the sentences (and its parses) from a text file generated by the *ContextParses* step. This is what we read into the UIMA Cas. This way, we instantly have access to all dependency relations for each word within the sentence. We process sentence by sentence. Next, we process each word within that sentence and get its corresponding DT entry. We go through each line of the DT entry and look, how many of the dependency relations (of e.g. _started#VBD_) appear in the feature list of each DT entry (which is a similar word). The (here fictitious) end results (for e.g. _started#VBD_) may look like this:

| 1 | started#VBD | 279.0 | [ ... i#NNP#nsubj ... ] |
| 1 | began#VBD | 142.0  | [ ... work#VB#xcomp ...] |
| 0 | start#VBP | 93.0 | [ ... ] |
| 2 | ... | ... | [ ... ] |

We create a new first column to note the count of how many direct dependency relations appear in the feature list. We call this new column the _local context_. The second column denotes the similar word and the third is the integer similarity count which at the same time denotes the number of features it shares with the DT entry word (e.g. _started#VBD_).

At last, we ran this Level 1 with 1000 parsed sentences and calculated statistics of how many words (and the sentences) at least have a local context of &gt; 0 or &gt; 5 in their DT entry. Most entries will be zero. The results are:

Level 1, 1000 sentences, entry &gt;= 1
| V: | 12,22% |
| N: | 18,96% |
| O: | 35,89% |
| J: | 5,27% |

Level 1, 1000 sentences, entry &gt;= 5
| V: | 4,88% |
| N: | 1,79% |
| O: | 24,02% |
| J: | 0,86% |

(V=Verbs, N=Nouns, J=Adjectives, O=Other)

Read it this way: In the 1000 sentences (parses), 12,22% of the verbs have an entry &gt;= 1 in the contextual column of their DT entry.

---++ Level 2

Level 1 was counting how many of a words dependency relations (within the sentence) appear in each line of its DT entry. For level 2, consider the above sentence and its dependency relations again (_I started to work it out._). Level 2 looks over all the dependency relations (of e.g. _started#VBD_). Let's take _work#VB#xcomp_, which is one feature/relation of _started#VBD_. It will retrieve the DT entry for _work#VB_, loop through its entries, take the relation ending name ( _#xcomp_) and create artificial relations with all the DT entries. If one of those artificial relations appear in one line the original DT entry ( _started#VBD_), this line will get the DT similarity value as a summand. Finally, the sum of all the similarity scores for the features found in the line will be the new local context. Let's take a real example and look at the DT entry for the word _started#VBD_:

| started#VBD |  279.0 |  395.0 |  6 | work#VB:change#VB#xcomp:55.0:10384;work#VB:working#VBG#xcomp:77.0:7134;work#VB:come#VB#xcomp:77.0:22308;work#VB:turn#VB#xcomp:55.0:7267;work#VB:get#VB#xcomp:64.0:79922;work#VB:look#VB#xcomp:67.0:12133; | [saturday#NNP#tmod, says#VBZ#-ccomp, school#NN#dobj, school#NN#nsubj, ... |
| began#VBD | 142.0 | 254.0 | 4 | work#VB:look#VB#xcomp:67.0:12133;work#VB:change#VB#xcomp:55.0:10384;work#VB:working#VBG#xcomp:77.0:7134;work#VB:turn#VB#xcomp:55.0:7267; | [yelling#VBG#xcomp, week#NN#tmod, was#VBD#-ccomp, was#VBD#-advcl, were#VBD#-advcl, and#CC#cc, ... |
| start#VBP | 93.0 | 208.0 | 3 | work#VB:get#VB#xcomp:64.0:79922;work#VB:look#VB#xcomp:67.0:12133;work#VB:working#VBG#xcomp:77.0:7134; | [time#NN#-rcmod, week#NN#tmod, watching#VBG#xcomp, ... |

The columns are as follows:

   * 1. The first is the (similar) word from the DT.
   * 2. The second is the similarity value belonging to the word.
   * 3. The third is the new local context (as explained above, the sum)
   * 4. The fourth is the number of summands for 3)
   * 5. The fifth is the source for the new local context (explaination below). The sources are concatenated and separated by semicolon; there are as many sources as there are summands in 4)
   * 6. The last are the features (the similar words have in common), there are as many features as the number in 2)
A single source entry consists of the following parts:

   * e.g. work#VB:change#VB#xcomp:55.0:10384;
work#VB is the (external) DT entry that was retrieved because work#VB appeared in a dependency relation. change#VB#xcomp is one (similarity) entry from this (external) DT entry that somewhere appears in the (internal/original/local) DT entry line feature list. It has a similarity score of 55.0 and is taken as a summand for the new local context. Finally, 10384 is a global and external count of the feature change#VB#xcomp.

---++ Level 2 Evaluation

We try to evaluate the Level 2 approach by using the [[http://www.ukp.tu-darmstadt.de/data/twsi-lexical-substitutions/ TWSI corpus]]. The corpus contains substitutions for words in the context of a sentence. The substitutions were collected using Amazon Mechanical Turk. The corpus consists of multiple files, one containing all the sentences and their IDs, and others containing the substitutions with respect to the sentence ID. There are around 23k sentences with substitution words available.

Here's an example how the substitutions are given (manual excerpt):

| fight++59998955||0 | fight | exchange  | 1 |
| fight++59998955||0 | fight | struggle  | 3 |
| fight++59998955||0 | fight | war | 1 |
| fight++59998955||0 | fight | resistance | 2 |
| fight++59998955||0 | fight | battle | 2 |

It says that in the sentence with the ID 59998955, three people replaced _fight_ with _struggle_ , and two with the term _battle_ . The sentence with the ID 59998955 is the following: 

| 59998955 | Jennifer puts up a fight and seeks shelter in the carousel where she turns it on and hides in it . |

The idea is that we parse these 23k sentences with the ContextParses step like we did in Level 2. We put an additional column to the line entries incorporating these TWSI substitutions. The parses output of that sentence looks like this (excerpt for the term _fight#NN_, feature list and sources left out):

| fight#NN | 283.0 | 345.0 | 4 | 0 |
| fights#NNS | 82.0 | 0.0 | 0 | 0 |
| battle#NN | 77.0 | 0.0 | 0 | 2 |
| bout#NN | 54.0 | 0.0 | 0 | 0 |
| battles#NNS | 47.0 | 0.0 | 0 | 0 |
| race#NN | 44.0 | 116.0 | 1 | 0 |
| struggle#NN | 42.0 | 0.0 | 0 | 3 |
| game#NN | 42.0 | 116.0 | 1 | 0 |
| match#NN | 38.0 | 116.0 | 1 | 0 |
| war#NN | 35.0 | 0.0 | 0 | 1 |
| fighting#NN | 31.0 | 27.0 | 1 | 0 |
| season#NN | 31.0 | 116.0 | 1 | 0 |
| contest#NN | 29.0 | 0.0 | 0 | 0 |
| challenge#NN | 29.0 | 116.0 | 1 | 0 |

The last column is the new TWSi score. You can see that these exactly match with the file from the TWSI corpus. I left out all other lines from the DT entry to _fight#NN_ because they all have TWSI score 0. Furthermore, no other substitutions are available for this sentence (ID 59998955). So the final score for this sentence is 6 points.

For the evaluation, the output was sorted by CT score (third column) and then the top6/top11 entries were taken. You can see results/statistics in the following graph: TODO

The goal is to find the best scoring (regarding the Gold standard).

---+++ Level 2 Evaluation: Combining DT and CT scores

With and without feature frequency normalization: Consider the single source entry example from above:

   * e.g. work#VB:change#VB#xcomp:55.0:10384;

We now use two variants for the frequency count as summand:<br/>
*a)* just 55 [as before]<br/> 
*b)* 55/log(10384) [new]

In order to find the best scoring, we try to combine the DT and CT scores. First, some simple formulas are used:

*1)* &#955; * DT + (1 - &#955;) * CT (with &#955; in [0,1 .. 0,9])<br/>
*2)* (1 + DT) * (1 + CT)

The output of this program looks like this (excerpt, sorted by CT score):

Sentenceinfo: Sentence 932: In 1968 , O ' Brien was appointed by Vice President Hubert Humphrey to serve nationally as the director of his presidential campaign and by Howard Hughes to serve in Washington as his public - policy lobbyist .
director#NN: DT_value:
| director#NN | 292.0 | 621.0 | 6 | 0 | 588.1 | 182246.0 | 100.91 | 23637.31 |
| secretary#NN | 53.0 | 621.0 | 6 | 0 | 564.2 | 33588.0 | 77.01 | 4356.36 |
| commissioner#NN | 43.0 | 621.0 | 6 | 0 | 563.2 | 27368.0 | 76.01 | 3549.63 | 
| president#NN | 89.0 | 614.0 | 5 | 0 | 561.5 | 55350.0 | 79.84 | 7183.61 | 
| chairman#NN | 73.0 | 614.0 | 5 | 0 | 559.9 | 45510.0 | 78.24 | 5906.53 | 
| chief#NN | 58.0 | 614.0 | 5 | 1 | 558.4 | 36285.0 | 76.74 | 4709.26 | 
Sentenceinfo: All TWSI substitutions for director+++2065523: [administrator(1), person charged with directing(1), executive(1), chief(1), leader(1), manager(1), controlling officer(1), head(1), boss(1), supervisor(1)]
Sentenceinfo: TWSI Sentence Score for sentence 932: 1

The columns are:

   * the similar word from the DT
   * the DT score
   * the CT score
   * the number of summands for the CT score
   * 1a
   * 2a
   * 1b
   * 2b

The last four names denote the combinations of the formulas and the variants (see above). In this example, &#955; is 0,1.
  
---++ Level 3

This is not a sequel of Level 2, rather an alternative approach.

As for data preparation I created two slightly new Mapreduce steps. The first is called *FreqSigWithT*, which is almost the same as !FreqSig but remains the parameter t (the count of both word and feature). The second is called *AggrPerFtWithParams* with does the same as !AggrPerFt but keeps the parameters t and s from the previous steps. The output of this Mapreduce step looks like this (manual excerpt):

| blizzard-like#JJ#amod | conditions#NNS(9.0;72.92124634640366) storm#NN(2.0;14.178686844865895) |
| blocked#VBN#-prep_from | view#NN(4.0;21.209072332225197) tv#NNP(2.0;12.437310694030762) servers#NNS(2.0;13.395695844257792) computers#NNS(2.0;11.510651851730753) flag#NN(2.0;11.240876610107788) voting#NN(3.0;20.21929161714793)  landing#NN(3.0;21.174500416650055) |
| blocker#NN#-prep_as | well#NN(2.0;18.481300373974857) |
| blockers#NNS#prep_on | taking#VBG(2.0;11.435084368142874) |
| blocking#VBG#-acomp | downfield#JJ(3.0;34.77798048992699) content#JJ(2.0;16.031553898536572) |
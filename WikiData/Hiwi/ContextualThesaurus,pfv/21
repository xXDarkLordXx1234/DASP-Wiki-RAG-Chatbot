%META:TOPICINFO{author="BaseUserMapping_999" date="1360762006" format="1.1" version="21"}%
%META:TOPICPARENT{name="RichardSteuer"}%
---+ Contextual Thesaurus

This page documents our work on the next level of the DistributionalThesaurus (DT), the *Contextual Thesaurus*.

The implementation to this research is based on a local UIMA pipeline (i.e. the cluster is not used here). It relies on the output of the *ContextParses* step, see the corresponding paragraph at the DT page.

Furthermore, the Berkeley Database (BDB) is used:

   * the complete DT (simsort-out) is put into one BDB
   * the results of the !FeatureCount step are put into another
   * the output of !AggrPerFtWithParams (see Level 3) is index into one

The goal of this second version of the DT is to examine whether or not a _local_ _context_ could help with automatic similarity (or disambiguation) calculation.

The current results are very promising.

-- Main.RichardSteuer - 2012-01-29

%TOC%

---++ Level 1

I'd like to illustrate this by example. Consider the following sentence:

   * _I started to work it out._
The sentence has the following dependency relations (according to the [[http://nlp.stanford.edu/software/lex-parser.shtml][Stanford Parser]]):

   * nsubj(started-2, I-1)
   * root(ROOT-0, started-2)
   * aux(work-4, to-3)
   * *xcomp(started-2, work-4)*
   * dobj(work-4, it-5)
   * prt(work-4, out-6)
Remember, we read all the sentences (and its parses) from a text file generated by the *ContextParses* step. This is what we read into the UIMA Cas. This way, we instantly have access to all dependency relations for each word within the sentence. We process sentence by sentence. Next, we process each word within that sentence and get its corresponding DT entry. We go through each line of the DT entry and look, how many of the dependency relations (of e.g. _started#VBD_) appear in the feature list of each DT entry (which is a similar word). The (here fictitious) end results (for e.g. _started#VBD_) may look like this:

| 1 | started#VBD | 279.0 | [ ... i#NNP#nsubj ... ] |
| 1 | began#VBD | 142.0  | [ ... work#VB#xcomp ...] |
| 0 | start#VBP | 93.0 | [ ... ] |
| 2 | ... | ... | [ ... ] |

We create a new first column to note the count of how many direct dependency relations appear in the feature list. We call this new column the _local context_. The second column denotes the similar word and the third is the integer similarity count which at the same time denotes the number of features it shares with the DT entry word (e.g. _started#VBD_).

---++ Level 2

Level 1 was counting how many of a words dependency relations (within the sentence) appear in each line of its DT entry. For level 2, consider the above sentence and its dependency relations again (_I started to work it out._). Level 2 looks over all the dependency relations (of e.g. _started#VBD_). Let's take _work#VB#xcomp_, which is one feature/relation of _started#VBD_. It will retrieve the DT entry for _work#VB_, loop through its entries, take the relation ending name ( _#xcomp_) and create artificial relations with all the DT entries. If one of those artificial relations appear in one line the original DT entry ( _started#VBD_), this line will get the DT similarity value as a summand. Finally, the sum of all the similarity scores for the features found in the line will be the new local context. Let's take a real example and look at the DT entry for the word _started#VBD_:

| started#VBD |  279.0 |  395.0 |  6 | work#VB:change#VB#xcomp:55.0:10384;work#VB:working#VBG#xcomp:77.0:7134;work#VB:come#VB#xcomp:77.0:22308;work#VB:turn#VB#xcomp:55.0:7267;work#VB:get#VB#xcomp:64.0:79922;work#VB:look#VB#xcomp:67.0:12133; | [saturday#NNP#tmod, says#VBZ#-ccomp, school#NN#dobj, school#NN#nsubj, ... |
| began#VBD | 142.0 | 254.0 | 4 | work#VB:look#VB#xcomp:67.0:12133;work#VB:change#VB#xcomp:55.0:10384;work#VB:working#VBG#xcomp:77.0:7134;work#VB:turn#VB#xcomp:55.0:7267; | [yelling#VBG#xcomp, week#NN#tmod, was#VBD#-ccomp, was#VBD#-advcl, were#VBD#-advcl, and#CC#cc, ... |
| start#VBP | 93.0 | 208.0 | 3 | work#VB:get#VB#xcomp:64.0:79922;work#VB:look#VB#xcomp:67.0:12133;work#VB:working#VBG#xcomp:77.0:7134; | [time#NN#-rcmod, week#NN#tmod, watching#VBG#xcomp, ... |

The columns are as follows:

   * 1. The first is the (similar) word from the DT.
   * 2. The second is the similarity value belonging to the word.
   * 3. The third is the new local context (as explained above, the sum)
   * 4. The fourth is the number of summands for 3)
   * 5. The fifth is the source for the new local context (explaination below). The sources are concatenated and separated by semicolon; there are as many sources as there are summands in 4)
   * 6. The last are the features (the similar words have in common), there are as many features as the number in 2)
A single source entry consists of the following parts:

   * e.g. work#VB:change#VB#xcomp:55.0:10384;
work#VB is the (external) DT entry that was retrieved because work#VB appeared in a dependency relation. change#VB#xcomp is one (similarity) entry from this (external) DT entry that somewhere appears in the (internal/original/local) DT entry line feature list. It has a similarity score of 55.0 and is taken as a summand for the new local context. Finally, 10384 is a global and external count of the feature change#VB#xcomp.

---++ Level 2 Evaluation

We try to evaluate the Level 2 approach by using the [[http://www.ukp.tu-darmstadt.de/data/twsi-lexical-substitutions/ TWSI corpus]]. The corpus contains substitutions for words in the context of a sentence. The substitutions were collected using Amazon Mechanical Turk. The corpus consists of multiple files, one containing all the sentences and their IDs, and others containing the substitutions with respect to the sentence ID. There are around 23k sentences with substitution words available.

Here's an example how the substitutions are given (manual excerpt):

| fight++59998955||0 | fight | exchange  | 1 |
| fight++59998955||0 | fight | struggle  | 3 |
| fight++59998955||0 | fight | war | 1 |
| fight++59998955||0 | fight | resistance | 2 |
| fight++59998955||0 | fight | battle | 2 |

It says that in the sentence with the ID 59998955, three people replaced _fight_ with _struggle_ , and two with the term _battle_ . The sentence with the ID 59998955 is the following: 

| 59998955 | Jennifer puts up a fight and seeks shelter in the carousel where she turns it on and hides in it . |

The idea is that we parse these 23k sentences with the ContextParses step like we did in Level 2. We put an additional column to the line entries incorporating these TWSI substitutions. The parses output of that sentence looks like this (excerpt for the term _fight#NN_, feature list and sources left out):

| fight#NN | 283.0 | 345.0 | 4 | 0 |
| fights#NNS | 82.0 | 0.0 | 0 | 0 |
| battle#NN | 77.0 | 0.0 | 0 | 2 |
| bout#NN | 54.0 | 0.0 | 0 | 0 |
| battles#NNS | 47.0 | 0.0 | 0 | 0 |
| race#NN | 44.0 | 116.0 | 1 | 0 |
| struggle#NN | 42.0 | 0.0 | 0 | 3 |
| game#NN | 42.0 | 116.0 | 1 | 0 |
| match#NN | 38.0 | 116.0 | 1 | 0 |
| war#NN | 35.0 | 0.0 | 0 | 1 |
| fighting#NN | 31.0 | 27.0 | 1 | 0 |
| season#NN | 31.0 | 116.0 | 1 | 0 |
| contest#NN | 29.0 | 0.0 | 0 | 0 |
| challenge#NN | 29.0 | 116.0 | 1 | 0 |

The last column is the new TWSi score. You can see that these exactly match with the file from the TWSI corpus. I left out all other lines from the DT entry to _fight#NN_ because they all have TWSI score 0. Furthermore, no other substitutions are available for this sentence (ID 59998955). So the final score for this sentence is 6 points.

For the evaluation, the output was sorted by CT score (third column) and then the top6/top11 entries were taken. 

The goal is to find the best scoring (regarding the Gold standard).

---+++ Level 2 Evaluation: Combining DT and CT scores

With and without feature frequency normalization: Consider the single source entry example from above:

   * e.g. work#VB:change#VB#xcomp:55.0:10384;

We now use two variants for the frequency count as summand:<br/>
*a)* just 55 [as before]<br/> 
*b)* 55/log(10384) [new]

In order to find the best scoring, we try to combine the DT and CT scores. First, some simple formulas are used:

*1)* &#955; * DT + (1 - &#955;) * CT (with &#955; in [0,1 .. 0,9])<br/>
*2)* (1 + DT) * (1 + CT)

The output of this program looks like this (excerpt, sorted by CT score):

Sentenceinfo: Sentence 1: His narrative of the Crusade is entitled De profectione Ludovici VII in Orientem ( On Louis VII ' s journey to the East ) , which relates the progress of the crusade from France to Antioch .<br/>
progress#NN: DT_value:
| progress#NN | 282.0 | 0.0 | 0 | 0 | 28.2 | 283.0 | 28.2 | 283.0 | 141.0 | 283.0 | 141.0 | 283.0 | 253.8 | 283.0 | 253.8 | 283.0 | 
| improvement#NN | 62.0 | 0.0 | 0 | 0 | 6.2 | 63.0 | 6.2 | 63.0 | 31.0 | 63.0 | 31.0 | 63.0 | 55.8 | 63.0 | 55.8 | 63.0 | 
| improvements#NNS	 | 48.0 | 0.0 | 0 | 0 | 4.8 | 49.0 | 4.8 | 49.0 | 24.0 | 49.0 | 24.0 | 49.0 | 43.2 | 49.0 | 43.2 | 49.0 | 
| growth#NN | 44.0 | 0.0 | 0 | 1 | 4.4 | 45.0 | 4.4 | 45.0 | 22.0 | 45.0 | 22.0 | 45.0 | 39.6 | 45.0 | 39.6 | 45.0 | 
| success#NN | 44.0 | 0.0 | 0 | 0 | 4.4 | 45.0 | 4.4 | 45.0 | 22.0 | 45.0 | 22.0 | 45.0 | 39.6 | 45.0 | 39.6 | 45.0 | 
| recovery#NN | 42.0 | 0.0 | 0 | 0 | 4.2 | 43.0 | 4.2 | 43.0 | 21.0 | 43.0 | 21.0 | 43.0 | 37.8 | 43.0 | 37.8 | 43.0 | 
Sentenceinfo: All TWSI substitutions for progress+++59971618: [course(1), transport(1), development(1), growth(1), advance(1), movement(1), advancement(2), transportation(1)]<br/>
Sentenceinfo: TWSI Sentence Score for sentence 1: 1

The columns are:

   * [0] the similar word from the DT
   * [1] the DT score
   * [2] the CT score
   * [3] the number of summands for the CT score
   * [4] the TWSI score
   * [5] 1a (&#955; = 0,1)
   * [6] 2a (&#955; = 0,1)
   * [7] 1b (&#955; = 0,1)
   * [8] 2b (&#955; = 0,1)
   * [9] 1a (&#955; = 0,5)
   * [10] 2a (&#955; = 0,5)
   * [11] 1b (&#955; = 0,5)
   * [12] 2b (&#955; = 0,5)
   * [13] 1a (&#955; = 0,9)
   * [14] 2a (&#955; = 0,9)
   * [15] 1b (&#955; = 0,9)
   * [16] 2b (&#955; = 0,9)

The last four names denote the combinations of the formulas and the variants (see above).
  
---++ Level 3

This is not a sequel of Level 2, rather an alternative approach. From the software engineering point of view, Level 3 incorporates Level 2 and adds a new colum - the Level 3 score.

As for data preparation I created two slightly new Mapreduce steps. The first is called *FreqSigWithT*, which is almost the same as !FreqSig but remains the parameter t (the count of both word and feature). The second is called *AggrPerFtWithParams* with does the same as !AggrPerFt but keeps the parameters t and s from the previous steps. The output of this Mapreduce step looks like this (manual excerpt):

| blizzard-like#JJ#amod | conditions#NNS(9.0;72.92124634640366) storm#NN(2.0;14.178686844865895) |
| blocked#VBN#-prep_from | view#NN(4.0;21.209072332225197) tv#NNP(2.0;12.437310694030762) servers#NNS(2.0;13.395695844257792) computers#NNS(2.0;11.510651851730753) flag#NN(2.0;11.240876610107788) voting#NN(3.0;20.21929161714793)  landing#NN(3.0;21.174500416650055) |
| blocker#NN#-prep_as | well#NN(2.0;18.481300373974857) |
| blockers#NNS#prep_on | taking#VBG(2.0;11.435084368142874) |
| blocking#VBG#-acomp | downfield#JJ(3.0;34.77798048992699) content#JJ(2.0;16.031553898536572) |

Level 3 works similar to Level 2: It loops over all dependency relations of each (pos-tagged) word. In the above sentence (see Level 1), _work#VB#xcomp_ is one feature/relation of _started#VBD_. It then will retrieve the aggrperftwithparams-entry (Level 3) for this feature and add the t values from those words that appear in the DT entry of the sentence word (e.g. _started#VBD_). Here's an example output:

We evaluated this way: For each column, sort by this column, limit the line entries and then look at the TWSI scores for this sorting.

Sentenceinfo: Sentence 1: His narrative of the Crusade is entitled De profectione Ludovici VII in Orientem ( On Louis VII ' s journey to the East ) , which relates the progress of the crusade from France to Antioch .<br/>
progress#NN: DT_value:<br/>
| progress#NN | 282.0 | 0.0 | 0 | 0.0 | 28.2 | 283.0 | 28.2 | 283.0 | 141.0 | 283.0 | 141.0 | 283.0 | 253.8 | 283.0 | 253.8 | 283.0 | 1412.0 | the#DT#det:progress#NN1412.0;;;;; |
| improvement#NN | 62.0 | 0.0 | 0 | 0.0 | 6.2 | 63.0 | 6.2 | 63.0 | 31.0 | 63.0 | 31.0 | 63.0 | 55.8 | 63.0 | 55.8 | 63.0 | 0.0 |
| improvements#NNS | 48.0 | 0.0 | 0 | 0.0 | 4.8 | 49.0 | 4.8 | 49.0 | 24.0 | 49.0 | 24.0 | 49.0 | 43.2 | 49.0 | 43.2 | 49.0 | 0.0 |
| success#NN | 44.0 | 0.0 | 0 | 0.0 | 4.4 | 45.0 | 4.4 | 45.0 | 22.0 | 45.0 | 22.0 | 45.0 | 39.6 | 45.0 | 39.6 | 45.0 | 3878.0 |  the#DT#det:success#NN3878.0;;;;; |
| growth#NN | 44.0 | 0.0 | 0 | 1.0 | 4.4 | 45.0 | 4.4 | 45.0 | 22.0 | 45.0 | 22.0 | 45.0 | 39.6 | 45.0 | 39.6 | 45.0 | 0.0 |
Sentenceinfo: All TWSI substitutions for progress+++59971618: [course(1), transport(1), development(1), growth(1), advance(1), movement(1), advancement(2), transportation(1)]<br/>
Sentenceinfo: TWSI score for sentence 1 by col 1: 1.0<br/>
Sentenceinfo: TWSI score for sentence 1 by col 2: 1.0<br/>
Sentenceinfo: TWSI score for sentence 1 by col 3: 1.0<br/>
Sentenceinfo: TWSI score for sentence 1 by col 5: 1.0<br/>
Sentenceinfo: TWSI score for sentence 1 by col 6: 1.0<br/>
Sentenceinfo: TWSI score for sentence 1 by col 7: 1.0<br/>
Sentenceinfo: TWSI score for sentence 1 by col 8: 1.0<br/>
Sentenceinfo: TWSI score for sentence 1 by col 9: 1.0<br/>
Sentenceinfo: TWSI score for sentence 1 by col 10: 1.0<br/>
Sentenceinfo: TWSI score for sentence 1 by col 11: 1.0<br/>
Sentenceinfo: TWSI score for sentence 1 by col 12: 1.0<br/>
Sentenceinfo: TWSI score for sentence 1 by col 13: 1.0<br/>
Sentenceinfo: TWSI score for sentence 1 by col 14: 1.0<br/>
Sentenceinfo: TWSI score for sentence 1 by col 15: 1.0<br/>
Sentenceinfo: TWSI score for sentence 1 by col 16: 1.0<br/>
Sentenceinfo: TWSI score for sentence 1 by col 17: 1.0<br/>
<br/>
The columns are as explained. The last column is the source (trace) for Level 3.
At the end of the output, you'll receive a summary for each sorting, how well it did in respect to the TWSI score.


---++ Results
We ran the different levels with all the 23k turk sentences and they didn't yield any promising results that are better compared to the DT results itself. That's why we do not pursue this approach any more.
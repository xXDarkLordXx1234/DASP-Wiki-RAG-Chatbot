%META:TOPICINFO{author="RichardSteuer" date="1331055232" format="1.1" version="8"}%
%META:TOPICPARENT{name="RichardSteuer"}%
---+ Contextual Thesaurus

This page documents our work on the next level of the DistributionalThesaurus (DT), the *Contextual Thesaurus*.

The implementation to this research is based on a local UIMA pipeline (i.e. the cluster is not used here). It relies on the output of the *ContextParses* step, see the corresponding paragraph at the DT page.

Furthermore, the Berkeley Database (BDB) is used:

   * the complete DT (simsort-out) is put into one BDB
   * the results of the !FeatureCount step are put into another (this was planned, but due to some errors, a MySQL DB is used)
The goal of this second version of the DT is to examine whether or not a _local_ _context_ could help with automatic similarity (or disambiguation) calculation.

The current results are very promising.

-- Main.RichardSteuer - 2012-01-29

%TOC%

---++ Level 1

I'd like to illustrate this by example. Consider the following sentence:

   * _I started to work it out._
The sentence has the following dependency relations (according to the [[http://nlp.stanford.edu/software/lex-parser.shtml][Stanford Parser]]):

   * nsubj(started-2, I-1)
   * root(ROOT-0, started-2)
   * aux(work-4, to-3)
   * *xcomp(started-2, work-4)*
   * dobj(work-4, it-5)
   * prt(work-4, out-6)
Remember, we read all the sentences (and its parses) from a text file generated by the *ContextParses* step. This is what we read into the UIMA Cas. This way, we instantly have access to all dependency relations for each word within the sentence. We process sentence by sentence. Next, we process each word within that sentence and get its corresponding DT entry. We go through each line of the DT entry and look, how many of the dependency relations (of e.g. _started#VBD_) appear in the feature list of each DT entry (which is a similar word). The (here fictitious) end results (for e.g. _started#VBD_) may look like this:

| 1 | started#VBD | 279.0 | [ ... i#NNP#nsubj ... ] |
| 1 | began#VBD | 142.0  | [ ... work#VB#xcomp ...] |
| 0 | start#VBP | 93.0 | [ ... ] |
| 2 | ... | ... | [ ... ] |

We create a new first column to note the count of how many direct dependency relations appear in the feature list. We call this new column the _local context_. The second column denotes the similar word and the third is the integer similarity count which at the same time denotes the number of features it shares with the DT entry word (e.g. _started#VBD_).

At last, we ran this Level 1 with 1000 parsed sentences and calculated statistics of how many words (and the sentences) at least have a local context of &gt; 0 or &gt; 5 in their DT entry. Most entries will be zero. The results are:

Level 1, 1000 sentences, entry &gt;= 1
| V: | 12,22% |
| N: | 18,96% |
| O: | 35,89% |
| J: | 5,27% |

Level 1, 1000 sentences, entry &gt;= 5
| V: | 4,88% |
| N: | 1,79% |
| O: | 24,02% |
| J: | 0,86% |

(V=Verbs, N=Nouns, J=Adjectives, O=Other)

Read it this way: In the 1000 sentences (parses), 12,22% of the verbs have an entry &gt;= 1 in the contextual column of their DT entry.

---++ Level 2

Level 1 was counting how many of a words dependency relations (within the sentence) appear in each line of its DT entry. For level 2, consider the above sentence and its dependency relations again (_I started to work it out._). Level 2 looks over all the dependency relations (of e.g. _started#VBD_). Let's take _work#VB#xcomp_, which is one feature/relation of _started#VBD_. It will retrieve the DT entry for _work#VB_, loop through its entries, take the relation ending name ( _#xcomp_) and create artificial relations with all the DT entries. If one of those artificial relations appear in one line the original DT entry ( _started#VBD_), this line will get the DT similarity value as a summand. Finally, the sum of all the similarity scores for the features found in the line will be the new local context. Let's take a real example and look at the DT entry for the word _started#VBD_:

| started#VBD |  279.0 |  395.0 |  6 | work#VB:change#VB#xcomp:55.0:10384;work#VB:working#VBG#xcomp:77.0:7134;work#VB:come#VB#xcomp:77.0:22308;work#VB:turn#VB#xcomp:55.0:7267;work#VB:get#VB#xcomp:64.0:79922;work#VB:look#VB#xcomp:67.0:12133; | [saturday#NNP#tmod, says#VBZ#-ccomp, school#NN#dobj, school#NN#nsubj, ... |
| began#VBD | 142.0 | 254.0 | 4 | work#VB:look#VB#xcomp:67.0:12133;work#VB:change#VB#xcomp:55.0:10384;work#VB:working#VBG#xcomp:77.0:7134;work#VB:turn#VB#xcomp:55.0:7267; | [yelling#VBG#xcomp, week#NN#tmod, was#VBD#-ccomp, was#VBD#-advcl, were#VBD#-advcl, and#CC#cc, ... |
| start#VBP | 93.0 | 208.0 | 3 | work#VB:get#VB#xcomp:64.0:79922;work#VB:look#VB#xcomp:67.0:12133;work#VB:working#VBG#xcomp:77.0:7134; | [time#NN#-rcmod, week#NN#tmod, watching#VBG#xcomp, ... |

The columns are as follows:

   * 1. The first is the (similar) word from the DT.
   * 2. The second is the similarity value belonging to the word.
   * 3. The third is the new local context (as explained above, the sum)
   * 4. The fourth is the number of summands for 3)
   * 5. The fifth is the source for the new local context (explaination below). The sources are concatenated and separated by semicolon; there are as many sources as there are summands in 4)
   * 6. The last are the features (the similar words have in common), there are as many features as the number in 2)
A single source entry consists of the following parts:

   * e.g. work#VB:change#VB#xcomp:55.0:10384;
work#VB is the (external) DT entry that was retrieved because work#VB appeared in a dependency relation. change#VB#xcomp is one (similarity) entry from this (external) DT entry that somewhere appears in the (internal/original/local) DT entry line feature list. It has a similarity score of 55.0 and is taken as a summand for the new local context. Finally, 10384 is a global and external count of the feature change#VB#xcomp.

---++ Level 2 Evaluation
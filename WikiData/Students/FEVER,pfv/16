%META:TOPICINFO{author="hanselowski" comment="save topic" date="1527164488" format="1.1" reprev="15" version="16"}%
%META:TOPICPARENT{name="StudentsList"}%
---++ Meeting: 24.05.2018:

   * Evidence extraction: 
      * Problem
         * Analysis predict scores on the evaluation set during training
         * Learner is not learning 
      * Different  
         * score = f(x|c)
         * score = f(x1|c) + f(x2|c) + g(x1,x2)
      * Methods
         * Glove
         * Stacked lstm
         * bilstm 
         * Combine tfidf representations with sentence embeddings 
      * Error analysis 
         * Entities resolution 
      * New problem setting: randomly sample sentences from the 5 documents

   * Claim validaion
      * 20 sentences with gold labels
      * Decomposable attention with many sentencees
      * Sentences encoder with attention on the sentence level  


   * Document level:
      * Entitiy linking on the development set running, results coming soon 



---++ Meeting: 18.05.2018:


   * [[https://arxiv.org/pdf/1704.00051.pdf][Facebook paper]], similar task also working on the wikipedia data
   * Evidence extraction: 
      * Google encoder can be fine tuned with small learning rate
      * Facebook document reader attention LSTM
         * Additional features: named entities, word overallap

      * TODO:
         * Sentenece encoder only pairs no negative sampling
         * Sentenece encoder with negative sampling
         * Attention model: sentence level (hierarchical encoding) (sequential information might help)

   * Textual entailment:
      * Problem definition: classification on text snippets, paragraphs, documents (multiple sentences) 
      * Stacked biLSTM 3 layer with shortcuts: 66%
      * Sentence level VS tocken level (in real problem setting which have to deal multiple sentences)
         * Try out SNLI models
      * Sentence level model to be also implemented (end-to-end sentence encoder and classifier (not need for lstm for for classification on sentence level)) 
   

---++ Meeting: 15.05.2018:

   * Document retrieval: 
      * Daniil will investigate entity linking methods for document retrieval

   * Evidence extraction: 
      * Use ranking loss: hinge loss, ranking loss, cosine similarity, maximum margin loss
      * More negative sentences to be taken into account 
      * Use GloVe embeddings trained on Wikipedia

   * Textual entailment: 
      * Baseline claim validation: 80% 
      * Conditional baseline: 62% 
      * Try out different SNLI models available models 
      * SNLI using alignment according to attention, are we also able to use this?  

   * Joint model:
      * Maxime suggested to optimize the ranking encoder as well as mutual information encoder jointly 

---++ Meeting: 09.05.2018:

   * Evidence extraction:
      * TF-Idf baseline 45% 
      * Universal encoder 40% recall
      * TODO:
         * Upper bound 55% (to be clarified) (to be identified for the development set: depending on the upper bound we have to focus more on document retrieval or evidence extraction)
         * We need to find out how many documents we should retrieve in order to extract the correct set of sentences 
         * Error analysis to be perform: what kind of errors are made, how can we improve
         * Other approaches: 
            * Train our own sentence encoder on the data and use ranking loss, use Google sentence encoder with trained units
            * Use average word embeddings
            * LSTM with attention from Andreas Rückle
         * Datasets:
            * Gold and randomly sampled
            * Only gold documents
            * Run the baseline document retrieval in order to extract the documents    

   * Evaluation:
      * Each claim has several evidence sets for support and we have to identify one of them 

   * Textual entailment: 
      * Google universal encoder (53%) does not outperform baseline (63% DecAttention)
      * TODO:
         * Need to train our own models starting with lstm, cond lstm and so on

((
   * Claim validation (classification problem): basically the same classifier as for the evidence aggregation
      * SOA SNLI models: u-v||u*v||u||v 
      * Sentence encoder from Andreas Rückle
 ))

   * TODO: Set up git lab and share with all

---++ Meeting: 02.05.2018:
   * Evaluation: 5 sentences the rest will not be considered

   * Aisha will apply contextual ranking methods in order to evidence extraction (Hao needs to provide the data)

   * Joint model: use an attention model for claim validaion
  
   * Evidence extraction
      * Extend the dataset with more irrelevant documents since in the real setting we will have documents with no evidence
      * Evidence extraction first results: We beat the baseline but recall is quite low since the dataset only contains a small number of evidence (biased distribution)
      * TODO Hao: 
         * Implement Deep rank model: scoring with negative examples be pushed away: https://arxiv.org/pdf/1404.4661.pdf

   * Claim validation
      * Prepared the dataset from training set with 5 sentences for each claim with random examples from additional unrelated sentences 
      * TODO Zile:
         * Slack ask question regarding number of sentences, are we just allowed to use the 5 sentences which are selected as evidence or more information for claim validation
         * Implement the FEVER baseline and reproduce the results
         * Implement classifiers for claim validation:
            * Google universal encoder (Facebook InferSent)
            * Separate encoder for the sentence and the claim
            * Conditional encoding
         * Find out state of the art models for SNLI and multiNLI (we could also use them)
   
   * TODOs: 
      * Evaluation metric for the task to be clarified: formal definition
      * Information retrieval: talk to Andreas, Richard, Chris, ... how this can be done?



---++ Important information about the task

   * Links:
      * [[https://arxiv.org/pdf/1803.05355.pdf][paper]] 
      * [[http://fever.ai/task.html][shared task]]
   * Modules:
      * Information retrieval: We need to create an index for search using [[https://lucene.apache.org/][Lucene]]; [[https://github.com/lemire/IndexWikipedia][Creating Lucene index for Wikipedia dump]]
         * Elastic search, BM 24, faiss Facebook semantic search 
      * Evidence aggregation (consider as classification problem on sentence level): universal encoders (Facebook, Google), try to train our own sentence encoder, claim validation models, conditional encoders, SNLI multiNLI models ....
         * Considering as ranking problem Can be later considered as content extraction or contextual ranking problem in order to facilitate the collection of linked evidence
         * Considering as content selection: Simply generate conditional sequences, Beam search, Monte Carlo Tree search, ILP, Decoder LSTM condition on claim select the most likely sentence, ... 
         * Considering as ranking problem: ...
      * Claim validation (classification problem): basically the same classifier as for the evidence aggregation
         * SOA NLI models: u-v||u*v||u||v 
      * Joint learning of sentence aggregation and claim validation: Memory networks, ILP, reinforcement learning

   * Models to be tried out:
      * Google universal encoder
      * Facebook InferSent
      * Separate encoder for the sentence and the claim
      * Conditional encoding
      * Attention based models
      * ...
      
   * Joint model:
      * One hop reasoning: Sentence encoder, conditional decoding LSTM (generates a sequence of evidence), inference module (predicts a label for the claim on the basis of the evidence sequence),  joint loss function for evidence selection and label prediction, (attention for selection, keys values to be considered)
      * Three hop reasoning: Same as above but we have additionally three options to choose from, additional selection module? 


   * Miscellaneous:
      * Documents: The document collection is whole Wikipedia (5.4M pages)
      * Evidence: "in 31.75% of the claims more than one sentence was considered appropriate evidence. Claims require composition of evidence from multiple sentences in 16.82% of cases. Furthermore, in 12.15% of the claims, this evidence was taken from multiple pages."
      




-- Main.AndreasHanselowski - 2018-04-24
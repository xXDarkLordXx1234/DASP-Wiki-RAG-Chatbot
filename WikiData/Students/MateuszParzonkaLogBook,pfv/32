%META:TOPICINFO{author="MateuszParzonka" date="1288465907" format="1.1" version="32"}%
%META:TOPICPARENT{name="MateuszParzonka"}%
---++Log Book

---+++++ 25 Oct 2010
Installed the web1t Google n-grams on the pool-machine. Takes up 90 GB of space. Integrated API that works in logarithmic time. Used in an annotator lookup for ca. 450000 nchunks took approx 1-2 hours.

---+++++ 24 Oct 2010
Baseline on C&M2007 using only subsets of {Token} with given post-processing. Sets are derived from the TextRank-paper to evaluate the gain of TextRank.<br>
Since this is supposed to be the candidate extraction step, high recall is crucial.<br>
Result: Tokens (N,A) merged (kept parts) has the highest recall with acceptable precision compared to NGrams. Removing the dangling adjectives (adjectives, that could not be merged with a noun) yields higher precision with minimal loss of recall, further increasing the quality of this token based candidate set.

| | *P* | *R* | *F* | *#IndexEntries* | *#GoldEntries* | Timestamp |
|Tokens (N) FilteredGold [BbiEvaluatorExact]|5.917 | 77.24 | 10.99 | 146094 | 11191 |25.10.2010 03:43|
|Tokens (N) [BbiEvaluatorExact]|5.917 | 64.16 | 10.83 | 146094 | 13471 |25.10.2010 03:43|
|Tokens (A) FilteredGold [BbiEvaluatorExact]|0.437 | 1.769 | 0.701 | 45329 | 11191 |25.10.2010 04:05|
|Tokens (A) [BbiEvaluatorExact]|0.437 | 1.47  | 0.673 | 45329 | 13471 |25.10.2010 04:05|
|Tokens (N) FilteredGold [BbiEvaluatorExact]|5.917 | 77.24 | 10.99 | 146094 | 11191 |25.10.2010 03:56|
|Tokens (N) [BbiEvaluatorExact]|5.917 | 64.16 | 10.83 | 146094 | 13471 |25.10.2010 03:56|
|Tokens (N,A) FilteredGold [BbiEvaluatorExact]|4.675 | 77.62 | 8.819 | 185811 | 11191 |25.10.2010 04:12|
|Tokens (N,A) [BbiEvaluatorExact]|4.675 | 64.48 | 8.718 | 185811 | 13471 |25.10.2010 04:12|
|Tokens (N,A) merged FilteredGold [BbiEvaluatorExact]|2.935 | 82.28 | 5.668 | 313754 | 11191 |25.10.2010 04:21|
|Tokens (N,A) merged [BbiEvaluatorExact]|2.956 | 68.84 | 5.668 | 313754 | 13471 |25.10.2010 04:21|
|Tokens (N,A) merged (kept parts) FilteredGold [BbiEvaluatorExact]|2.666 | 92.06 | 5.181 | 386500 | 11191 |25.10.2010 04:29|
|Tokens (N,A) merged (kept parts) [BbiEvaluatorExact]|2.682 | 76.94 | 5.183 | 386500 | 13471 |25.10.2010 04:29|
|Tokens (N,A) merged (kept parts, no dangling adj) FilteredGold [BbiEvaluatorExact]|2.959 | 91.68 | 5.732 | 346783 | 11191 |25.10.2010 15:30|
|Tokens (N,A) merged (kept parts, no dangling adj) [BbiEvaluatorExact]|2.977 | 76.62 | 5.73  | 346783 | 13471 |25.10.2010 15:30|

---+++++ 22 Oct 2010
TextRank using different setting on unsegmented dataset. NChunks perform best, comforms to the results in _zesch2009approximate_ : SP dataset of long documents has R-precision of 12% using NChunks.

| | *P* | *R* | *F* | *#IndexEntries* | *#GoldEntries* | Timestamp |
|Lemma (N,A), windowSize=2, weighted=false [BbiEvaluatorExact]|1.271 | 1.195 | 1.232 | 12669 | 13471 |22.10.2010 22:11|
|Lemma (N,A), windowSize=2, weighted=true [BbiEvaluatorExact]|1.492 | 1.403 | 1.446 | 12669 | 13471 |22.10.2010 22:02|
|Lemma (N,A), windowSize=3, weighted=false [BbiEvaluatorExact]|1.184 | 1.114 | 1.148 | 12669 | 13471 |22.10.2010 22:33|
|Lemma (N,A), windowSize=3, weighted=true [BbiEvaluatorExact]|1.334 | 1.255 | 1.293 | 12669 | 13471 |22.10.2010 22:22|
|Lemma (N,A), windowSize=4, weighted=false [BbiEvaluatorExact]|1.058 | 0.995 | 1.025 | 12669 | 13471 |22.10.2010 22:59|
|Lemma (N,A), windowSize=4, weighted=true [BbiEvaluatorExact]|1.31  | 1.232 | 1.27  | 12669 | 13471 |22.10.2010 22:46|
|NChunks, windowSize=2, weighted=false [BbiEvaluatorExact]|13.46 | 12.66 | 13.05 | 12669 | 13471 |22.10.2010 21:08|
|NChunks, windowSize=2, weighted=true [BbiEvaluatorExact]|13.57 | 12.76 | 13.16 | 12669 | 13471 |22.10.2010 20:58|
|NChunks, windowSize=3, weighted=false [BbiEvaluatorExact]|13.45 | 12.64 | 13.03 | 12669 | 13471 |22.10.2010 21:28|
|NChunks, windowSize=3, weighted=true [BbiEvaluatorExact]|13.61 | 12.80 | 13.19 | 12669 | 13471 |22.10.2010 21:18|
|NChunks, windowSize=4, weighted=false [BbiEvaluatorExact]|13.39 | 12.59 | 12.98 | 12669 | 13471 |22.10.2010 21:52|
|NChunks, windowSize=4, weighted=true [BbiEvaluatorExact]|13.57 | 12.76 | 13.16 | 12669 | 13471 |22.10.2010 21:40|
|Token (N,A), windowSize=2, weighted=false [BbiEvaluatorExact]|1.271 | 1.195 | 1.232 | 12669 | 13471 |22.10.2010 23:18|
|Token (N,A), windowSize=2, weighted=true [BbiEvaluatorExact]|1.492 | 1.403 | 1.446 | 12669 | 13471 |22.10.2010 23:08|
|Token (N,A), windowSize=3, weighted=false [BbiEvaluatorExact]|1.184 | 1.114 | 1.148 | 12669 | 13471 |22.10.2010 23:41|
|Token (N,A), windowSize=3, weighted=true [BbiEvaluatorExact]|1.334 | 1.255 | 1.293 | 12669 | 13471 |22.10.2010 23:29|
|Token (N,A), windowSize=4, weighted=false [BbiEvaluatorExact]|1.058 | 0.995 | 1.025 | 12669 | 13471 |23.10.2010 00:07|
|Token (N,A), windowSize=4, weighted=true [BbiEvaluatorExact]|1.31  | 1.232 | 1.27  | 12669 | 13471 |22.10.2010 23:54|

---+++++ 21 Oct 2010
Wrote AnnotationFilter. Drops all annotations except of the one specified. Purges your jcas before storing it. <br>
Wrote JCasIterableStore aka JCasIterableIterable. Iterates over JCasIterables : Needed to reuse segments of documents stored to disk. These segments store f.e. ranked keyphrases that can be used lateron in an yet unknown new ranking function.

---+++++ 20 Oct 2010
Wrote MultiSegmentWriter. Stores segmented documents to disk as jcases in XMI-representation.

---+++++ 19 Oct 2010
Tagged a version of thesis: [[https://maggie.tk.informatik.tu-darmstadt.de/svn/dkpro_students/parzonka/bachelor-thesis/tags/bachelor-thesis_0.0.2/bachelor-thesis.pdf][bachelor-thesis_0.0.2]] 

---+++++ 18 Oct 2010
Reading papers, writing thesis. <br>
Prepared pipeline using TextRank with and without segmentation.

---+++++ 17 Oct 2010
First iteration of introduction and Related Work: GlossEx.

---+++++ 15 Oct 2010
Changed thesis structure after review by TZ and working on introductory parts of the thesis and "related work".

---+++++ 14 Oct 2010
Tagged a downloadable beta version of the thesis structure (comments colored red): [[https://maggie.tk.informatik.tu-darmstadt.de/svn/dkpro_students/parzonka/bachelor-thesis/tags/bachelor-thesis_0.0.1_(structure_beta)/bachelor-thesis.pdf][bachelor-thesis_0.0.1]] 

---+++++ 13 Oct 2010
Working on a thesis structure.

---+++++ 12 Oct 2010
*Problem:* Preprocessing failed because the JVM hang while writing the XMI of the augmented JCas of the biggest book. No exception was thrown, XMI has size 0k and Eclipse is still processing since > 17 hours. =XmiWriterCasConsumer_type.xml= had size 0k too. Killed process, rebooted machine and increased heap size to 3GB. Restarted processing.<br>
*Update:* Preprocessing succeeded using the settings specified above. Took 2 hours and 42 minutes on Dual Core Pentium 2.5 Ghz 4GB.

---+++++ 11 Oct 2010
Created baselines for the other indexes:
   * Index.Full ("fine grained") - " _A long index, containing the full reconstructed index entries, which corresponds to a more fine-grained indexing strategy._ "
   * Index.Full.Filtered ("filtered") - " _Excludes the potentially ungrammatical phrases created by reconstruction of the inverted compound entries. Allows us to measure a system performance with a higher upper bound, meaning that a larger number of index entries are present in the text and could be potentially found by an automatic indexing system. Filtering is applied by measuring the frequency of each index entry on the web and discarding all phrases with less then 2 occurences on AltaVista._ "
   * Index.Trimmed ("coarse grained") " _A short index, consisting only of head phrases_ "

| | *P* | *R* | *F* | *#IndexEntries* | *#GoldEntries* |
|.index.full nchunk| 1.261 | 35.65 | 2.436 | 459815 | 16263 |
|.index.full nerheur| 10.16 | 31.42 | 15.35 | 50276 | 16263 |
|.index.full nerstanford [crf3]| 13.67 | 27.20 | 18.20 | 32361 | 16263 |
|.index.full nerstanford [crf3distsim]| 13.58 | 26.74 | 18.01 | 32021 | 16263 |
|.index.full ngram [no punctuation]| 0.112 | 51.61 | 0.223 | 7510468 | 16263 |
|.index.full tfidf [ngram, top 0_0035]| 6.528 | 5.085 | 5.717 | 12669 | 16263 |
|.index.full.filtered nchunk| 1.202 | 44.28 | 2.341 | 459815 | 12485 |
|.index.full.filtered nerheur| 9.756 | 39.28 | 15.63 | 50276 | 12485 |
|.index.full.filtered nerstanford [crf3]| 13.04 | 33.80 | 18.82 | 32361 | 12485 |
|.index.full.filtered nerstanford [crf3distsim]| 12.96 | 33.25 | 18.65 | 32021 | 12485 |
|.index.full.filtered ngram [no punctuation]| 0.107 | 64.12 | 0.213 | 7510468 | 12485 |
|.index.full.filtered tfidf [ngram, top 0_0035]| 6.323 | 6.416 | 6.369 | 12669 | 12485 |
|.index.trimmed nchunk| 1.608 | 54.88 | 3.125 | 459815 | 13471 |
|.index.trimmed nerheur| 12.63 | 47.14 | 19.92 | 50276 | 13471 |
|.index.trimmed nerstanford [crf3]| 17.48 | 42.00 | 24.69 | 32361 | 13471 |
|.index.trimmed nerstanford [crf3distsim]| 17.50 | 41.61 | 24.64 | 32021 | 13471 |
|.index.trimmed ngram [no punctuation]| 0.146 | 81.59 | 0.292 | 7510468 | 13471 |
|.index.trimmed tfidf [ngram, top 0_0035]| 10.39 | 9.777 | 10.07 | 12669 | 13471 |
| GiD .index.full nchunk| 1.261 | 68.59 | 2.476 | 459815 | 8453 |
| GiD .index.full nerheur| 10.16 | 60.45 | 17.40 | 50276 | 8453 |
| GiD .index.full nerstanford [crf3]| 13.67 | 52.34 | 21.68 | 32361 | 8453 |
| GiD .index.full nerstanford [crf3distsim]| 13.58 | 51.44 | 21.49 | 32021 | 8453 |
| GiD .index.full ngram [no punctuation]| 0.108 | 96.38 | 0.217 | 7510468 | 8453 |
| GiD .index.full tfidf [ngram, top 0_0035]| 6.528 | 9.784 | 7.831 | 12669 | 8453 |
| GiD .index.full.filtered nchunk| 1.202 | 68.53 | 2.363 | 459815 | 8067 |
| GiD .index.full.filtered nerheur| 9.756 | 60.80 | 16.81 | 50276 | 8067 |
| GiD .index.full.filtered nerstanford [crf3]| 13.04 | 52.32 | 20.88 | 32361 | 8067 |
| GiD .index.full.filtered nerstanford [crf3distsim]| 12.96 | 51.46 | 20.71 | 32021 | 8067 |
| GiD .index.full.filtered ngram [no punctuation]| 0.104 | 96.36 | 0.207 | 7510468 | 8067 |
| GiD .index.full.filtered tfidf [ngram, top 0_0035]| 6.323 | 9.929 | 7.726 | 12669 | 8067 |
| GiD .index.trimmed nchunk| 1.608 | 66.07 | 3.14  | 459815 | 11191 |
| GiD .index.trimmed nerheur| 12.63 | 56.75 | 20.66 | 50276 | 11191 |
| GiD .index.trimmed nerstanford [crf3]| 17.48 | 50.55 | 25.98 | 32361 | 11191 |
| GiD .index.trimmed nerstanford [crf3distsim]| 17.50 | 50.09 | 25.94 | 32021 | 11191 |
| GiD .index.trimmed ngram [no punctuation]| 0.144 | 96.95 | 0.289 | 7510468 | 11191 |
| GiD .index.trimmed tfidf [ngram, top 0_0035]| 10.39 | 11.76 | 11.03 | 12669 | 11191 |

Initiated preprocessing of XMI containing all needed annotations.

---+++++ 9 Oct 2010
Researched for Back-Of-The-Book Indexing papers. Updated [[MateuszParzonkaReferences][Bibliography]]. <br>
Reorganized jUnit-tests.<br>
Fixed all warnings in project.<br>

---+++++ 8 Oct 2010
Conducted baseline experiments on pool computer: <br>
(GiD) means: Gold Index Phrases appeared in Document.

| | *P* | *R* | *F* | *#IndexEntries* | *#GoldEntries* |
| NGram | 0.146 | 81.59 | 0.292 | 7510468 | 13471 |
| NGram (GiD) | 0.144 | 96.95 | 0.289 | 7510468 | 11191 |
| NGram [tfidf-ranked, top 0.0035%] | 10.39 | 9.777 | 10.07 | 12669 | 13471 |
| NGram [tfidf-ranked, top 0.0035%] (GiD) | 10.39 | 11.76 | 11.03 | 12669 | 11191 |
| NChunk | 1.608 | 54.88 | 3.125 | 459815 | 13471 |
| NChunk (GiD) | 1.608 | 66.07 | 3.14  | 459815 | 11191 |
| NERheur | 12.63 | 47.14 | 19.92 | 50276 | 13471 |
| NERheur (GiD) | 12.63 | 56.75 | 20.66 | 50276 | 11191 |
| NERstanford [crf3] | 17.48 | 42.00 | 24.69 | 32361 | 13471 |
| NERstanford [crf3] (GiD) | 17.48 | 50.55 | 25.98 | 32361 | 11191 |
| NERstanford [crf3distsim] | 17.50 | 41.61 | 24.64 | 32021 | 13471 |
| NERstanford [crf3distsim] (GiD) | 17.50 | 50.09 | 25.94 | 32021 | 11191 |

Why NGram (GiD) has no recall of 100% using the BbiEvaluatorExact: Because =GiD(phrase) = true= when =document.contains(phrase) = true=. But =matches(phrase, gold) = true= when =phrase.equals(gold) = true=. Therefore we miss the plurals: when gold:= "cannon", phrase:="cannons",  then "cannon" is contained by "cannons" but not equal.<p>
Updated [[MateuszParzonkaInstallation][installation instructions]]. Refresh is needed after postUpdateHook.

---+++++ 7 Oct 2010
Refactored and deployed project at a computer in the UKP-Pool. Supervisors can now check out and install the project using [[MateuszParzonkaInstallation][this instructions]]. All documented experiments should be conductable. I've just initiated the calculation of all baseline experiments on the pool machine to verify results. The heapsize of 2GB was enough so far, so the experiments should run through until the next morning.

---+++++ 6 Oct 2010
Refactoring, commenting and begin of deployment of project on a machine in the UKP-Pool.

---+++++ 5 Oct 2010
Tfidf-baseline: The extracted index entries are a subset of the 4-grams and ranked by Tfidf (created over the set of 29 in the dataset).
The first 0.35 percent of these ranked index entries are taken into account for evaluation.

*Tfidf on Ngrams, Top 0.35 percent*
| | *P* | *R* | *F* | *#IndexEntries* | *#GoldEntries* |
| All Gold Entries | 10.39 | 9.777 | 10.07 | 12669 | 13471 |
| Only Gold Entries that appeared in document | 10.39 | 11.76 | 11.03 | 12669 | 11191 |

Quite similar to the values of C&M with the same percentage of ranked 4-grams, although their Tfidf-Table was created using the British National Corpus.
| | *P* | *R* | *F* | *#IndexEntries* | *#GoldEntries* |
| C&M with British National Corpus as document-set | 10.33 | 11.12 | 10.71 | ? | ? |

---+++++ 3 Oct 2010
Heavy refactoring of pipelines and experiments. <br>
Developing some utilities to make annotators testable.

---+++++ 2 Oct 2010
Heavy refactoring of pipelines and experiments.

---+++++ 30 Sep 2010
Researched in type system agnostic annotation techniques.<br>
Implemented some tests.

---+++++ 27 Sep 2010
Implemented a TfidfConsumer that works with the C&M Dataset.
Hopefully i will manage to evaluate a baseline of tfidf-weighted n-grams cut at a threshold derived from the appiopriate ratio (tokens-in-book / entries-in-index) for the next status meeting.

---+++++ 25 Sep 2010
Baselines for Stanford NER:
| | *P* | *R* | *F* |
| Stanford NER CRF3 | 18.33 |42.30| 25.58|   
| Stanford NER CRF3Distsim |18.40| 41.93|25.58|
| Lingpipe Toolkit (used by C&M)  |15.79|39.10|22.49|

   * Stanford NER performs significantly better then the NER from the Lingpipe Toolkit which C&M used.
   * *Problem:* Stanford offers a set of newer Models (from 2008) which doesn't work with the DKPro StanfordNamedEntityRecognizer.
Rewrote the DKPro NGramAnnotator to allow a more sophisticated filtering of tokens. While using exact matching when evaluating it seems impossible to have an recall higher then 81.63 (at least with the used BreakIteratorSegmenter and exact evaluation).

%EDITTABLE{}%
| | *P* | *R* | *F* |
| No filtering | 0.132 | 81.33 | 0.263 |
| Filtering single characters that are punctuation signifying sentence endings | 0.137 | 81.32 | 0.273 |
| Filtering single characters that are punctuation | 0.146 | 81.57 | 0.292 |
| Filtering single characters that are not alphanumerics | 0.146 | 81.63 | 0.292 |

*Problem:* DKPro TfidfConsumer builds such a huge HashMap during processing that the heap size is never (even when maxed to over 2.5 GM) enough. The hashmap stored to disk would be extremely big too. Trying to develop a tfidf annotator that uses one serialization per document for the term frequencies  instead of one per corpus.

---+++++ 24 Sep 2010
First baselines for "pure" candidate extraction (all micro):

*NGrams:*
| | *P* | *R* | *F* |
| We | 0.146 | 81.63 | 0.292 |
| C&M | 0.01 | 84.20 | 0.02 |
So we have a slightly worse recall. But we have a ten-times higher precision then C&M do. _(Updated on 28.9. with best NGram filter setting)_


*NChunks:*
| | *P* | *R* | *F* |
| We | 1.71 | 55.3 | 3.32 |
| C&M | 4.40 | 28.99 | 7.64 |
They use NP-Chunks, we use N-Chunks. So we have very different results.


*Heuristic Named Entity Recognition:*
| | *P* | *R* | *F* |
| We | 3.11 | 47.37 | 20.54 |
| C&M | 11.43 | 43.34 | 18.08 | 
Our heuristic seems to be better then C&Ms.
 
---+++++ 23 Sep 2010
Heavy refactoring. Decided to separate the concerns of creating index entries and evaluation them into ExtractionPipelines and a EvaluationPipelines.

---+++++ 22 Sep 2010
Extracted the "unproblematic" subset from the main data set following Csomai & Mihalcea.

---+++++ 21 Sep 2010
Processing and evaluation of collections is working.

---+++++ 18 Sep 2010
Wrote PRF-Evaluator for back-of-the-book-indexing using gold index entries.

---+++++ 17 Sep 2010
Pipelines are now jUnit-testable.

---+++++ 15 Sep 2010
Wrote base pipeline for testing and experiments.

---+++++ 13 Sep 2010
Split initial jCas into segments with demultiplier and applied a simplified LexRank to the sub-CASes. Wrote a AnnotationPrinter to visualize the results.

---+++++ 12 Sep 2010
Played around with LexSemRank, seems to be quite slow. Using the  3,2GB "wikipedia_lemma_en" esa cached index.

---+++++ 10 Sep 2010

Played around with TextRank.

---+++++ 09 Sep 2010

Research about State-Of-The-Art Keyphrase Extraction. 

---+++++ 08 Sep 2010

Checked in project
//@GrabResolver(name='maven.central',
//		root='http://repo1.maven.org/maven2/')
@GrabResolver(name='ukp-oss-releases', 
      root='http://zoidberg.ukp.informatik.tu-darmstadt.de/artifactory/public-releases')
@GrabResolver(name='ukp-oss-snapshots',
      root='http://zoidberg.ukp.informatik.tu-darmstadt.de/artifactory/public-snapshots')
@GrabResolver(name='apache.releases', 
      root='https://repository.apache.org/content/groups/public/')
@GrabResolver(name='apache.snapshots', 
      root='http://repository.apache.org/snapshots')
// next repo for jwnl
@GrabResolver(name='open-nlp',
		root='http://opennlp.sourceforge.net/maven2')

@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.opennlp-asl', 
      version='1.5.0-SNAPSHOT')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.io.text-asl', 
      version='1.5.0-SNAPSHOT')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.io.imscwb-asl', 
      version='1.5.0-SNAPSHOT')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.opennlp-asl', 
      version='1.5.0-SNAPSHOT')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.opennlp-model-tagger-de-maxent', 
      version='20120616.0')      
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.stanfordnlp-gpl', 
      version='1.5.0-SNAPSHOT')      
//@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
//      module='de.tudarmstadt.ukp.dkpro.core.matetools-gpl', 
//      version='1.5.0-SNAPSHOT')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.io.tcf-asl', 
      version='1.5.0-SNAPSHOT')
@Grab(group='de.wangtang', 
      module='diplom', 
      version='1.1.0-SNAPSHOT')

import static org.apache.uima.fit.factory.AnalysisEngineFactory.createEngineDescription;
import static org.apache.uima.fit.factory.CollectionReaderFactory.createReader;

import java.io.File;
import java.io.IOException;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.apache.commons.lang.StringUtils;
import org.apache.uima.UIMAException;
import org.apache.uima.analysis_engine.AnalysisEngineDescription;
import org.apache.uima.collection.CollectionReader;
import org.uimafit.pipeline.SimplePipeline;

import de.tudarmstadt.ukp.dkpro.core.io.tcf.TcfWriter;
import de.tudarmstadt.ukp.dkpro.core.matetools.MateLemmatizer;
import de.tudarmstadt.ukp.dkpro.core.opennlp.OpenNlpPosTagger;
import de.tudarmstadt.ukp.dkpro.core.stanfordnlp.StanfordSegmenter;
import de.wangtang.diplom.additional.DocxWriter;
import de.wangtang.diplom.additional.MetaphorAnnotator;
import de.wangtang.diplom.additional.MetaphorToNeConverter;
import de.wangtang.diplom.additional.YearPublishedAnnotator;
import de.wangtang.diplom.classifier.FeatureVector;
import de.wangtang.diplom.uima.annotator.CandidateAnnotator;
import de.wangtang.diplom.uima.annotator.CandidateAnnotator.CandidateType;
import de.wangtang.diplom.uima.annotator.CandidateVectorCreator;
import de.wangtang.diplom.uima.annotator.CandidateVectorCreator.SimilarityMeasure;
import de.wangtang.diplom.uima.reader.DocxHighlightedTextReader;

public class IdentifyMetaphors
{
	private static final int RESOURCES_PATH = 0;
	private static final int INPUT_PATH = 1;
	private static final int OUTPUT_PATH = 2;
	private static final int OUTPUT_FORMAT = 3;
	private static final int TOP_K = 4;
	
	// public static String SOURCE = "src/main/resources";// Config.DOCUMENTS_DIR
	// public static String TARGET = "target/tcf";
	public static List<String> FORMATS = Arrays.asList("tcf", "docx");

	public static void main(String[] args)
		throws IOException, UIMAException
	{
		String resources, source, target, format;
		int topK = 50;
		String[] features = [FeatureVector.FOCUS_VS_CONTEXT,FeatureVector.CONTEXT_VS_CONTEXT, FeatureVector.SUB, FeatureVector.NEGATIVE];
//				,FeatureVector.SEL_ASSOC };
		File lucenePath;

		if (args.length < 4) {
			System.out
					.println("Usage: IdentifyMetaphors.groovy resource-directory input-directory output-directory output-format [topK] [features...] ");
			System.out.println("\nMandatory parameters:");
			System.out.println(" - resources-directory: path to the folder where the resources (e.g. Lucene index, GBN files, etc.) are located.");
			System.out.println(" - input-directory: path to the folder where the docx files are located.");
			System.out.println(" - output-directory: path to the folder where the annotated documents should be written to.");
			System.out.println(" - output-format: one of the following formats: " + StringUtils.join(FORMATS, " "));
			System.out.println("\nOptional parameters:");
			System.out
					.println(" - topK: an integer ___. Default value is 50, reasonable values range from 0 to 100.");
			System.out
					.println(" - features: list of features to use, separated by space. Have to be from this list:");
//			for (String f : FeatureVector.FEATURES) {
//				System.out.println("\t" + f + "\t" + descriptions.get(f));
//			}
			return;
		}
		else {
			resources = args[RESOURCES_PATH];
			source = args[INPUT_PATH];
			target = args[OUTPUT_PATH];
			format = args[OUTPUT_FORMAT];
			
			lucenePath = new File(resources, "lucene");

			if (!FORMATS.contains(format)) {
				System.out.println("* Format has to be one of: " + StringUtils.join(FORMATS, " "));
				return;
			}

			if (args.length > 4) {
				try {
					topK = Integer.parseInt(args[TOP_K]);
				}
				catch (NumberFormatException e) {
					System.out.println("* topK has to be an integer, but you specified [" + args[TOP_K]
							+ "]. Using [" + topK + "] instead.");
				}
			}
//			if (args.length > 5) {
//				features = new String[args.length - 4];
//				for (int i = 4; i < args.length; i++) {
//					if (!FeatureVector.FEATURES.contains(args[i])) {
//						System.out.println("* The feature [" + args[i]
//								+ "] you specified does not exist.");
//						return;
//					}
//					features[i - 4] = args[i];
//				}
//			}
		}

		// read docx
		CollectionReader reader = createReader(DocxHighlightedTextReader.class,
				DocxHighlightedTextReader.PARAM_CREATE_NO_BG_ANNO, false,
				DocxHighlightedTextReader.PARAM_LANGUAGE, "de",
				DocxHighlightedTextReader.PARAM_SOURCE_LOCATION, source,
				DocxHighlightedTextReader.PARAM_PATTERNS, ["[+]*.docx"]);
		
		// annotate the year a document was published
		AnalysisEngineDescription year = createEngineDescription(YearPublishedAnnotator.class,
				YearPublishedAnnotator.PARAM_SOURCE_PATH, source);

		// tokenize, split
		AnalysisEngineDescription tokenizer = createEngineDescription(StanfordSegmenter.class,
				StanfordSegmenter.PARAM_CREATE_SENTENCES, true);

		// tag and lemmatize
//		AnalysisEngineDescription tagger = createPrimitiveDescription(TreeTaggerPosLemmaTT4J.class,
//				TreeTaggerPosLemmaTT4J.PARAM_LANGUAGE_CODE, "de");
		AnalysisEngineDescription tagger = createEngineDescription(
				createEngineDescription(OpenNlpPosTagger.class,
						OpenNlpPosTagger.PARAM_LANGUAGE, "de")
						,
				createEngineDescription(MateLemmatizer.class,
						MateLemmatizer.PARAM_LANGUAGE, "de",
						MateLemmatizer.PARAM_VARIANT, "tiger")
						);
		
		// find candidates
		AnalysisEngineDescription candidates = createEngineDescription(CandidateAnnotator.class,
				CandidateAnnotator.PARAM_CANDIDATE_TYPES, CandidateType.values());

		// create feature vectors
		AnalysisEngineDescription vectors = createEngineDescription(
				CandidateVectorCreator.class,
				CandidateVectorCreator.PARAM_RESOURCE_PATH, resources, 
				CandidateVectorCreator.PARAM_SIM_MEASURE, SimilarityMeasure.LUCENE, 
				CandidateVectorCreator.PARAM_SIM_RESOURCE_PATH, lucenePath,
				CandidateVectorCreator.PARAM_FEATURES, features,
				CandidateVectorCreator.PARAM_SINGLE_FOCUS, false,
				CandidateVectorCreator.PARAM_TIME_FRAME, 30,
				CandidateVectorCreator.PARAM_VECTOR_TOP_K, topK);

		AnalysisEngineDescription classifier = createEngineDescription(MetaphorAnnotator.class,
				MetaphorAnnotator.PARAM_FEATURES, features,
				MetaphorAnnotator.PARAM_TOP_K, topK);

		// write to TCF Weblicht format
		AnalysisEngineDescription writer;
		if (format.equalsIgnoreCase("tcf")) {
			writer = createEngineDescription(
					createEngineDescription(MetaphorToNeConverter.class),
					createEngineDescription(TcfWriter.class,
						TcfWriter.PARAM_STRIP_EXTENSION, true,
						TcfWriter.PARAM_TARGET_LOCATION, target,
						TcfWriter.PARAM_FILENAME_SUFFIX, ".tcf"));
		}
		else if (format.equalsIgnoreCase("docx")) {
			writer = createEngineDescription(DocxWriter.class,
					DocxWriter.PARAM_STRIP_EXTENSION, true,
					DocxWriter.PARAM_TARGET_LOCATION, target);
		}
		else {
			throw new IllegalArgumentException("Not implemented yet.");
		}

		SimplePipeline.runPipeline(reader, 
//				year, 
				tokenizer,
				tagger,
				candidates, vectors, classifier
				,
				writer
				);
	}
}

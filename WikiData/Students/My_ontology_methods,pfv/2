%META:TOPICINFO{author="LizhenQu" date="1186579488" format="1.1" reprev="2" version="2"}%
%META:TOPICPARENT{name="LizhenQu"}%

For each semantic relation a statistical model will be learned and used to predict the relation between unknown words. 5 types of relation will be covered. Hypernymy(Hyponymy), Meronymy(holonymy), Synonymy, Entailment, Troponymy. 
The method introduced in paper (Snow 2006) is utilized as the basic approach to learn the models and the core of the whole system. The core system serves also as a baseline system to see how blog specific processing steps improve the quality of the learned ontology.<br />The core learned model using this method is based on the extracted dependency paths from the corpus sentences.  The occurence frequency of each dependency paths in a word pair build the feature vector of the word pair. So the differences of the core models between different semantic relation types are the different label of the feature vector (Hypernym, Meronym) and the different set of word pairs. 
<br />The core model is not sufficient to identify new word relation. The core model will be extended in several ways. One approach used in (Snow 2006) to learn hypernym relation is that the similarity of each noun is interpreted as occurrence of the context words. Based on the similarity measures a model is learned to identify the taxonomic cousin relation between nouns. The prediction of the cousin model is merged with that of the core model for a better classification. Another text based method is based on the state of art research  that some relations need addtional information for prediction, e.g. Meronymy need the relative position of 2 concepts in a taxonomy as addtional features.<br />One important approach of extending the model is to use blog specific features to extend the core feature matrix or build a seperate model as the above solution.  The new features focus on topic related terms and tags. The coocurrence between them builds a matrix to represent the strength of the relation between 2 words. The link structure may also be used to improve the prediction accuracy. Any further approach will also be conducted after more detailed analysis.
<br />Additionally, the blog corpus requires additional preprocessing steps. Duplicate and near duplicate elimination are expected to build better training set. splog detection and community analysis are expected to improve the learned ontology quality. Some parser enhancement preprocessing steps are already implemented.

Core System Overview:<br />The core system consists of standard preprocessing steps and all methods stated in (Snow 2005), (Snow 2006) and the referenced papers from the 2 papers. <br />

Developement cost:<br />The implementation of the methods in (Snow 2006) is the most time consuming part of the whole system. Due to the need of specific design for dealing with huge volume data it will take over 60% time. The extended models are basically extension of the core model, which add some addtional features. So it will only take 20% of the remaining time. The other 20% time is for splog detection and link analysis.

-- Main.LizhenQu - 08 Aug 2007
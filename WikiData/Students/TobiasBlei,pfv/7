%META:TOPICINFO{author="blei" comment="" date="1597222829" format="1.1" reprev="7" version="7"}%
%META:TOPICPARENT{name="StudentsList"}%
---+++ *Bachelor-Thesis: Style Transfer for Creative Text Generation* 

*Start date:* 27.05.2020

*Supervisor:* Kevin Stowe


---+++++ Week 27.05 - 03.06.2020
   * Done:
      * Code refactoring.
      * Trained it on the lyric + blogs data.
      * Shrinked the vocabulary from 75k -> 25k
      * Read: [[https://paperswithcode.com/paper/multiple-text-style-transfer-by-using-word][Multiple Text Style Transfer by using Word-level Conditional Generative Adversarial Network with Two-Phase Training]]
      * Started reading: [[https://arxiv.org/abs/1706.03762][Attention Is All You Need]] 
   * Problems:
      * Performance is really bad, it takes 24h for 45.000 iterations, not fully utilizing the GPU.
      * The resulting transfers are bad, due to the low number of iterations.
   * TODO
      * Reclone the repository and train it on the original data, to check the performance.
      * Try to train with fewer tokens.
      * Build my own transformer model.
      * Use transfer learning for metaphore style transfer or set the metaphore style as an additional style (N:N transfer).
      * Train on the metaphore data directly.

---+++++ Week 03.06 - 10.06.2020
   * Done:
      * I recloned the Style-Transformer-Repo and trained it on their original Data to test out the training time. It trained alot faster. The major difference between their data and the data I'm using is the sequence length.
      * Therefore I shortened the sequence length for my data. This improved the training performance. Furthermore I used fixed pretrained embeddings which again increased the training speed significantly because it takes less memory and therefore I can increase the batch size from 8 => 100. All in all I can now train the model alot faster.
      * I tried to implement the model from scratch using PyTorch libraries and the Huggingface library. This was quite difficult because they are not designed to run differentiable inference. This could improve as they are currently developing a EncoderDecoder Interface.
       
   * Problems:
      * The results are bad after training it for a while. It learns more or less the identity, not more. A reason could be, that differentiating a short sequence from a blog post and from a lyric is very hard. Even I can't really differentiate the majority. Lyrics are only differentiable from other text, if they are fully viewed (multiple sentences). But the training becomes too expensive with too long sequences.
      * Sometimes the training process randomly crashes with a cuda error. I didn't find a specific line or help on the internet.
      
   * TODO
      * The main task is to find and use new data.  
      * Fix the mentioned cuda error.


---+++++ Week 10.06 - 17.06.2020
   * Done:
      * I trained the model for a lot longer, to see if it maybe learns more, but it didn't.
      * I build a good inference and random inference interface. 
      * I fixed bugs in the model output => text logic.
      * I inspected the outputs with the new logic and noticed a few bugs. => The model doesnt produce <pad>'s or <eos> tokens. Even though it is trained on it.
      * I build preprocessing for training on sentences only. => Didn't improve the results.
   * Problems:
      * I still dont have new data
      * Without Huggingface I can't use pretrained models
   * TODO
      * Build a input pipeline for the Metaphores.
      * Search for new formal and creative text datasets


---+++++ Week 17.06 - 24.06.2020
   * Done:
      * Trained the model for a long time, on sentences only. The results didn't improve. This was kind of expected but I still wanted to do it, because I didn't train it for long on the sentences only dataset.
      
---+++++ Week 24.06 - 01.07.2020
   * Done:
      * I tried to build the model based on the huggingface library. This has numerous advantages like pretrained models. But after a lot of digging through source code and  QA on stackoverflow and twitter it turns out, that it isn't possible / huggingface isn't intended to be used for this.
      * Build a preprocessing pipeline for the metaphore dataset and the brown corups
      * Started building a classifier based on huggingface to distingulish between the two kinds of text.
   * TODO
      * Improve the metaphore preprocessing.
      * Examine the poem dataset and build a preprocessing pipeline.
      * Continue building the classifier.
   * Problems
      * Good data is still the main problem.
      * It will be alot harder to get good results without huggingface / good pretrained models.

---+++++ Week 01.07 - 08.07.2020
   * Done:
      * The classifier works and can distingulish well between metaphores and brown corpus sentences. But its not finished yet.
      * Analyzed the poem dataset and started building the preprocessing for it.
      

---+++++ Week 08.07 - 12.07.2020
   * Done:
      * Improved the classifier and added a evaluation step. Set the train test split to 50:50.
      * Trained the classifier on different datasets.



---+++++ Week 12.07 - 19.07.2020
   * Done:
      * Build a better preprocessing for the metaphores, to use the formal sentences as well.
      * Setup the cluster configuration.
      * Trained experiments on the Cluster, didnt achieve good results.
      * Implemented reddit post preprocessing.
      * Added an implementation of Toward Controlled Generation of Text.

---+++++ Week 19.07 - 26.07.2020
    
   * Done:
      * Build the preprocessing for the new Model. Trained it on the metaphore data, without success yet. 
      * Build the new model, based on pretrained Transformers with the library Texar.
         * This model generates sentences at training time. Then it is trained with an adversarial loss on the produced output.
         * Furthermore I build the preprocessing for it.
         * Trained it on the cluster
         * It turned out to be really slow in training. It was able to do ~ 1.5 epochs of the metaphore data in 3 days on a TitanRTX. Furthermore it has huge memory requirements and is only able to run with a batchsize of 1 on a TitanRTX with 24 Gb ob memory.
         * The results after 3 days were bad, as expected with only 1.5 epochs.
  
---+++++ Week 26.07 - 05.08.2020
    
   * Done:
      * Started with a new approach called speedy_styler. This one doesnt rely on decoding at runtime, to speed up the training.
         * It uses denoising autoencoding and latent disentanglement.
         * A Feed Forward Adversarial NN is used on the pooled output of Bert for disentanglement.
         * Didn't work, just learned the identity.
         * I used pretraining on the adversarial network for disentanglement and noised its input, to make it more robust. This didn't help.
      * I build a new preprocessing for metaphores: Just cutting them off after 30 tokens. Halfing the input lengths. I trained the lstm based disentanglement approach on it, but it didn't turn out to work.
      * Started writing the thesis.
       


---+++++ Week 05.08 - 12.08.2020
   * Done:  
      * Implemented metaphore shortening based on m_ tags. For the formal sentences the verbs are used as the center of a window with radius 5. => training length of 11 tokens.
      *  Trained the controlled text generation model on shorter metaphores (2 times). Using the window approach around verbs and m_ tagged words.
         * It did't learn the identity. 
         * It replaces words or fills in new ones. 
         * But they don't make a lot of sense most of the time.
      
      * Extended the speedy_styler by implementing a transformer discriminator instead of a simple Feed Forward discriminator. + Tried other tricks to encourage rewriting the sentence.
         * I trained it multiple times on sentiment data, to ensure the metaphores are not the reason why it doesn't work.
         * If the right words are noised out, it fills in the right words.
         * If these words arent noised out it is just an identity function.
         * 
      * Started with a new approach called abstractive_styler
         * It uses the idea of a cycle gan on the output of the Bert encoder.
         * This is then fed to the decoder.
         * This is a very complex architecture but should allow the model to rewrite sentences and not only learn the identity.
         * It isn't finished yet.


            



-- Main.TobiasBlei - 2020-06-03

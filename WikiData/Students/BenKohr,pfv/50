%META:TOPICINFO{author="kohr" comment="reprev" date="1496161062" format="1.1" reprev="49" version="50"}%
%META:TOPICPARENT{name="StudentsList"}%
---+!! Ben Kohr (Bachelor thesis)

---++ Links
   * Repo: https://git.ukp.informatik.tu-darmstadt.de/kuznetsov/BenKohr_BA_SRI
   * Overleaf: https://www.overleaf.com/9080638sxrmhsvkghzv


---++ Things done (this week, 26.05 - 01.06.)
   * Read emails
   * Changed separator in CSV files from semicolon to a tab separator
   * Changed cluster number for (COP)KMeans to 53
   * Implemented a adjacency matrix printer in the given format ("VERBNAME_nodes.txt", "VERBNAME_edges.txt"), used for the verbs of Lang and Lapata.
   * Changed Brown feature to be more reasonable
 

---++ IDEAS:
   * Tested some modifications and thought about some possible refinements. To get results fast, I only tested most of my experimental ideas on small datasets that are not representative, just to get an impression. These are my ideas:
      * I'm irritated about the under-mergin problem. CW is run for each single verb and the cluster numbers produced are not correlated to the ones for another verb. Cluster number X in clustering Y is not the same class as cluster number X in clustering Z.
		So, there are not more than 1000 clusters produced per clustering, but (in current results) in most cases less than 50. The same is true for the other algorithms: Same numbers of different clusterings don't indicate same clusters. 
	  * But still, some verbs produce high number of clusters (e.g. say). This problem of clusters in CW could be resolved by getting all but the 53 biggest clusters and merge the elements in these small clusters to "nearby" (connected) vertices of the biggest clusters.
      * I set the number of clusters from 53 to 10 for the KMeans algorithms, because it seemed to me that this high number of clusters leads to worse results (bad collocation). Maybe a small cluster number is better, even if not the whole amount of labels can be captured.
      * I tried (experimental) some custom distance functions (with rather less success). Probably we should weight the individual matches (AL, P, F, AP, ...) individually?
      * I tested a distance function which does not return 0 as the smallest value, but 1, while giving more weight to the "well known" matches, hoping the merging could be engaged with more available links. This probably weakens the efficiency. The programm ran out of heap space over night, even though I already increased it...
      * I also tried to required lower the percentage of nodes that "want to" vote for the CW algorithms, but that did not provide better results. I hoped to achieve more merges of clusters.
      * I changed the "cooldown" of the CW L/Lapata algorithm, so that it does not start with the maximum value, allowing more cluster label changes.
      * Besides that, I thought about
         * dynamically determine the EPS value of DBSCAN (according to the current cluster distances),
         * looking at parts of the syntactic path (not always the whole path) and define a fitting distance function
         * integrate must-links for COPKMeans and DBSCANWithContraints in case of a small distance
		 * Using different "base" distance functions (ARGUMENT_SIMILARITY) for the different algorithms
      * A Whole-CoNLL-Test with 10 clusters for (COP)KMeans, the new "cooldown", and the improved Brown distance function (see above) yields the following result: [[%ATTACHURL%/Results_29_05_2017_%281%29.pdf][Results_29_05_2017_(1).pdf]]
      * A Whole-CoNLL-Test with this configuration AND the merging of infrequent clusters into nearby ones (CW) yields the following result (only CW): [[%ATTACHURL%/Results_29_05_2017_%282%29.pdf][Results_29_05_2017_(2).pdf]]

---++ FEEDBACK (IK):
   * Yes, cluster numbers should not be the same. I mean, these are just indicators! Does it cause problems with evaluation? Because it shouldn't, since Arg1 of "see" is also not same as Arg1 of, say, "burn". The stuff should be evaulated per verb as in Lang/Lapata. I think that's what you do, but just to be sure.
   * Number of clusters is a parameter so the value must be justified somehow. Is the purity higher with more clusters?
   * Giving some weights to AL/P/F/AP might be not a bad idea since you will give clusters another change to merge (at the expense of graph density for CW, but it's still less dense than the original one). You can play around with that. If you have time, run the clusterings just with the syntactic path feature, path+voice, path+lemma etc.
   * I'd keep minimal distance at 0 and add individual features instead, otherwise the graph might get too dense.
   * EPS, K and other parameters - well, usually one gets them from the validation/development set. Dynamically determining the EPS might complicate things, so let's think about it more.
   * Parts of the syntactic path - this needs a thought. I *think* it will not be helpful for the task, but I'll think more.
   * BTW our syntactic baseline is lower than the one in Lang/Lapata. Take another careful look at the paper to see if there are any mismatches in evaluation etc. This might be due to a different syntactic parser though.
 
---++ BK:
	* I read the emails and the feedback.
	* Regarding the frequencies of verbs, I took another look at the CoNLL reader and it seems that this is due to removing duplicates. I don't add every
	argument to the Sets, but only those that are not already included. The reader finds about 20000 possible arguments and about 15500 are discared as they
	are already contained (regarding the StringToCompare). Say has many arguments and therefore many duplicates.
	* Some verbs are missing in the DBSCAN result, because DBSCAN clustering results that have only outliers are not added to the result file and the evaluation.
	* Yes, I evaluate on single verbs and average afterwards. But you probably already checked that. :)
	* The purity is higher with more clusters, yes. That's why I thought about reducing it. This yields better results (see below).
	
	* I did some experiments with the distance function. I used a small subset of whole_conll.closed (about a third), hoping I get results rather fast and that they are
	(a bit) representative, too. 
		* I used P && V match as the only criterion first, to have a baseline.
		* And then, I added feature after feature, always ensuring that the current combination of them yields better better results on the small data set.
		* I also experimented with cutting the "#[preposition]" away if the paths don't match at first, rating a match without the "#..." a bit lower. But that
		didn't succed on the dataset.
		* Finally, I obtained a custom function, that yielded better results compared to the initial results on the rediced dataset, so ran the whole procedure
		with this function, after having made some adjustions to the project in order to deal with the function.
		* The results can be found in "res, 30.05.17" in Dropbox. There are the reports and results, and the matrices (in VERB_nodes.txt and VERB_edges.txt for the Lang/Lapata verbs).
		  After all of this work and optimization of the function on the smaller data set ("train (med).closed"), the result on the complete CoNLL is better than the previous best result (for (most of/all?) of the algorithms except SyntacticBaseline), 
		  BUT syntactic baseline still is the best. :( 
		  The corresponding PDF can be found in the mentioned folder.
	* I also updated the reference data I use (complete folder structure) in Dropbox (in "data").

---++ Questions (this week, 26.05 - 01.06.)

---++ Meetings

---+++ 20.04.2017
   * TODO(IK): <del>hand in the CoNLL-2008 data</del>
   * TODO(IK): schedule talks

---+++ 13.04.2017
   * Infrastructure setup
   * Questions regarding implementation

---+++ 6.04.2017
   * _Cancelled: EACL_

---+++ 1-30.03.2017

   * Topic introduction
   * Task description
   * Administration






---++ Things done and questions per week

---+++ 19.05.17 - 25.05.17 (Things)

   * Checked the passive verb (see Notes IK). The syntactic head of "told" in this sentence is "roll" (an not be), that's why the passive recipe does not say that it's passive.
   * Implemented the syntactic baseline as another clustering algorithm with the following results on whole_conll, original lemma:
      * PURITY: 0.8591531262384537
      * COLLOCATION: 0.5592415327866354
      * FSCORE: 0.6774900175473992
      * Time: 0.016667 minutes
   * Implemented directed dependency path generation
   * Printing number of clusters after each clustering
   * Added Brown cluster feature
   * Clustering results after the above steps: [[%ATTACHURL%/Results.pdf][Results.pdf]], see also ZIP file in Dropbox (21.05.); remark: I was irritated that the ClassicKMeans needs more time than the COPKMeans, but I could not find any reason why this is the case. I ran the KMeans (after some further changes) later and it took less than one minute (as COPKMeans), which is what I would expect. I checked the different run configurations in the Main class and the ProcessingContainer class (because in the second run, I used the configuration for only one algorithm and not the one for all five), but I could not detect a bug (if there is any). It's still a bit strange. Maybe it's because of my computer getting slower after the long run time temporarily (?). I couldn't detect a bug that causes this - and maybe it's all correct, even if it seems wierd (For course, feel free to check my code, if you have the time :) ).
   * Updated DBSCANWithConstraints to only extract the problematic cannot-link element from a cluster and to assign it to another cluster (if possible). Updated the rest of the algorithm to still respect all cannot- and must-link constraints.
   * Improved clustering speed of ChineseWhispers (Biemann) from about 27 minutes (whole conll, original lemma) to 9 minutes; improved speed of CW (Langa/Lapata) from 72 minutes to also appr. 9 minutes. 
   * Tested all algorithms' performances (current state) with a different Brown cluster file. For the above test, I used the English file paths_1000_min_50. For this test, I used paths_1000_min_3. The results have not changed much by the different cluster file ([[%ATTACHURL%/Results.pdf][Results.pdf]], see also Dropbox -> results, another cluster file).


---+++ 19.05.17 - 25.05.17 (Questions)


---+++ 12 - 18.05.2017

   * TO-prepositions are treated like IN-prepositions now
   * Printing of prepositions in next lemma mode, if two or more children are observed or if no children is observed 
   * Random seed used now to specify behavior of ChineseWhispers (Biemann / Lang, Lapata), of KMeans and COPKMeans.
   * Documented the current parameters that can be modified for optimization: [[%ATTACHURL%/Parameters.pdf][Parameters.pdf]]
   * Path annotation in case of next lemma usage (There was only one single case in train.closed, where the path to the NEW node had only one step, so in this one case I annotated the last (and only) step to the child noun).
   * Printing clustering results into a file (as a table)
   * Added the whole sentence to the table output
   * All clustering and reading times are printed now
   * Reading results are separately printed as a table
   * I did a performance and clustering test on all (currently) five algorithms, with original and next lemma and on the single verbs. The results for all verbs are documented here: [[%ATTACHURL%/Results.pdf][Results.pdf]]. Some were obtained after some (possible) further processing (if needed), e.g. removing empty argument sets before clustering for single verbs, removing results consisting of outliers only (DBSCAN) etc. Documented are the purity, collocation and fscore for alle algorithms and the times. I put the result files (reports (Output text) and results (Tables), all in all 264 files) into Dropbox, because it was far to big to upload it here. You may take the zip archive "results.zip" and remove it from the Dropbox again, I still have a copy.
   * Implemented a DBSCAN algorithm inspired by C-DBSCAN, which respects constraints -> DBSCANWithConstraints
   * Tested DBSCANWithConstraints on CoNLL (results see Dropbox, folder "DBSCANWithConstraints").
      * Purity: 0,89362
      * Collocation: 0,38070
      * F-score: 0,53394
      * Time: 0,16667 minutes
   * Improved speed of COPKMeans from about 13 minutes (whole CoNNL, all verbs) to less than one minute.

---++ Questions BK (this week, 12.05 - 18.05.)

---++ Notes IK
   * (just to make sure) double check distance/similarity conversion
   * General note on results: higher purity, lower collocation. How can it be improved?
      * Lower collocation means that clusters are fragmented, which is understandable because our distance function is so strict.
         * Try using Brown clusters in addition to lemmas? https://github.com/rug-compling/dep-brown-data (if you use, don't forget to cite!)
         * Make sure to take the English clusters and not the Dutch ones :)
         * Also consider reading the paper
   * PASSIVE/ACTIVE debug, conll2008_clustering.DBSCAN_[2]_next_lemma_[tell]_RESULT.csv src line 334522, LGSby -> Active. Why not passive?
   * Implement syntactic baseline?

---+++ 05. - 11.05.2017

Things done BK:
   * Words now only are nolinks, if they have the same sentence id and verb id
   * Added NO_LINK constant
   * Treated NO_LINK as special value for CW (Biemann): Now, Integer.MIN_VALUE indicates these NO_LINKs. They are less frequently taken chosen as voted class.
   * Treated NO_LINK as special value for CW (Lang/Lapata): In addition to CW (Biemann), if one node is only connected to NO_LINK-distant nodes, it must not vote.
   * Treated NO_LINK as special value for KMEANS: NO_LINK nodes have Integer.MAX_VALUE distance in that case. This probably does not make a huge difference, as all nodes are jus compared to the cluster center nodes and just in very few cases there is such a distance occurence, but it still takes the knowledge into account.
   * Treated NO_LINK as special value for COP-KMEANS: NO_LINKs between Arguments result in cannotLink-Elements, before clustering is done.
   (For DBSCAN, in my opinion there is no "good" way to (intuitively) add the information of NO_LINKs)
   * Added stop criterions to reduce number of iterations significantly for CW (Bieman, L&L): Number of changes / change wishes of nodes determine level of change; low level of changes results in a stop of the algorithm.
   * Added stop criterions to reduce number of iterations significantly for ClassicKMeans and CopKMeans: differences between old and new centroids is determined and may result in a stop, if they are similar.
   * Added usage of child node if node is a preposition (configurable for the reader)
   * Better communication between the reader, the clustering algorithms (on one side) and the Evaluation (on the other side). The specialities of the reader (verbs with no arguments), DBSCAN (Ourliers) and COPKMeans (Unable to construct clusters) caused problems during evalutaion which are (hopefully) solved now.
   * I edited the Pipeline class for supporting several individual pipeline runs. One of the next steps will be refactoring the Pipeline as due to adding aspect after aspect to it, it's pretty complex now and configuration management is not easy anymore. I still hope these results are correctly configured and determined by the system: In [[%ATTACHURL%/Results.zip][this]] ZIP archive I inserted the output for the whole CoNLL (all CoNLL files in one single file) with all five algorithms (Chinese Whispers original, CW Lang/Lapata, DBSCAN, KMeans, COPKMeans) with configurations as in the current Pipeline.java file on the original lemma usage ("...original lemma..." in the file name) and the next lemma usage ("...next lemma..."). The output contains the processing steps, the time and the evaluation metrics. I also ran the program to determine the clustering results for single verbs (those in the Lang/Lapata paper). As mentioned, I will improve the Pipeline class in terms of readability and efficency as soon as I have the time for it.
   * I made a new processing core of the project which is (in my optinion and, and, if written correctly) much more understandable and flexible. Next step is to make it faster. Currently, e.g. the single verbs are clustered individually while their results could be stored during the clustering of all verbs and used later on. Some results of the "new" Pipeline are collected here and they seem to be as in the "original" pipeline (but with some randomness aspects, due to the algorithms): [[%ATTACHURL%/Result2.zip][These are the results.]]
 
Questions BK:
   * Currently, I just take the "IN" prepositions to switch to the child node. Should I use others, too, for example the "TO" POS tag?
      * IK: Well I would do the "TO" too, but see if it's easy or not. If not, drop it for now.
      

Notes IK:
   * So can the no-link'ed elements get merged in COP-KMEANS / KMEANS?
   * Let's go through all parameters again
   * Randomness - fix the seed!
   * Lemma comparison - lowercase?..
   * Result2.xip - add clustered arguments?
   * Print full argument signature (all the features you use to get the distance, e.g. path, lemma, POS etc.)



---+++ 28.04 - 04.05.2017
Things done:
   * Implemented and integrated dependency finding between two given nodes
   * Implemented and integrated voice detection from the recipe
   * CoNLL-Reader timing test on the corpus
   * Improved CoNLL reading speed (I got rid of some readers, the tests are still working, the output file ([[%ATTACHURL%/newoutput.txt][newoutput.txt]]) seems to have the identical arguments (String representation), but now the reading for "say" takes only 0.7 minutes instead of 172 minutes).
   * Implemented the confidence aspect of Lang and Lapata to the Chinese Whispers Algortihm. Now, confidence usage can be added or not via a boolean.
   * Added support for all verbs: All verbs contained and marked in the corpus are searched and arguments are detected per verb.
   * small enrichements of the Pipeline class (mode for CoNLL: one verb, list of verbs, all verbs; evaluation)
   * Speed improvement for finding all verbs and constructing arguments for each verb: From (probably much) more than 105 minutes (I aborted it...) to now only 2.3 minutes (I degugged it and it seems to be working, please feel free to take a look at it, I certainly will add more tests, but it seems to be correctly working). This is the attached output for the reading of all verbs: [[%ATTACHURL%/output_allverbs.txt][output_allverbs.txt]]
   * Added tests for evaluation metrics and the reading of all verbs
   * Now, the Reader only retrieves verbs (POS: V...)
   * Improved reader code
   * Implemented the similarity function by Lang and Lapata (based on sentences numbers)
      * IK: what if there are several Arg0 in the sentence, but they belong to different predicates?

Questions:
   * [ANSWERED] If I remember right, you said that we focus on verbs only, not on predicates for clustering. Does this mean I have to mention that explicitely in the Latex paper (when there is time for that, later)? And also, in the CoNLL file often nouns (predicates) get a frame association, too. Shall we exclude them, because they are not verbs? 
   * [ANSWERED] Lang and Lapata used a function with range [-1, 1] union -inf. How should I adapt this, when given arguments with 5 oder six discrete features? I currently implemented a dummy function which only produces the result of the "normal" argument similarity and I would have to change some constructors and functions (which currently expect integer values) to double value usage. 
   * My distance/similarity function produce integer values, but the class Integer does not offer a -Inf. The class Double does (Double.NEGATIVE_INFINITY). I instead chose Integer.MIN_VALUE (to not have to change alle constructor signatures and methods affected by the distance/similarity functions). Is this also okay or does it have to be the negative infinity by class Double?
      * IK: hm, that should be fine (given the distance function, you will never come anywhere close to Integer.MIN_VALUE), but those minimum values should be treated differently anyway, because _you don't want them to vote at all_. I would suggest, let it be Integer.MIN_VALUE, but store it separately as a constant. It's basically just a way to store the NO-LINK value. Let's talk about this.


---+++ 20. - 27.04.2017
Things done:
   * Downloaded CoNLL (link can be removed!)
   * Implemented a first version of the CoNLL Reader
   * First version of evaluation metrics, integration into the pipeline
   * First try of dependency graphs, necessary data structures implemented
   * Tests for CoNLL and the evaluation
   * Fixed the FIXMEs
Questions:

   * There are some warning when I don't silence System.err in the Pipeline by DKPro. Maybe they aren't important, I'm not sure. What is your opinion? ([[%ATTACHURL%/warnings.txt][warnings.txt]])
      * IK: ignore
   * I don't have Outlook or something similar. In case the UKP meetings are each Tuesday, could you tell me the time and the place?
      * IK: see email
   * How to distinugish between agentive and non-agentive verbs?
      * IK: They have a hint in the paper, agentive are the ones that take Arg0 as subject I guess. You will need dependency trees for that.
   * How to distinguish between active and passive voice? I thought about using the POS tags, but I guess they don't give this information.
      * IK: Yes, that's one of the underspecified parts of the paper, POS tags are not enough. The same POS can mark passive ("The movie was banned") and active ("The government has banned the movie") voice. You will need dependency tree for that, so focus on that first.
   * Please feel free to look over the current version (I guess this is better than informing you on Mondays, even if the version slightly changes afterwards), especially over the CoNLL Reader. I stepped through it (with the Debugging Tool) and tested it on the CONLL data and it seems working for me (except the todos, of course, see the two list items above), but maybe I understood something wrong or made a mistake...
      * I'll have a look
   * Does the syntactic path always lead from a node to the sentence's root?
      * No, a syntactic path leads from node A to node B, sometimes (rarely!) passing through the root. I think I understand what confused you here. I will explain on our meeting. In short, linguistically speaking the main predicate is a root. However, the dependency parsing algorithms require each node in the sentence to have a parent (it's just easier to formulate this way), so they add an abstract node "ROOT" which is the parent of the actual root. So: informally speaking, if you're dealing with the main predicate, your paths would be from the root to the arguments. However, _strictly_ speaking - no, you're not going through the root node, just through the main predicate whos parent is the abstract root. If it's still unclear, I'll give an example on Thursday.
   * Is it possible to move the weekly meetings on Thursday from 13:00 to 12:00?
      * Yes, let's move to 12:00. We won't get the nice room anymore though, but it should be fine.


%META:FILEATTACHMENT{name="warnings.txt" attachment="warnings.txt" attr="h" comment="" date="1492890276" path="warnings.txt" size="667" user="kohr" version="1"}%
%META:FILEATTACHMENT{name="newoutput.txt" attachment="newoutput.txt" attr="" comment="" date="1493466071" path="newoutput.txt" size="40623" user="kohr" version="1"}%
%META:FILEATTACHMENT{name="output_allverbs.txt" attachment="output_allverbs.txt" attr="" comment="" date="1493553829" path="output_allverbs.txt" size="1965805" user="kohr" version="1"}%
%META:FILEATTACHMENT{name="Results.zip" attachment="Results.zip" attr="" comment="" date="1494417520" path="Results.zip" size="875501" user="kohr" version="1"}%
%META:FILEATTACHMENT{name="Result2.zip" attachment="Result2.zip" attr="" comment="" date="1494478594" path="Result2.zip" size="1285048" user="kohr" version="1"}%
%META:FILEATTACHMENT{name="chp3A10.10072F978-3-540-72530-5_25.pdf" attachment="chp3A10.10072F978-3-540-72530-5_25.pdf" attr="" comment="C-DBSCAN paper" date="1494497238" path="chp%253A10.1007%252F978-3-540-72530-5_25.pdf" size="331048" user="kuznetsov" version="1"}%
%META:FILEATTACHMENT{name="Parameters.pdf" attachment="Parameters.pdf" attr="" comment="" date="1494581910" path="Parameters.pdf" size="311048" user="kohr" version="1"}%
%META:FILEATTACHMENT{name="Results.pdf" attachment="Results.pdf" attr="" comment="" date="1495647752" path="Results.pdf" size="266959" user="kohr" version="3"}%
%META:FILEATTACHMENT{name="Results_29_05_2017_(1).pdf" attachment="Results_29_05_2017_(1).pdf" attr="" comment="" date="1496055335" path="Results_29_05_2017 (1).pdf" size="266740" user="kohr" version="1"}%
%META:FILEATTACHMENT{name="Results_29_05_2017_(2).pdf" attachment="Results_29_05_2017_(2).pdf" attr="" comment="" date="1496055369" path="Results_29_05_2017 (2).pdf" size="265369" user="kohr" version="1"}%
%META:PREFERENCE{name="TOPICTITLE" title="TOPICTITLE" type="Local" value="Ben Kohr"}%

%META:TOPICINFO{author="santos" comment="save topic" date="1510910492" format="1.1" reprev="9" version="13"}%
%META:TOPICPARENT{name="StudentsList"}%
---++++ *Bachelor Thesis* : GANs for data augmentation

*Start date* :  

*Supervisors*: Pedro Santos, Dr Steffen Eger

---++++ Current Status
   * 12.11.2017 
      * Done last week
         * I started writing the related work section of the thesis, the PDF of the current version is attached to this page. I also found some additonal papers on data augmentation in NLP and GANs for data augmentation. Their description is included in the related work.
            * SE: we can give you feedback on it. There's also the following paper: Adversarial Generation of Natural Language, https://arxiv.org/pdf/1705.10929.pdf, which has been slightly controversial. But you should probably know it.
         * I had a look at the 20 Newsgroups and the argumentation mining dataset. The latter looks like a sequence labelling data set, but I guess I'm suppposed to just classify the sentences individually?
            * SE: ArgMin has format "(DE_Sentence)TAB(LABEL)TAB(EN_Sentence)". Each sentence is an instance, the label is the label. Goal is to do standard classification, with one sentence as input and the label as output. Ignore the German data. Also don't forget to adapt the labels as mentioned previously.
         * I was able to connect via ssh, install tensorflow and keras and run tensorflow on the gpu
         * I downloaded the datasets from https://github.com/facebookresearch/SentEval
      * Problems
         * I'm unable to access the following pages ("Access check on UKP.GpuUsagePolicy failed. Action "VIEW": access not allowed on web."): https://wiki.ukp.informatik.tu-darmstadt.de/bin/viewauth/UKP/GpuUsagePolicy https://wiki.ukp.informatik.tu-darmstadt.de/bin/viewauth/UKP/ServerJobs
            * PSa: Reported it to sys-admin. 
            * These pages are not available for students, so I did this workaround: GpuServer GpuUsagePolicy
         * Extracting some of the SentEval datasets failed because cabextract is not installed on desktop-k40 (I'm not allowed to use sudo, right?)
            * SE: You should still be able to install it locally. Anyway, you can find the data in my home on desktop-k40: senteval.tgz  
      * Plans for next week
         * Read about multimodal embeddings and add them to the related work
         * Have a look at the MOSI dataset and the SentEval datasets and write about all datasets in the thesis (in what detail?)
            * SE: 2-3 pages overall suffice, maybe even less. Don't start initial preparations of the experiments too late. Experiments are after all the most important part of the thesis.
            * PSa: When you start working on the MOSI dataset, you should give a try on this code: [[https://github.com/A2Zadeh/CMU-MultimodalDataSDK]] I have not used it yet, but it seems to facilitate the extraction of features and embeddings for all modalities, so you do not have to implement preprocessing code from scratch.
   * 06.11.2017:
      * thanks for the suggestions. I read them and took notes on them. Unfortunately, I didn't do more last week.
      * Thanks for the mail concerning the GAN code from Yevgeniy, but it doesn't contain an attachment for me.


---++++ Meeting protocol - 13.11.2017
   * @discussion: 
      * NLP Datasets: Can choose the 6 datasets from Table 1 in the InferSent paper, plus SICK-E and SICK-R
         * Arg.Min dataset: The data set is the 402 persuasive essays from Christian Stab (it's the 2017 paper from "Computational Linguistics"). However, the data is split into sentences, and the token-level annotations are projected onto the full-sentence
      * Can try out three variants of using GANs for discrete labeled datasets:
         * Conditional GANs (do they still use all of the data?)
         * One GAN for each class 
         * GANs that generate instances AND labels --> labels will be real numbers. How to discretize them?
      * Starting early with experiments avoids mid-way surprises
      * Make a plan for your framework
      * Try doing the preprocessing of all the datasets once, creating pickle files for each dataset, so that you can easily use them later on without having to do any preprocessing again.
   * @info(PSa, SE) Points in the agenda:
      * Administration:
         * Enrollment form
         * Starting date?
         * New midterm presentation date: 30.01.2018
         * New final presentation date: 10.04.2018
      * Discussion (what has been done, what will be done, problems).

---++++ Meeting protocol - 07.11.2017
   * Discussion:
      * Finish related work section soon to get feedback from us
      * Grade should be available by End of April
         * Need to present twice (midterm, final) - so both need to be before End of April
         * Then you have less than 6 months (plus two weeks of holiday/absence)
            * organize your work schedule accordingly
            * you need to work more because you have less time
      * Register within the next 3-4 weeks
      * Purpose of literature reading on data augmentation in NLP:
         * identify good baselines
         * if no good baselines can be identified, go with:
            * self-training
            * Language Model
      * Come to the UKP Meetings on Tuesdays regularly
      * Send us a status report every Monday evening
      * Start writing as soon as possible, especially the related work section since you have just finished reviewing the literature
      * Fill out an applicable form available here: [[https://www.informatik.tu-darmstadt.de/de/studierende/studium/abschlussarbeiten/]]
         * Please hand enrollment form to us.
         * Please send a TuCan printout to us which shows the official deadline to hand in the thesis.
      * Information about the servers: [[https://wiki.ukp.informatik.tu-darmstadt.de/bin/view/UKP/ServerJobs]]
         * Every job has to be scheduled in the server before being run
         * If you need access to other servers, just let us know and we can see what we can do.
         * Information on how to install the libraries in the GPU server: [[https://wiki.ukp.informatik.tu-darmstadt.de/bin/viewauth/DKPro/GpuServer]]
         * Policies and some hints on how to use the GPU server, e.g. important details to pay attention when using tensorflow/keras: [[https://wiki.ukp.informatik.tu-darmstadt.de/bin/viewauth/UKP/GpuUsagePolicy]]
   * @info(PSa, SE) Points in the agenda:
      * Datasets;
      * Code;
      * Literature review (in case anything new was found) + Writing (templates): [[https://wiki.ukp.informatik.tu-darmstadt.de/pub/Students/InformationenFuerDiplomanden/UKPThesisTemplate.zip]];
         * Template is neither complete nor obligatory, form of the thesis can vary depending on the context...
      * Deadlines:
         * Registration of thesis (inform us);
         * Mid-term presentation: 06th March;
         * Final presentation: 12th Jun.
      * Change password for your user.

---++++ Meeting protocol - 24.10.2017
   * Discussed details about multimodal embeddings and the sentence classification tasks
   * Explanation of the pipeline for preprocessing the sentence and document classification datasets
   * Details about administrative issues
   * Expected for next week: literature review on data augmentation for NLP, literature review on GANs (state-of-the-art)

-- Main.PedroSantos - 2017-10-24

%META:FILEATTACHMENT{name="thesis.pdf" attachment="thesis.pdf" attr="" comment="" date="1510510935" path="thesis.pdf" size="67596" user="schaeffler" version="1"}%

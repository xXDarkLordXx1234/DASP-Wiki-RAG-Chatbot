%META:TOPICINFO{author="kaster" comment="" date="1574892332" format="1.1" reprev="20" version="21"}%
%META:TOPICPARENT{name="StudentsList"}%
---+!! Bachelor Thesis: Multilingual Contextual Probing
   * Deadline: 27 December 2019
   * Begin: 26 June 2019
   * Midterm presentation: 10 October 2019
   * Final presentation: 14 January 2020

---++ 21-27 November 2019
   * Added the contextual probing tasks to LINSPECTOR WEB
   * The static embeddings and all models which work with the static probing tasks work with the contextual probing tasks too
   * We can only support models without own code. Models with custom code (e.g. a new model, embedding) would require uploading additional code files (and executing them -> security).
   * Every model needs a predictor. It is easy to implement support for new models which already have a [[https://allenai.github.io/allennlp-docs/api/allennlp.predictors.html][predictor]]
   * At the moment linspector web gives the models only one word for the prediction. For the contextual models it makes sense to give the model the whole sentence. I am not sure how hard it is to extract the vector for one word at different layers (e.g. ordPiece tokenization) 
   * Support for sentence representations?

---++ 21 November 2019 Meeting
   * Add the contextual probing tasks
   * Support at least one downstream model
   * Which downstream models are easy to support?

---++ 14-20 November 2019
   * Started working on extending LINSPECTOR WEB with the contextual probing tasks
   * Added a page for selecting the type of the probing tasks
   * Some writing

---++ 14 November 2019 Meeting
   * It seems that we can't improve BERT through fine-tuning on our probing tasks
   * Focus on LINSPECTOR WEB

---++ 07-13 November 2019
   * Fixed the experimental setting. Updating only the last layer. 
   * Fine-tuned separate case, gender, numer, person, pos and tense models and one combined model (all models were fine-tuned for all xnli languages).
   * Evaluated them on XNLI.
      * No improvement for Bulgarian and Chinese
      * Small improvement for other languages on some tasks.
      * Combined model improves only for Spanish and Turkish
   * Evaluated them on MLQA (Training on squad, evaluating on mlqa)
      * Combined model improves for the half of the languages
      * Improvements on all models for Vietnamese
      * Hindi improves only on Case
      * All improvements are small (<1% point)
      * Other languages show improvements for some tasks.

---++ 01-06 November 2019
   * I fine-tuned M-BERT with different settings. One probing task, POS and Case combined, updating all layers, updating a few layers, updating only the last layer. Then I fine-tuned those models on English XNLI.
   * No/Minor( <1,5% points) improvement
   * Read "To Tune or Not to Tune?"

---++ 31 October 2019 Meeting
   * XNLI: Try simple experiments. Fine-tuning on one probing task. 
   * Try updating only a few layers during fine-tuning
   * Also try fine-tuning a single probing task in all languages
   * Max 3 weeks for the experiments

---++ 24-30 October 2019
   * I got better XNLI results. The results are similar to the paper. I get an accuracy of 33,333% for German and Urdu. The results are better if I use a different seed.
   * MLQA is easy to use. I can use the example script from the huggingface transformer frameork. They provide a dev and test file for every language. The test file is much bigger than the dev file. Results are similar to the paper for English if I use the dev file as prediction file and the test file as train file. I am not sure if this is the right way to do this. They also provide cross-lingual files. Constext & answer is in one language and the question in another. A cross-lingual evaluation could be using those files or a monolingual file in another language.
   * Cross-lingual evaluation of fine-tuned BERT (probing tasks)
      * The fine tuned BERT is worse in cross-lingual evaluation than the classifier trained on top of BERT. Many tasks are lower than the baseline.
      * Gender: Good results within the language family. Bad results otherwise
      * POS: English, French and Spanish is good. But only less language pairs which share labels
      * Tense: Good are English-German, Russian-Spanish (same language families)
      * Person: Bulgarian, French, Russian, Spanish are ok/good
      * Number: Bulgarian-Russia, English-Russian/spanish, German-Russian, Spanish-Bulgarian/French/Russian are good
      * The other tasks are bad or have to less language pairs which share labels
      * We see on some tasks that language pairs with a shared word order have better results. But the results are less than the baselines. 
      * Averaging language pairs is problematic because of the different baselines.
   * Cross-lingual XNLI evaluation
      * Good results.
      * Best results for SVO models which are evaluated on SVO. SOV models are even better on SVO languages than om SOV languages. 
      * Hindi, Urdu, Arabic are the worst.
      * Germanic, Balto-Slavic, Italic models are best
      * Hindi model on Urdu is better than the monolingual Urdu evaluation
      * Language pairs which share word order aren't better!
   * Cross-lingual lingual evaluation (probing tasks without fine-tuning)
      * Aspect: Hindi/Russian-Arabic is good. Rest is bad
      * Definiteness: Bulgarian evaluation is worse than the baseline. English evaluation is the best. English, French, German, Spanish are good pairs.
      * Finiteness: Hindi is the worst (even lower than the baseline) but all other languages are good (all Indo-European)
      * Gender: Best results within the same language family. Urdu is bad.
      * Number: High baselines. Russian model is best. Urdu evaluation is bad. 
      * POS: Only few shared labels. Some good (fr-bg/es-en) results, some bad
      * Person: Turkish evaluation worse than the baseline. French/German/Russian/Spanish are good pairs
      * Hindi, Turkish and Urdu are often bad in cross-lingual.
      * Tense: Hindi model worse than the baselines but hindi evaluation is good. Bulgarian evaluation is bad. English/French/Russian/Spanish good pairs
      * CharacterBin: Often good within the language family. Chinese/Urdu/Vietnamese worse than the baseline.
    * Word Order: Hindi, Turkish and Urdu are the only SOV languages and often bad in cross-lingual evaluation. Hindi and Urdu are good language pairs but they share the language family. Turkish shares labels only on a few tasks.

---++ 17-23 October 2019
   * Fine-tuned German BERT on XNLI. Results are a little bit better than the classifier results but still much worse than the literature results.
   * Tried a new fine-tuning approach. Instead of predicting a label for a whole sentence and an index, BERT predicts a label for every token. The results are much better.
      * I had to create new probing datasets. We need labels for every token in the sentence.
      * We need a label for "no label".
      * I calculated the accuracy only on labels which should be not none. But maybe the overall accuracy or the accuracy of the none labels could be interesting too.
      * I set the threshold to 4K (number of sentences) instead of 10K instances.
      * Problem: We get a high number of instances (with a not none label) per sentence on some probing tasks and a very low number of instances  (<10K) for other probing tasks.
      * I started fine-tuning on a different probing dataset which contains a fixed (10K) number of instances and varying number of sentences. But very low number for sentences on some probing tasks.
    * Some writing


---++ 17 October 2019 Meeting
   * XNLI results are worse than the results in the literature. We are training a classifier on top of the BERT embeddings. They are fine tuning BERT. Try to finetune BERT too.
   * No clear pattern for the word order in the cross-lingual results.

---++ 10-16 October 2019
   * Run XNLI for Bulgarian, Chinese, English, French, German, Hindi, Russian, Spanish, Turkish, Urdu and Vietnamese
      * Got an tokenization error for Arabic
   * Code cleaning
   * Added the cross-lingual results for Arabic, Bulgarian, Chinese, English, French, German, Hindi, Russian, Spanish, Turkish, Urdu and Vietnamese to the spreadsheet  
   * Fine tuned BERT on German case. Results are worse compared with the probing task results. I tested all recommend parameters.

---++ 10 October 2019 Meeting
   * Gave midterm presentation
   * Start fine-tuning of BERT
   * Run NLI for the new languages
   * Check if we can use entity linking as second downstream task   

---++ 02-09 October 2019
   * Worked on the presentation
   * Cross-lingual evaluation of bert on all probing tasks for German, Russian, Spanish and Turkish
      * Turkish has a different word order compared with German, Russian and Spanish
      * We can compare Turkish to the other languages on Number and Person. Turkish has always the worst accuracy.
   * Cross-lingual evaluation for Arabic, Bulgarian, Chinese, English, French, Hindi, Urdu and Vietnamese is still running.
   * I updated the statistics of the probing dataset. There was an error in the column of ambiguous words. I have also calculated the number of ambiguous and rare words for every probing task and language.
   * The contextual embeddings seems to benefit from the context on probing tasks which have a high amount of ambiguous word.

---++ 02 October 2019 Meeeting
   * Cross-lingual NER evaluations seems to be not possible. The languages have different labels.
   * Focus on cross-lingual probing tasks.
   * 4 weeks for fine-tuning results
   * Prepare the repository for moving to the ukp repository in 2-3 weeks
   * Simpler models: Use ratio instead of the absolute difference
   * Got feedback for my midterm presentation slides

---++ 26 September - 1 October 2019
   * Trained simpler models for german. I added the results to the spreadsheet.
      * 1st layer with dim=100
      * Only 1 relu layer
      * Only 1 linear layer
      * Only 1 linear layer without dropout
   * Created probing tasks for every language for which we have data
   * Worked on the midterm presentation
   * Checked which language pairs share labels (probing tasks). I added the results to the spreadsheet.
   * I had problems with the cross-lingual NER evaluation. Every language has different label sets. AllenNLP sees an unknown label and throws an exception.
   * Read "What Does BERT Look At? An Analysis of BERT’s Attention"

---++ 26 September 2019 Meeting
   * For next meeting:
   * Train simpler models for the probing tasks in one language.
   * Create probing tasks for Hindi
   * Start mBert experiments with a few language pairs.
   * Check which languages share the same labels. The model can't learn a label if the language doesn't have this label.
   * Send the presentation slides before the next meeting

---++ 17-25 September 2019
   * Added the probing task results (incl. bert, bert-c, CharacterBin, TagCount) to the spreadsheet
   * Bert uses word piece tokenization but our labels are for whole words. I tried using the first word piece (some papers did this) and an averaged word vector (like bpe). Using an averaged vector shows minor better results.
   * Got an error for Russian w2v voice ("No embeddings of correct dimension found; you probably misspecified your embedding_dim parameter, or didn't pre-populate your Vocabulary").
   * The results indicate that context information are helpful for some tasks. Improvements on case, pos, animacy and finiteness. Minor improvements or worse results on the other tasks. Elmo mostly outperforms bert.
   * Probing tasks, XNLI and NER are available for:
   * SVO: English, french, German, Russian, Spanish
   * SOV: Turkish, Hindi, Urdu
   * Some writing on related work
   * Read "Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing"
   * Read "How to Fine-Tune BERT for Text Classification?"
   * Read "How to (Properly) Evaluate Cross-Lingual Word Embeddings: On Strong Baselines, Comparative Analyses, and Some Misconceptions"
   * Read "Polyglot Contextual Representations Improve Crosslingual Transfer"
   * Read "Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond"
   * Read "Probing What Different NLP Tasks Teach Machines about Function Word Comprehension"
   * Read "BERTSCORE: Evaluating Text Generation with BERT"

---++ 17 September 2019 Meeting
   * Still waiting on probing tasks results. I will add them when they are available.
   * Midterm presentation on 10th October. Prepare slides during the next 2 weeks. Should talk about motivation, what have I done? (probing/downstream tasks, results), how effective are the embeddings on our tasks?, future work (fine tuning ...). Max 20 minutes
   * Read more about cross-lingual bert and fine-tuning
   * Check which languages are available and how they are related
   * Bert performs good on cross-lingual zero-shot tasks. It performs good on language pairs which share the word order even if  they have no vocabulary overlap. It performs bad on language pairs which do not share the word order. Hypothesis: Training bert on our probing tasks which correlate with the word order improves the performance for language pairs which have different word orders.
   * Experimental setting:
   * 1) Show that m-bert performs bad on cross-lingual tasks with unrelated language pairs
   * 2) Show that m-bert performs good on cross-lingual tasks with unrelated language pairs if we fine-tune m-bert on our probing tasks
   * 3) Show that fine-tuned m-bert performs similar on cross-lingual tasks with related language pairs

---++ 09-16 September 2019
   * Added CharacterBin and TagCount to the probing tasks
   * Improved the probing datasets (removed long sentences and fixed word splitting)
   * A z-score of 2 seems to be good for the sentence lengths.
   * The sigmorphon19 tasks 2 dataset does not contain information about the word splitting. These information are in the original ud dataset.
   * Checked pytorch-transformers framework. Bert should work with our intrinsic probing tasks. It should be possible to use the multilingual bert model directly in allennlp (useful for downstream tasks).
   * Some restructuring of the repo
   * Started the new probing tasks
   * Read "How multilingual is Multilingual BERT?" (2019)
   * Read "Cross-lingual Language Model Pretraining" (2019)
   * Read "Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT" (2019)
   * Read "Informing Unsupervised Pretraining with External Linguistic Knowledge" (2019)

---++ 09 September 2019 Meeting
   * Talked about the downstream results. The contextualized elmo performs only better on NER. No improvement on XNLI, DEP and POS
   * For the next meeting:
   * Check sentence length distribution of the treebank
   * Remove long sentences from the probing dataset
   * Fix that some words in Turkish (and probably Spanish, Russian) were separated in the probing task datasets
   * Prepare the repository for moving into the UKPLab repository (more comments and restructuring)
   * Add TagCount and CharacterBin to the probing tasks
   * Support bert
   * Read more literature about probing on bert and cross-lingual transfer
   * Update this wiki page

---++ 12 August - 08 September 2019
   * Finished the experiments on the downstream tasks for elmo
   * The contextualized elmo performs only better on NER. No improvement on XNLI, DEP and POS
   * Setup thesis template
   * Read "SentEval: An Evaluation Toolkit for Universal Sentence Representations"
   * Read "Probing Biomedical Embeddings from Language Models"
   * Read "GLUE: A MULTI-TASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTANDING"
   * Read "What’s in an Embedding? Analyzing Word Embeddings through Multilingual Evaluation"
   * Read "A Survey of Word Embeddings Evaluation Methods"
   * Read "Linguistic Knowledge and Transferability of Contextual Representations"
   * Read "Evaluation of sentence embeddings in downstream and linguistic probing tasks"
   * Read "Probing Multilingual Sentence Representations With X-PROBE"
   * Read "WHAT DO YOU LEARN FROM CONTEXT? PROBING FOR SENTENCE STRUCTURE IN CONTEXTUALIZED WORD REPRESENTATIONS"
   

---++ 18-26 July 2019
   * Trained classifier for Downstream tasks (dep, xnli, ner, pos) for german, spanish, turkish and russian

---++ 17 July 2019: Meeting with Max and Gözde
   * Knowledgetransfer

---++ 10-17 July 2019

   * Trained classifier for fasttext, bpe, muse and elmo embeddings on Finnish, German, Spanish, Turkish and Russian
   * Collected results in a spreadsheet

---++ 10 July 2019 Meeting
   
---++ 27 June - 10 July 2019
   * Trained classsifiers for intrinsic probing tasks on german, finnish, spanish, russian and turkish

---++ 26 June 2019 Meeting
   

---++ 20-26 June 2019
   * Statistics about the sigmorphon19 dataset

---++ 19 June 2019 Meeting

---++ 13-19 June 2019
   * Wrote reader for the sigmorphon19 task 2 dataset and created probing dataset


---++ 12 June 2019 Meeting
   * First meeting
   * Introduction to the topic

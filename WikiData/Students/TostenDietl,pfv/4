%META:TOPICINFO{author="dietl" comment="reprev" date="1480621121" format="1.1" reprev="4" version="4"}%
%META:TOPICPARENT{name="StudentsList"}%
-- Main.TorstenDietl - 2016-12-01 - Telefonkonferenz
   * Es wurde besprochen, wie DKPro auf Windows genutzt werden kann und wie Miralium eingebunden werden kann.
   * Es sollten am besten die EvalUtil Methoden genutzt werden, um  zu evaluieren, da diese bereits generisch und entkoppelt sind. Wo nötig können eigene Evaluation Klassen geschrieben werden, wenn Evaluationsmethoden benötigt werden, die so nicht in der EvalUtil vorhanden sind (Named Entity Evaluation mit BIO Methode)
   * Generell ist es in Ordnung eigene Klassen zu erstellen, die Verhalten von OpenNLP Klassen o.ä. klonen, um zum einen den Hintergrund zu verstehen und sich zum anderen nicht abhängig zu machen.
   * Des Weiteren soll eine Generalisierung des Trainers eher weniger in Form von Klassenhierarchien und Vererbung gesehen, sondern eher durch lose Kopplung von generischen Util-Klassen. Dafür mal das Statistics Modul in DKPro anschauen, dass geht in ungefähr die gleiche Richtung.
   * DKPro Statistics kann auch für Agreement Measures genutzt werden
   * Notizen für Abarbeitung aktueller Probleme (Einrichtung unter Windows, etc.):
      * Miralium: https://zoidberg.ukp.informatik.tu-darmstadt.de/artifactory/webapp/search/artifact?2&q=mira
      * POM Dependencies:
       <verbatim>
<repositories>
        <repository>
            <id>ukp-oss-releases</id>
            <url>http://zoidberg.ukp.informatik.tu-darmstadt.de/artifactory/public-releases</url>
            <releases>
                <enabled>true</enabled>
                <updatePolicy>never</updatePolicy>
                <checksumPolicy>warn</checksumPolicy>
            </releases>
            <snapshots>
                <enabled>false</enabled>
            </snapshots>
        </repository>
    </repositories>
 <dependency>
   <groupId>de.tu-darmstadt.langtech.mira</groupId>
   <artifactId>mira</artifactId>
   <version>1.1.0</version>
  </dependency>
  <dependency>
   <groupId>cplex</groupId>
   <artifactId>cplex</artifactId>
   <version>1.0.0</version>
  </dependency>
  <dependency>
   <groupId>net.sf.trove</groupId>
   <artifactId>trove</artifactId>
   <version>2.1.0</version>
  </dependency>
</verbatim>
      * snapshot von DKPRO für Windows nutzen. (https://dkpro.github.io/dkpro-core/pages/setup-maven/) um AUX Problem. M2E Ver. 1.7. nutzen um zu schauen ob Probleme mit POM.


-- Main.TorstenDietl - 2016-11-21 - Telefonkonferenz

   * POM auf 1.8 stellen durch:
      * DKPro ParentPOM als Parent im Projekt eintragen. (um Java Encoding, und Version etc. voreinstellen zu lassen) (Version 15 ist die neuste)
   * Anstelle von Main Methoden lieber Test Methoden und JUnit nutzen.
   * In POM DependencyManagement die OpenNLP POM als Import POM eintragen. 
   * Nächster Termin: 29.11.2016 10:00


-- Main.TorstenDietl - 2016-11-14 - Telefonkonferenz

   *  Es wurden die Zugangsdaten zu dem UKP Git und Wiki durchgegeben. Es wurde abgesprochen das Projekt auf das UKP Git zu ziehen und die Besprechungen, Fragen, o.ä. in Form von Notizen in der studentenspezifischen Seite im Wiki festzuhalten.

   *  Die Verwirrung um UIMA Komponenten und den CasSampleStream wurde aufgelöst:
      * Es soll in einem ersten Ansatz auf UIMA oder den CasSampleStream komplett verzichtet werden. Insbesondere ist ein inkrementelles "Befüllen" des CasSampleStreams nicht trivial und unnötig komplex.
      * Stattdessen soll die OpenNLP API direkt genutzt und die EvalUtil Klasse genutzt werden um Samples aus dem GoldStandard als Liste zu laden, die dann inkrementell als TrainingSet genutzt werden kann.

   * Des Weiteren wurden nochmal folgende Projektspezifizierungen festgehalten:
      * Es soll in Batchgrößen trainiert werden (z.B. n=10)
      * Die kleinste Einheit für ein (randomisiertes) Laden, sollte ein Satz sein. Achtung die EvalUtil Klasse liefert vermutlich noch keine Gruppierung von POSTag in ihre Sätze, allerdings sind vermutlich Meta Informationen,   wie z.B. die Id o.ä. vorhanden, um eine Gruppierung relativ einfach erstellen zu können. 
      * Nach jedem Batch-Training sollen zwei Auswertungen geführt werden, zum einen eine Auswertung auf alle bisher gelernten Daten und zum anderen eine Auswertung auf die nächsten n Daten.
      * Es sollen mehrere Ansätze getestet werden:
         * Beginnend mit OpenNLP und lernen auf einem anwachsenden Trainingsdatenset
         * Über Miralium für einen echten inkrementellen Ansatz
         * Hin zu einem inkrementellen DeepLearning-Ansatz
      * Es ist die Idee, später aus den verschiedenen Ansätzen einen mehrstufigen Klassifizierer zu erstellen, der bei wenig Trainingsdaten einen Ansatz wählt, der eine hohe Precision hat und bei einer größeren Trainingsdatenmenge dann zu Ansätzen wechselt, die einen höheren Recall haben. Damit soll der User am Anfang nicht mit vielen unsinnigen Vorschlägen überschüttet werden. Des Weiteren ist die Annahme, dass man den "Verlust" der Precision bei einem "komplexeren" Ansatz damit kompensieren kann, dass man zum Zeitpunkt des Wechsels auf den höheren Ansatz bereits genug Trainingsdaten hat.
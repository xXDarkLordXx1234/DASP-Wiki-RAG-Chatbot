%META:TOPICINFO{author="dietl" comment="reprev" date="1481323573" format="1.1" reprev="5" version="6"}%
%META:TOPICPARENT{name="StudentsList"}%
-- Main.TorstenDietl - 2016-12-08 - Telefonkonferenz
   * Folgende Anmerkungen:
      * Eigenen Reader implementieren, geht schon, ist aber eher hinderlich, da die Formate sich durchaus sehr unterscheiden.
      * *TODO*: Um an Sätze etc. zu kommen sollte folgendes Verfahren genutzt werden: Von Grob nach Fein, d.h. mit dem Reader über das JCas iterieren, um die gröbste Einheit (z.B. Sätze) auszulesen. Dann mit UIMA über diese Einheit, also z.B. die Sätze iterieren um die nächstfeinere Einheit auszulesen (z.B. Tokens und danach die POS Tags). Also insbesondere nicht versuchen, die POS-Tags auszulesen und danach diese irgendwie den Sätzen zuzuordnen.
      * *<del>TODO</del>*: LinkedHashMap für das Zwischenspeichern der Chart Datasets nutzen, um die Sortierung zu gewährleisten.
      * *TODO*: Mehr mit Argumenten und Returns arbeiten um das Codeverständnis zu vereinfachen (z.B. bei der Methode processData, hier nicht direkt auf die Klassenattribute zugreifen, sondern diese übergeben bzw. zurückgeben)
      * *TODO*: Exceptions weiter werfen und nicht mit System.err.println o.ä. arbeiten
      * *TODO*: Wirklich überall Try (Ressource) nutzen (insbesonder bei getTagger mit dem Stream.close) um dort unnötigen Code zu sparen.
   * Ziele bis zum nächsten Mal:
      * Code verbessern (d.h. Anmerkungen umsetzen, aufräumen, kommentieren)
      * NamedEntityRecognizer zum Laufen bringen, die dortige IndexOutOfBounds Exception beheben.
      * Miralium implementieren/nutzen
         * Dafür Beispiele im Internet suchen bzw. WebAnno Code anschauen, dort wird es bereits genutzt
      * Optional: Einstellungsmöglichkeiten des OpenNLP Taggers finden. Kann dieser konfiguriert werden, oder muss die Klasse DefaultPosTagGenerator (getContext-Methode) geändert werden, um unterschiedliche Features, n-gramme, etc. zu nutzen? Vielleicht auch mal in die model.bin reinschauen um zu sehen, was aktuell als Context so erzeugt wird.
   * Nächster Termin: 21.12.2016 - 10:00 

-- Main.TorstenDietl - 2016-12-01 - Telefonkonferenz
   * Es wurde besprochen, wie DKPro auf Windows genutzt werden kann und wie Miralium eingebunden werden kann.
   * Es sollten am besten die EvalUtil Methoden genutzt werden, um  zu evaluieren, da diese bereits generisch und entkoppelt sind. Wo nötig können eigene Evaluation Klassen geschrieben werden, wenn Evaluationsmethoden benötigt werden, die so nicht in der EvalUtil vorhanden sind (Named Entity Evaluation mit BIO Methode)
   * Generell ist es in Ordnung eigene Klassen zu erstellen, die Verhalten von OpenNLP Klassen o.ä. klonen, um zum einen den Hintergrund zu verstehen und sich zum anderen nicht abhängig zu machen.
   * Des Weiteren soll eine Generalisierung des Trainers eher weniger in Form von Klassenhierarchien und Vererbung gesehen, sondern eher durch lose Kopplung von generischen Util-Klassen. Dafür mal das Statistics Modul in DKPro anschauen, dass geht in ungefähr die gleiche Richtung.
   * DKPro Statistics kann auch für Agreement Measures genutzt werden
   * Notizen für Abarbeitung aktueller Probleme (Einrichtung unter Windows, etc.):
      * Miralium: https://zoidberg.ukp.informatik.tu-darmstadt.de/artifactory/webapp/search/artifact?2&q=mira
      * POM Dependencies:
       <verbatim>
<repositories>
        <repository>
            <id>ukp-oss-releases</id>
            <url>http://zoidberg.ukp.informatik.tu-darmstadt.de/artifactory/public-releases</url>
            <releases>
                <enabled>true</enabled>
                <updatePolicy>never</updatePolicy>
                <checksumPolicy>warn</checksumPolicy>
            </releases>
            <snapshots>
                <enabled>false</enabled>
            </snapshots>
        </repository>
    </repositories>
 <dependency>
   <groupId>de.tu-darmstadt.langtech.mira</groupId>
   <artifactId>mira</artifactId>
   <version>1.1.0</version>
  </dependency>
  <dependency>
   <groupId>cplex</groupId>
   <artifactId>cplex</artifactId>
   <version>1.0.0</version>
  </dependency>
  <dependency>
   <groupId>net.sf.trove</groupId>
   <artifactId>trove</artifactId>
   <version>2.1.0</version>
  </dependency>
</verbatim>
      * snapshot von DKPRO für Windows nutzen. (https://dkpro.github.io/dkpro-core/pages/setup-maven/) um AUX Problem. M2E Ver. 1.7. nutzen um zu schauen ob Probleme mit POM.


-- Main.TorstenDietl - 2016-11-21 - Telefonkonferenz

   * POM auf 1.8 stellen durch:
      * DKPro ParentPOM als Parent im Projekt eintragen. (um Java Encoding, und Version etc. voreinstellen zu lassen) (Version 15 ist die neuste)
   * Anstelle von Main Methoden lieber Test Methoden und JUnit nutzen.
   * In POM DependencyManagement die OpenNLP POM als Import POM eintragen. 
   * Nächster Termin: 29.11.2016 10:00


-- Main.TorstenDietl - 2016-11-14 - Telefonkonferenz

   *  Es wurden die Zugangsdaten zu dem UKP Git und Wiki durchgegeben. Es wurde abgesprochen das Projekt auf das UKP Git zu ziehen und die Besprechungen, Fragen, o.ä. in Form von Notizen in der studentenspezifischen Seite im Wiki festzuhalten.

   *  Die Verwirrung um UIMA Komponenten und den CasSampleStream wurde aufgelöst:
      * Es soll in einem ersten Ansatz auf UIMA oder den CasSampleStream komplett verzichtet werden. Insbesondere ist ein inkrementelles "Befüllen" des CasSampleStreams nicht trivial und unnötig komplex.
      * Stattdessen soll die OpenNLP API direkt genutzt und die EvalUtil Klasse genutzt werden um Samples aus dem GoldStandard als Liste zu laden, die dann inkrementell als TrainingSet genutzt werden kann.

   * Des Weiteren wurden nochmal folgende Projektspezifizierungen festgehalten:
      * Es soll in Batchgrößen trainiert werden (z.B. n=10)
      * Die kleinste Einheit für ein (randomisiertes) Laden, sollte ein Satz sein. Achtung die EvalUtil Klasse liefert vermutlich noch keine Gruppierung von POSTag in ihre Sätze, allerdings sind vermutlich Meta Informationen,   wie z.B. die Id o.ä. vorhanden, um eine Gruppierung relativ einfach erstellen zu können. 
      * Nach jedem Batch-Training sollen zwei Auswertungen geführt werden, zum einen eine Auswertung auf alle bisher gelernten Daten und zum anderen eine Auswertung auf die nächsten n Daten.
      * Es sollen mehrere Ansätze getestet werden:
         * Beginnend mit OpenNLP und lernen auf einem anwachsenden Trainingsdatenset
         * Über Miralium für einen echten inkrementellen Ansatz
         * Hin zu einem inkrementellen DeepLearning-Ansatz
      * Es ist die Idee, später aus den verschiedenen Ansätzen einen mehrstufigen Klassifizierer zu erstellen, der bei wenig Trainingsdaten einen Ansatz wählt, der eine hohe Precision hat und bei einer größeren Trainingsdatenmenge dann zu Ansätzen wechselt, die einen höheren Recall haben. Damit soll der User am Anfang nicht mit vielen unsinnigen Vorschlägen überschüttet werden. Des Weiteren ist die Annahme, dass man den "Verlust" der Precision bei einem "komplexeren" Ansatz damit kompensieren kann, dass man zum Zeitpunkt des Wechsels auf den höheren Ansatz bereits genug Trainingsdaten hat.
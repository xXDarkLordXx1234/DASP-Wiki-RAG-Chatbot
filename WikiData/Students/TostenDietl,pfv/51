%META:TOPICINFO{author="dietl" comment="save topic" date="1493462766" format="1.1" reprev="45" version="51"}%
%META:TOPICPARENT{name="StudentsList"}%
-- Main.TorstenDietl - 2016-04-13 - Telefonkonferenz
   * TODOs left to do:
      * *TODO* DL4J in Webanno registrieren
      * *TODO* Selected Classifier Anzeige prüfen!
      * *TODO* Training und Prediction etc. wirklich nur ausführen wenn sich das Document geändert hat! (Momentan ist es für Tests noch deaktiviert)
      * *TODO* TODOs im Code entfernen.
      * *TODO* Recommendation-Project-Tab: Standardbelegung von Classifiern und Limits vorgeben


      * *TODO* Mehrere Layer in verschiedenen Farben rendern.
      * *TODO* Statusanzeige über Trainings und Prediction Status im DetailEditorPanel o.ä. anzeigen und Zustand pushen, wenn fertig Button zum Aktualisieren anbieten.
      * *TODO* Recommendation-Project-Tab: String Matching Classifier nur an/aus nicht mit Konfidenz.
      * *TODO* Loadable Detachable Model für LayerBox nutzen (Project Tab)
      * *TODO* SentenceLevelKommentare nutzen (Brat Visualizer.js) um Kommentare alá Satz ist komplett annotiert anzeigen zu können
      * *TODO* Selection-Bug auf EvaluationPage fixen
      * *TODO* Tab Panel Refresh Bug auf EvaluationPage fixen
      * *TODO* Recommendation Evaluation Page: Einstellungen für Limits als Spielerei und Button zum Übernehmen in Projekt.
   * Finished TODOs:
      * <del> *TODO* Recommendation Evaluation Page: Auch Layer auswählen lassen.</del>
      * <del> *TODO* Standard AnnotationLayer sind UIMA Klassen POS etc.</del>
      * <del> *TODO* AnnotationPage: Anzeigen, welcher der ausgewählte Klassifizierer ist.</del>
      * <del> *TODO* Automatisches Umschalten der Classifier (von String, zu OpenNlp, o.ä. etc.) je nach Konfidenz</del>
      * <del> *TODO* Mehrere Classifier in webanno verfügbar machen </del>
      * <del> *TODO* Trainieren auf allen Dokument pro Layer Pro User Pro Projekt </del>
      * <del> *TODO* EvaluationService für alle verfügbaren classifier (in webanno) laufen lassen</del>
      * <del> *TODO* Mira Template Load: Stream Closed Fehler beheben.</del>
      * <del> *TODO* Incremental Evaluation Service an Evaluation Page binden</del>
      * <del> *TODO* StringMatchingClassifier für NER bauen</del>
      * <del> *TODO* Nur automatisierbare Layer auch für die Automatisierung nutzen.</del>
      * <del> *TODO* Ner Classifier in Webanno registrieren</del>
      * <del> *TODO* Übernehmen von Vorschlägen durch Klick auf das Label. </del>
      
-- Main.TorstenDietl - 2016-03-29 - Telefonkonferenz
   * In RepositoryServiceDBData: getSourceDocumentFile() um Dateien (pro Projekt) von der Platte zu laden (z.B. Model) 
   * <del>Für Einstellungen ne Tabelle die mit der Projekt-ID als Primärschlüssel arbeitet.</del>
   * <del>Recommendation-Project-Tab: Nur automatisierbare Layer anzeigen</del>   
   * <del>Recommendation-Project-Tab: Aufteilung der Trainingsdaten in Training und Test pro Layer.</del>
   * <del>Überlegen wie man vorhandene Trainingsdaten aufteilt in Training und Testdaten. --> 40% Testdaten</del>

-- Main.TorstenDietl - 2016-03-17 - Telefonkonferenz
      
   * Finished To-Dos
      * <del> *TODO* Logging Ausbauen, damit man auch sieht was passiert z.B. wann Trainiert wird etc. dafür SLF4J aus webanno api übernehem (Binder, etc.), SLF4J nur als TestDependency in der IncrementalSuite</del>
      * <del> *TODO*: Fehler bei SettingsPage wenn neues Project ausgewählt.</del>
      * <del> *TODO*: ProjectSelection bei Evaluation Page wieder zu einfacher ListChoice.</del>
      * <del> *TODO*: Recommendation-Project-Tab: An/Ausschaltbar </del>
      * <del> *TODO*: try catch (Throwable) um alle Task run Methoden um fehler zu fangen.</del>
      * <del> *TODO* AnnotationDocument sollte ein Timestamp haben um Dirty Eigenschaft herauszufinden, sodass der TrainingsTask nur dann was macht, wenn sich die AnnotationsDaten geändert haben und der Prediction Task nur dann wenn sich die Trainingsdaten geändert haben.</del>
      * <del> *TODO* DKPro Testing auf scope test setzen und die TestUtil Klasse in testsrc packen.</del>
      * <del> *TODO* DKPro Testing aus webanno POM nehmen.</del>
      * <del> *TODO* RecommendationServiceImpl wieder einfügen, das ist verloren gegangen.</del>
      * <del> *TODO* Classifier auf das neue Frameworkkonzept umziehen, das alte dann entfernen.</del>
      * <del> *TODO* TrainingSuiteConfiguration überarbeiten</del>
      * <del> *TODO* Result Dateien in eigenen Ordner schreiben (_gitignore_)</del>
      * <del> *TODO* Die genannte Ordnerstruktur entweder einstellbar machen, oder als Konvention erwarten und entsprechend in der Doku beschreiben.</del>
      * <del> *TODO* Mira-Templates in Resourcen Ordner verschieben</del>
      * <del> *TODO* Ordner für Model-Dateien erstellen, in dem alle Modells gespeichert werden (_gitignore_)</del>
      * <del> *TODO* Ordner für Debug Infos erstellen, in dem alle Debug Infos gespeichert werden (_gitignore_)</del>
      * <del> *TODO* Glove Dateien in DKPro Datasets eintragen und in Zukunft herunterladen</del>
      * <del> *TODO* RecommendationEvaluationPage von BasePage ableiten. Von der Idee an die MonitoringPage anlehnen</del>

-- Main.TorstenDietl - 2016-03-08 - Telefonkonferenz
   * AnnotationDocument sollte ein Timestamp haben um Dirty Eigenschaft herauszufinden.
   * Prüfen ob wir die neuen Predictions auch an die AnnotationPage pushen können (http://stackoverflow.com/questions/20100732/trigger-repainting-a-wicket-component-via-push-message)
   * Evaluations-Schedule-Task der auf allen verfügbaren KnownData Punkten alle möglichen Classifier testet und den besten für die Prediction nimmt.
   * Den Copy Classifier immer benutzen, andere Classifier überschreiben den CopyClassifier wenn die Konfidenz einen gewissen Threshold überschreiten.
   * Den Copy Classifier nutzen um nicht vollständige Sätze zum trainieren zu nutzen.
   * Strategie zur Berechnung der Unknown-Batch-Data-Size anpassbar machen.

-- Main.TorstenDietl - 2016-02-24 - Telefonkonferenz
   * <del>Zum Debuggen Annotation Doc mal als XMI exportieren.</del>
   * *<del>TODO</del>* Konsistenzprüfung abschalten: settings.properties im Webanno Home ändern: debug.casDoctor.forceReleaseBehavior=true
   * *<del>TODO</del>* Training und Prediction Asynchron: Entweder mit Java Schedular Service oder Quartz Bibliothek.
   * *<del>TODO</del>* Beim Evaluieren die bisherigen bekannten Annotationsdaten als Gesamtdaten ins Framework packen.
      * Evaluationsseite (orientieren an AnnotationPage.java/html o.ä. und in die WelcomePage eintragen)
   * *TODO* Ein Dirty-Zustand einführen, sodass der TrainingsTask nur dann was macht, wenn sich die AnnotationsDaten geändert haben und der Prediction Task nur dann wenn sich die Trainingsdaten geändert haben.

-- Main.TorstenDietl - 2016-02-03 - Telefonkonferenz
   * Bei adapter.updateFeature() ca. Zeile 797 recommendationService.addAnnotationForTraining(jCas, aBModel, selectedTag, selection);
   * Im BratAnnotator dann die vorgeschlagenen Annotationen in das JSON packen. Damit diese nur angezeigt, aber nicht persisitiert werden.
   * BratRenderer manipulieren oder eher eigenen schreiben, der nach dem BratRenderer aufgerufen wird, um die Recommendations anzuzeigen. 
      * BratSpanRenderer kopieren und anpassen.
   * Und AnnotaterStateImpl nutzen, um an die aktuellen Sätze zu kommen.
   * Issue wegen Internem DB Problem bei Windows.

-- Main.TorstenDietl - 2016-02-03 - Telefonkonferenz
   * Lokaler Klon von Webanno (aus GitHub ziehen) lokal und dann in GitLab ein neues Projekt erzeugen (Am besten remote Referenz auf Github und Gitlab fürs Fetchen, aber nur auf GitLab fürs pushen)
   * Neues Modul anlegen für incremental trainer. 
   * Reinhängen in WebAnno Brat (BratAnnotator.java) und evtl. WebAnno UI Annotation.

-- Main.TorstenDietl - 2016-01-30 - Telefonkonferenz
   * Fragen aus der SIG-USER
      * "Iteration"-Label auf der X-Achse ist verwirrend weil es auch bei den Classifier Parametern eine "Iteration" gibt. Vorschlag: "Increment"?
      * "Next data" verwirrt auch etwas 
      * Warum stürzt Mira im letzten Schritt ab? Das war vorher nicht.
      * Es verwirrt dass *trainingSetIncrementationFunction* und *knownDataIncrementationFunction* getrennt sind - wird die "known data" nicht als "training data" verwendet?

-- Main.TorstenDietl - 2016-01-25 - Telefonkonferenz
   * 1. *<del>TODO</del>* DL4J Komponente fertigstellen
      * BinaryVectorizer (zum Einlesen der Word-Embeddings) mal mit der DKPro Version probieren (https://github.com/dkpro/dkpro-core/tree/master/dkpro-core-api-embeddings-asl/src/main/java/org/dkpro/core/api/embeddings)
      * Falls das nicht funktioniert mit den Textdateien von http://nlp.stanford.edu/projects/glove/ (6B) arbeiten und diese mit dem BinaryVectorizerTest aus dem DL4J Playground konvertieren.
   * 2. Konfigurierbar machen 
      * *<del>TODO</del>* Generell erstmal schauen welche Konfigparameter existieren und sinnvoll sind und vielleicht über eine Konfigklasse erstmal steuern
      * *TODO* (mit JSON oder YAML), SnakeYAML als Framework/API nutzbar, oder Kommandozeilen Params, z.B. mit Args4J. .
   * 3. *<del>TODO</del>* NamedEntityRecognizer zum Laufen bringen.
   * 4. *<del>TODO</del>* Für Montagsmeeting etwas herstellen und mit Doku anfangen.
   * 5. Einbauen in WebAnno.

-- Main.TorstenDietl - 2016-01-18 - Telefonkonferenz
Übrig gebliebene TODOs:
   * *<del>TODO</del>* Randomisierungsfunktion für die Trainingsdaten (nur Sätze!)
   * *TODO* DL4J einbauen, Zeit aufholen
   * *TODO* Tests über z.B. JSON Config-Dateien konfigurierbar machen.
   * *TODO* Testauszüge/kleinen Report für die drei Algos (OpenNlp, Mira, DL4j) mit verschiedenen Konfigurationen für die Special Interest Group (trifft sich Montags) umsetzen.
   * *TODO* In WebAnno (System und die GUI) einarbeiten, Überlegungen/Gedanken zur Architektur der WebAnno Erweiterung machen (Ideen s.u.)
   * *TODO* NamedEntityRecognizer zum Laufen bringen.
   * *TODO* In Lambda Funktionen Exception fangen und RuntimeException schmeißen um StackTrace durchgeben zu können.
   * *TODO* Issue mit GUM-Loading untersuchen und dann in DKPro eintragen

Gedanken:
- DL4J Tuning durch HistogrammEvaluationListener "sichtbar" machen. Diesen aktivieren um einen Webservice scharf zu schalten, der Fehlerquote etc. anzeigt.
- Für WebAnno: Zwischen eigentlichem System und den BratAnnotator werden JSON Nachrichten verschickt. Idee: Sich dazwischen hängen, um das generierte JSON zu manipulieren und um z.B. die vom ML-Algo vorgeschlagenen Annotations erweitern.
- Auf dem Rückweg, d.h. wenn der User auf solch eine Annotation klickt um sie zu akzeptieren, die Nachricht abfangen und z.B. in eine "Annotation erstellen" Message umwandeln

-- Main.TorstenDietl - 2016-01-11 - Telefonkonferenz
Übrig gebliebene TODOs:
   * *TODO* In Lambda Funktionen Exception fangen und RuntimeException schmeißen um StackTrace durchgeben zu können.
   * *TODO* Issue mit GUM-Loading untersuchen und dann in DKPro eintragen
   * *<del>TODO</del>* Miralium Train Methode "nachbauen" um inkrementell selbst zu trainieren, da die Train Methode ein fertiges Dokument (Datei) erwartet.
   * *<del>TODO</del>* Bei NextData wieder doch festen Batch-Size benutzen.
   * *<del>TODO</del>* EvaluateNext prüfen/debuggen. Ergebnisse erscheinen falsch. Vielleicht mal "händisch" einen 80/20 Split oder ähnliches trainieren/testen und vergleichen.
   * *<del>TODO</del>* Doch wieder Iterationsnummer auf der X-Achse nehmen. TrainingsSetSize nur in Tabelle nutzen.
   * *<del>TODO</del>* Miralium zum Laufen bringen
   * Nächster Termin: 18.01.2017 - 14:00


-- Main.TorstenDietl - 2016-12-21 - Telefonkonferenz
Es wurde der jetztige Codestand besprochen, und die weiteren Ziele/TODOs erarbeitet:
   * *<del>TODO</del>* Auf DKPro.org --> Contributing --> CodeStyle Format runterladen und nutzen, d.h. Klassen formatieren. Klassen JavaDocs schreiben
   * *<del>TODO</del>* IncrementalTrainer: SentenceData in AllData umbenennen.
   * *<del>TODO</del>* Die loadAnnotationObjects Methode der Klasse GenericEvalUtil so abändern, das eine Liste von Listen von AnnotationObject zurückkommt, also eine Satzbasierte Liste, damit spart man sich die processData Methode, da diese aktuell noch Sentence Objekte nutzt und diese zum CAS gehören, d.h. am Ende des Lesens verworfen werden.
   * *<del>TODO</del>* Hash und Equals Methoden im AnnotationObject deklarieren und direkt die Liste von AnnotationObjects der DumpResult Methode übergeben. Damit spart man sich die transformingFunction.
   * *TODO* In Lambda Funktionen Exception fangen und RuntimeException schmeißen um StackTrace durchgeben zu können.
   * *<del>TODO</del>* Die For-Schleife in der addIncrement Methode verständlicher schreiben.
   * *<del>TODO</del>* Timeline in Google Doc erstellen (d.h. Ziele der Projektbeschreibung in einen Wochenplan eintragen)
   * *TODO* Issue mit GUM-Loading untersuchen und dann in DKPro eintragen
   * *<del>TODO</del>* In Git UKP ein neues Projekt für Miralium Source Code anlegen, dieses in Maven nutzen, um Miralizm debuggen zu können.
   * *TODO* Miralium Train Methode "nachbauen" um inkrementell selbst zu trainieren, da die Train Methode ein fertiges Dokument (Datei) erwartet.

   Weitere Anmerkungen/Anregungen/Ideen von Richard:
   * *<del>TODO</del>* Ability to limit number of training samples, e.g. to 1000 to speed up the experimentation process.
   * *<del>TODO</del>* use fibonacci scale for increasing training set size so we get more resolution in the beginning   
   * Save results to files
      * for tabular reports, can use DKPro Lab FlexTable class
      * *<del>TODO</del>* for writing charts as PDFs can use DKPro Lab ChartUtil
      * Generate images from saved results
      * *<del>TODO</del>* Include curves for all measures (f, prec, rec, accuracy) in a single image
      * *<del>TODO</del>* in the curve, show size of training set on x axis, not interation number
      * ability to raw dump gold standard results to files
      * ability to raw dump tagging results to files so we can compare directly against gold standard
   * include confidence scores in dumps
   * can we also get the second-best tagging option and its confidence score?
   * *<del>TODO</del>* record time required for training/tagging in each step - that allows us to compare true online learning (e.g. using mira) vs always retraining
   * *<del>TODO</del>* ability to run experiments as JUnit tests, not via GUI

   Nächster Termin: 11.01.2017 - 10:00 Uhr

-- Main.TorstenDietl - 2016-12-08 - Telefonkonferenz
   * Folgende Anmerkungen:
      * Eigenen Reader implementieren, geht schon, ist aber eher hinderlich, da die Formate sich durchaus sehr unterscheiden.
      * *<del>TODO</del>*: Um an Sätze etc. zu kommen sollte folgendes Verfahren genutzt werden: Von Grob nach Fein, d.h. mit dem Reader über das JCas iterieren, um die gröbste Einheit (z.B. Sätze) auszulesen. Dann mit UIMA über diese Einheit, also z.B. die Sätze iterieren um die nächstfeinere Einheit auszulesen (z.B. Tokens und danach die POS Tags). Also insbesondere nicht versuchen, die POS-Tags auszulesen und danach diese irgendwie den Sätzen zuzuordnen.
      * *<del>TODO</del>*: LinkedHashMap für das Zwischenspeichern der Chart Datasets nutzen, um die Sortierung zu gewährleisten.
      * *<del>TODO</del>*: Mehr mit Argumenten und Returns arbeiten um das Codeverständnis zu vereinfachen (z.B. bei der Methode processData, hier nicht direkt auf die Klassenattribute zugreifen, sondern diese übergeben bzw. zurückgeben)
      * *<del>TODO</del>*: Exceptions weiter werfen und nicht mit System.err.println o.ä. arbeiten
      * *<del>TODO</del>*: Wirklich überall Try (Ressource) nutzen (insbesonder bei getTagger mit dem Stream.close) um dort unnötigen Code zu sparen.
   * Ziele bis zum nächsten Mal:
      * Code verbessern (d.h. Anmerkungen umsetzen, aufräumen, kommentieren)
      * NamedEntityRecognizer zum Laufen bringen, die dortige IndexOutOfBounds Exception beheben.
      * Miralium implementieren/nutzen
         * Dafür Beispiele im Internet suchen bzw. WebAnno Code anschauen, dort wird es bereits genutzt
      * Optional: Einstellungsmöglichkeiten des OpenNLP Taggers finden. Kann dieser konfiguriert werden, oder muss die Klasse DefaultPosTagGenerator (getContext-Methode) geändert werden, um unterschiedliche Features, n-gramme, etc. zu nutzen? Vielleicht auch mal in die model.bin reinschauen um zu sehen, was aktuell als Context so erzeugt wird.
   * Nächster Termin: 21.12.2016 - 10:00 

-- Main.TorstenDietl - 2016-12-01 - Telefonkonferenz
   * Es wurde besprochen, wie DKPro auf Windows genutzt werden kann und wie Miralium eingebunden werden kann.
   * Es sollten am besten die EvalUtil Methoden genutzt werden, um  zu evaluieren, da diese bereits generisch und entkoppelt sind. Wo nötig können eigene Evaluation Klassen geschrieben werden, wenn Evaluationsmethoden benötigt werden, die so nicht in der EvalUtil vorhanden sind (Named Entity Evaluation mit BIO Methode)
   * Generell ist es in Ordnung eigene Klassen zu erstellen, die Verhalten von OpenNLP Klassen o.ä. klonen, um zum einen den Hintergrund zu verstehen und sich zum anderen nicht abhängig zu machen.
   * Des Weiteren soll eine Generalisierung des Trainers eher weniger in Form von Klassenhierarchien und Vererbung gesehen, sondern eher durch lose Kopplung von generischen Util-Klassen. Dafür mal das Statistics Modul in DKPro anschauen, dass geht in ungefähr die gleiche Richtung.
   * DKPro Statistics kann auch für Agreement Measures genutzt werden
   * Notizen für Abarbeitung aktueller Probleme (Einrichtung unter Windows, etc.):
      * Miralium: https://zoidberg.ukp.informatik.tu-darmstadt.de/artifactory/webapp/search/artifact?2&q=mira
      * POM Dependencies:
       <verbatim>
<repositories>
        <repository>
            <id>ukp-oss-releases</id>
            <url>http://zoidberg.ukp.informatik.tu-darmstadt.de/artifactory/public-releases</url>
            <releases>
                <enabled>true</enabled>
                <updatePolicy>never</updatePolicy>
                <checksumPolicy>warn</checksumPolicy>
            </releases>
            <snapshots>
                <enabled>false</enabled>
            </snapshots>
        </repository>
    </repositories>
 <dependency>
   <groupId>de.tu-darmstadt.langtech.mira</groupId>
   <artifactId>mira</artifactId>
   <version>1.1.0</version>
  </dependency>
  <dependency>
   <groupId>cplex</groupId>
   <artifactId>cplex</artifactId>
   <version>1.0.0</version>
  </dependency>
  <dependency>
   <groupId>net.sf.trove</groupId>
   <artifactId>trove</artifactId>
   <version>2.1.0</version>
  </dependency>
</verbatim>
      * snapshot von DKPRO für Windows nutzen. (https://dkpro.github.io/dkpro-core/pages/setup-maven/) um AUX Problem. M2E Ver. 1.7. nutzen um zu schauen ob Probleme mit POM.


-- Main.TorstenDietl - 2016-11-21 - Telefonkonferenz

   * POM auf 1.8 stellen durch:
      * DKPro ParentPOM als Parent im Projekt eintragen. (um Java Encoding, und Version etc. voreinstellen zu lassen) (Version 15 ist die neuste)
   * Anstelle von Main Methoden lieber Test Methoden und JUnit nutzen.
   * In POM DependencyManagement die OpenNLP POM als Import POM eintragen. 
   * Nächster Termin: 29.11.2016 10:00


-- Main.TorstenDietl - 2016-11-14 - Telefonkonferenz

   *  Es wurden die Zugangsdaten zu dem UKP Git und Wiki durchgegeben. Es wurde abgesprochen das Projekt auf das UKP Git zu ziehen und die Besprechungen, Fragen, o.ä. in Form von Notizen in der studentenspezifischen Seite im Wiki festzuhalten.

   *  Die Verwirrung um UIMA Komponenten und den CasSampleStream wurde aufgelöst:
      * Es soll in einem ersten Ansatz auf UIMA oder den CasSampleStream komplett verzichtet werden. Insbesondere ist ein inkrementelles "Befüllen" des CasSampleStreams nicht trivial und unnötig komplex.
      * Stattdessen soll die OpenNLP API direkt genutzt und die EvalUtil Klasse genutzt werden um Samples aus dem GoldStandard als Liste zu laden, die dann inkrementell als TrainingSet genutzt werden kann.

   * Des Weiteren wurden nochmal folgende Projektspezifizierungen festgehalten:
      * Es soll in Batchgrößen trainiert werden (z.B. n=10)
      * Die kleinste Einheit für ein (randomisiertes) Laden, sollte ein Satz sein. Achtung die EvalUtil Klasse liefert vermutlich noch keine Gruppierung von POSTag in ihre Sätze, allerdings sind vermutlich Meta Informationen,   wie z.B. die Id o.ä. vorhanden, um eine Gruppierung relativ einfach erstellen zu können. 
      * Nach jedem Batch-Training sollen zwei Auswertungen geführt werden, zum einen eine Auswertung auf alle bisher gelernten Daten und zum anderen eine Auswertung auf die nächsten n Daten.
      * Es sollen mehrere Ansätze getestet werden:
         * Beginnend mit OpenNLP und lernen auf einem anwachsenden Trainingsdatenset
         * Über Miralium für einen echten inkrementellen Ansatz
         * Hin zu einem inkrementellen DeepLearning-Ansatz
      * Es ist die Idee, später aus den verschiedenen Ansätzen einen mehrstufigen Klassifizierer zu erstellen, der bei wenig Trainingsdaten einen Ansatz wählt, der eine hohe Precision hat und bei einer größeren Trainingsdatenmenge dann zu Ansätzen wechselt, die einen höheren Recall haben. Damit soll der User am Anfang nicht mit vielen unsinnigen Vorschlägen überschüttet werden. Des Weiteren ist die Annahme, dass man den "Verlust" der Precision bei einem "komplexeren" Ansatz damit kompensieren kann, dass man zum Zeitpunkt des Wechsels auf den höheren Ansatz bereits genug Trainingsdaten hat.
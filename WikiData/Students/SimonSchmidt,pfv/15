%META:TOPICINFO{author="schmidt" comment="" date="1622494420" format="1.1" reprev="14" version="15"}%
%META:TOPICPARENT{name="StudentsList"}%
-- Main.TilmanBeck - 2021-04-22

*Supervisor*: M.Sc. Tilman Beck 

*Start Date*: 12.04.2021

*Mid-Term Presentation Date*: 20.07.2021

*Final Presentation Date*: 21.09.2021 

*End Date*: 12.10.2021 [To be finalised by Informatik Pr√ºfungskommission]

---++++ *25.05.2021-01.06.2021*
---++ Progress
   * Adjusted section mapping (all sections before results that are not in any other category are now methods)
   * Created a new training dataset with 8 entries per abstract sentence
      * Two positives and negatives for the default specter and tfidf respectively
      * The two most similar sentences of the section were chosen as positives, the two most dissimilar sentences as negatives
   * Split the data into train and test set and determined metrics (mean, median and variance for all/positive/negative labelled sentences)
   * Pretrained on the new training data with evaluation on the test set
      * One run with no thresholds
      * One with thresholds based on the lower value of mean and median of the positive instances
      * One with thresholds based on the lower value of mean and median on all instances
      * One with slightly higher values than the higher value of mean and median of the positive instances
      * One with thresholds based on the higher value of mean and median of the negative instances
   * Thresholds based on the positive instances show good performance on the test set (~0.8 cosine F1)
   * Finetuned and evaluated the two runs with the best performance on the pretraining testset and compared against the run with no thresholds:
      * The two best runs have a cosine F1 of ~0.8, while the run without thresholds has a cosine F1 of ~0.76 (all on the pretraining testset)
      * The two runs show a slightly higher performance on the CL-SciSumm task (~0.085 vs ~0.08), indicating a correlation between pretraining performance and downstream task performance
      * The results are still below our baselines
        
---++ Encountered Problems
   * Made a mistake when creating the data and had to throw away the first run

---++++ *11.05.2021-25.05.2021*
---++ Progress
   * Extracted and labeled ARC abstracts
   * Created dataset from ARC abstracts and the corresponding text
      * Construction: for each abstract sentence label take sentences of the corresponding section of the same paper and add the resulting pair to the training data. Sentences were chosen based on different metrics (SPECTER, tf-idf, bm25 or random)
      * Version 1: 2 sentences chosen based on (default) SPECTER cosine similarity, 2 based on tf-idf and one random
      * Version 2: 2 sentences chosen based on (default) SPECTER cosine similarity, 2 based on bm25 and one random
      * Contains roughly 190.000 sentence pairs
   * Pretrained both SPECTER and SciBERT for five and one epoch and then finetuned on the CL-SciSumm data like the previous models
      *  one epoch pretraining outperforms five epochs, all further tests were done with one epoch
      * SPECTER performance was worse than SciBERT performance
      * Pretraining was done with MultipleNegativeRankingLoss
   * All finetuned models showed worse performance than the current best model (SciBERT trained only on CL-SciSumm data)
   * Models were finetuned with:
      * The previous sequence pair approach
      * sbert ContrastiveLoss
      * sbert SoftmaxLoss
      * Other parameters were the same as in the previous experiments (though I did also try some slight variations)
      * Best Task 1A performance was an F1 score of ~0.09 (vs. ~0.12 previously) using ContrastiveLoss
   * Tried TSDAE based on SciBERT
      * Pretrained on the abstract sentences for 1 epoch
      * finetuned with ContrastiveLoss for 3 epochs
      * Reaches an F1 score of ~0.10 for Task 1A
   * The second version of the dataset resulted in worse performance (same SciBERT as above with F1 of ~0.065 instead of ~0.10)
   * SimCSE reaches an F1 score of ~0.06

---++ Encountered Problems
   * Not all abstract sentences have a section corresponding to their assigned label
   * Furthermore some sections are too short to pick even two sentences; these were skipped (and the corresponding abstract sentence too)

---++ Next Steps
   * Finish SimCSE tests
   * Potentially have a look at the S2ORC data on [[https://www.sbert.net/examples/training/paraphrases/README.html][the sbert site]](titles only unfortunately)
---++++ *04.05.2021-11.05.2021*
---++ Progress
   * Extracted text from vertical ARC and split the data into documents
   * Created section title statistics; I've used only articles with at least 4 sections for this, leaving us with 20761 abstracts (and thus articles) out of 25504 articles
   * Downloaded and extracted overlap between pubmed-rct and the PubMedCentral Open Access Subset
   * Tested some filtering techniques for PubMed Data (removing images, tables, etc.)

---++ Encountered Problems
   * Downloading from PubMedCentral takes some time
   * Some minor formatting issues after extracting text from the vertical ARC

---++++ *27.04.2021-04.05.2021*
---++ Progress
   * Able to get most section headers in cleaned ACL ARC using regex
   * Can remove page numbers from cleaned ACL ARC
   * Found a way to distinguish (most) numbered lists from section headers
   * Structured cleaned ACL ARC into pairs of section header and text for each section
   * Trained CSAbstruct
   * The Open Access subset of PubMedCentral contains 13978 articles from the PubMed-RCT dataset

---++ Encountered Problems
   * Section names are not standardized except for "Abstract" and "Introduction". As a result, separating methods from results and previous works can be difficult.
     Especially since "Methods" doesn't seem to appear as a header in most papers.
   * Regex can't reliably distinguish between section headers and numbered lists or beginning with specific numbers in general
   * Section headers are sometimes split into two or more lines; solution can result in errors in specific lines
   * CSAbstruct training did not work out of the box
   * PubMedCentral [[https://www.ncbi.nlm.nih.gov/pmc/about/copyright/][does not allow bulk downloads of articles except for a specific subset]]

---++ Next Steps
   * Create structure from parsed file content

---++++ *20.04.2021-27.04.2021*
---++ Progress
   * Tested different thresholds for deciding whether multiple references should be selected
      * Threshold slightly improves task 1A sentence overlap, but reduces Rouge score (from 0.139 to 0.045)
      * Threshold greatly improves task 1B performance from 0.170 to 0.375 (both F1 micro)
   * Took a look at downloading ACL ARC and Pubmed RCT full texts

---++ Encountered Problems
   * Had to restructure training/validation data for task 1A and retrain model for threshold tests
   * Retrained model performance is slightly worse than previous (likely due to small changes in the data splits)
   * Predicted probabilities vary and are extremely close on the validation data, hard thresholds had bad results
   * Decided to use a portion of the highest probability reference as threshold, e.g. if the probability of a reference is within 0.1% of the highest probability it is selected too
   * The [[https://acl-arc.comp.nus.edu.sg/][ACL ARC page]] appears to be offline

---++ Next Steps
   * Determine how many articles in Pubmed RCT are freely available
   * Write tool to automatically crawl those articles
   * Possibly get ACL ARC papers via Github if the site stays offline

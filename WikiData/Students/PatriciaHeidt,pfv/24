%META:TOPICINFO{author="heidt" comment="" date="1593520392" format="1.1" reprev="22" version="24"}%
%META:TOPICPARENT{name="StudentsList"}%
---+!! Master Thesis

*Supervisor:*
   * Tilman Beck

*Presentations:*
   * 11.08.2020 Mid-term presentation
   * 27.10.2020 final presentation 

*Deadline:*
   * 16.11.2020

---++ 22.06. - 05.07.:
---+++ Crowdclustering implementation from Cocos and Callison-Burch: _Clustering Paraphrases by Word Sense_ (2016)
   * they cluster paraphrases for targetwords (in our case this would be clustering arguments for topics)
   * clustering HITs are run in multiple iterations and in each iteration 10 words are randomly selected for each targetword
   * there is only one HIT per iteration and targetword
   * after each iteration, the annotations are evaluated and clusters are constructed
   * not sure how many iterations would be needed

---++ 15.06. - 21.06.:

---+++ Methoden zur Datensatz Annotation
   1 Crowdclustering, z.B.
      * Gomes et al.: _Crowdclustering_ (2011)
      * Cocos and Callison-Burch: _Clustering Paraphrases by Word Sense_ (2016)
   2 Elemente im Datensatz mit vorgegebenen Kategorien annotieren, z.B.
      * Xu et al.: _Implicit feature identification in Chinese reviews using explicit topic mining model_ (2015)
      * Goldberg et al.: _May All Your Wishes Come True: A Study of Wishes and How to Recognize Them_ (2009)
      * Lu and Zhai: _Opinion integration through semi-supervised topic modeling_ (2008)
      * Li and Roth: _Learning question classifiers_ (2002)
      * Rosell et al.: _Comparing comparisons: Document clustering evaluation using two manual classifications_ (2004)
   3 Daten zu vorgegenen Kategorien sammeln/erstellen, z.B.
      * Sun et al.: _Query-adaptive shape topic mining for hand-drawn sketch recognition_ (2012)
      * Sang and Xu: _Browse by chunks: Topic mining and organizing on web-scale social media_ (2011)
      * Hu et al.: _Exploiting internal and external semantics for the clustering of short texts using world knowledge_ (2009)
   4 Daten crawlen, die von Nutzern schon gelabelet sind, z.B.
      * Li et al.: _Mining User Interest in Microblogs with a User-Topic Model_ (2014)
      * He et al.: _Short Text Feature Extraction and Clustering for Web Topic Mining_ (2007)
      * Rasiwasia et al.: _A new approach to cross-modal multimedia retrieval_ (2010)
      * Griffiths and Steyvers: _Finding scientific topics_ (2004)
      * Yan et al: _A biterm topic model for short texts_ (2013)
      * Jin et al: _Transferring topical knowledge from auxiliary long texts for short text clustering_ (2011)
      * Zhang and LeCun: _Text understanding from scratch_ (2015)
      * Zubiaga and Ji: _Harnessing Web Page Directories for Large-Scale Classification of Tweets_ (2013)

---++++ Crowdclustering (Gomes et al. 2011)
   * Ansatz:
      1 Elemente des Datensatzes werden zufällig auf HITs aufgeteilt
      2 Crowdworker teilen die Daten eines HITs in Cluster auf
      3 Annotationen der Worker werden als paarweiser Vergleich zwischen den einzelnen Datenpunkten interpretiert
      4 generatives Modell berechnet wahrscheinlichste Cluster
      5 es ergibt sich eine hierarchische Struktur der Cluster, aus der ersichtlich wird, wie einzelne Worker die "atomaren" Cluster zusammengelegt haben
   * Paper ist sehr "mathematisch" und die Implementierung des vierten Schrittes könnte sehr kompliziert sein

---++ Meetings:

---+++ 22.06.2020
   * 3 mögliche Ansätze für Annotation
      1 2-teiliges Crowdsourcing:
         1 Argumente clustern
         2 Menschen für jedes Cluster aus etwa 5 Argumenten das repräsentativste/wichtigste auswählen lassen
         3 Menschen alle Argumente zu den im vorherigen Schritt ausgewählten Argumenten klassifizieren lassen
      2 aus externer Quelle Kategorien erhalten und danach klassifizieren
      3 Crowdclustering: Menschen von Hand Teile des Datensatzes clustern lassen und dann Ergebnisse zusammenführen
   * nächste Schritte:
      * Ansätze formalisieren
      * wie kann man die Ergebnisse vergleichen?

---+++ 10.06.2020
   * Ziele für Annotation Guideline:
      * Ansatz, der für Crowd-Sourcing geeignet ist
      * nur von den Daten zu Clustern, ohne aus irgendeiner Quelle Kategorien zu nehmen
   * als nächstes recherchieren, wie Clusteringdatensätze für andere Problemstellungen annotiert werden
   * nächstes Meeting am 24.6. um 16 Uhr

---+++ 15.05.2020
   * an Related Work weiterarbeiten
      * auch Paper erwähnen, die nur ungefähr zum Thema passen
      * bis ~22.05. abschließen
      * besonders auch Paper, die die Annotation von Clustering Datensätzen beschreiben
   * Überblick über bestehende Datensätze erstellen (Domäne, Größe, Annotationsschema, ...)
   * Gedanken machen, wie man Unsicherheit in der Zuordnung im Clustering modellieren kann (z.B. Soft Clustering)
   * Clusteringalgorithmen, -metriken ausprobieren

---+++ 05.05.2020
   * Besprechung des bisherigen Standes der Related Work
      * Paper zur Aspect Classification deutlich ausführlicher beschreiben
      * auf Unterschiede zur Arbeit eingehen
   * Literatur zur Annotation von Clusterings und zu Short Text Clustering lesen
   * Thema überlegen und per E-Mail abklären, dann Anmeldeformular ausfüllen und ans Prüfungssekretariat schicken
   * nächstes Meeting am 15.5.

---+++ 20.04.2020


---++ 01.06. - 07.06.:

---+++ Datensatz Annotation

---++++ Ziele Datensatz
   * einfach als Clustering Gold Standard zu verwenden
   * sinnvolle Cluster (gute Anzahl, gute Größe, sinnvolle Unterschiede und Gemeinsamkeiten)
   * passt zum Anwendungsfall (welcher Anwendungsfall?)

---++++ Bestehende annotierte Datensätze
   * Reason Datensatz (Hasan and Ng: _Why are you taking this stance? Identifying and classifying reasons in ideologic debates_) ziemlich genau das, was ich mir vorstellen würde
   * Arguing Datensatz (Conrad et al.: _Recognizing arguing subjectivity and arguing tags_) wahrscheinlich auch sehr gut, aber mit der hierarchischen Struktur muss man sich irgendwie überlegen, auf welcher Ebene man die Cluster trennen sollte
   * Webis Datensatz (Ajjour et al.: _Modeling frames in argumentation_) ähnlich einfach zu verwenden wie Reason, aber die Qualität der Annotationen ist wahrscheinlich nicht optimal, da es für jedes Thema komplett unabhängig voneinander ist
   * bei ComArg Datensatz (Boltuzic and Snajder: _Back up your stance: Recognizing arguments in online discussions_) muss man sich irgendwie überlegen, wie man die Attack/Support Annotationen in Cluster übersetzen kann
   * ArgKP Datensatz (Bar-Haim et al.: _From arguments to key points: towards automatic argument summarization_): wenn man die Informationen der "non-matching" Paare einfach wegschmeißt, kann man ihn genauso verwenden wie den Reason Datensatz
   * AFS (Misra et al.: _Measuring the similarity of sentential arguments in dialogue_) und UKP Aspect (Reimers et al.: _Classification and clustering of arguments with contextualized word embeddings_) Datensätze annotieren Ähnlichkeiten zwischen Sätzen und bräuchten daher wahrscheinlich ganz andere Evaluationsmethoden

| *Name*     | *Topics* | *Arguments*           |     *Arguments per topic* | *Sentences per argument* |
| Arguing    | 1      | 1,215               | 1,215                   | 1.0 |
| Reason     | 4      | 2,813               | 703                     | 1.4 |
| Webis      | 465    | 12,326              | 27                      | 4.1 |
| ComArg     | 2      | 373                 | 187                     | 6.0 |
| ArgKP      | 28     | 6,515               | 233                     | 1.2 |
| AFS        | 3      | 2,648               | 883                     | 1.0 |
| UKP Aspect | 28     | 2,010               | 72                      | 1.0 |



---++++ Ziele Annotation
   * gute Skalierung mit großen Datensätzen
   * einfach verständlich
   * möglichst objektiv und reproduzierbar
   * enstehender Datensatz passt zu Zielen

---++ 18.05. - 31.05.:

---+++ Related Work

---++++ Evaluation

*Silhouettes: a graphical aid to the interpretation and validation of cluster analysis (Rousseeuw)*
   * uses pairwise (dis)similarity scores
   * compute closest neighboring cluster for each element
   * compare how close that neighbor is and how close the other points from the same cluster are
   * can use averages of scores to assess quality of each cluster and whole clustering

*Estimating the number of clusters in a data set via the gap statistic (Tibshirani et al.)*
   * uses data points/vectors; use with pairwise (dis)similarities is complicated
   * compare differences in clusters with an appropriate expectation function
   * maximum of difference is the best clustering/best number of clusters

*V-Measure: A conditional entropy-based external cluster evaluation measure (Rosenberg and Hirschberg)*
   * compares clustering result from some clustering approach with annotated target clustering
   * maximize homogeneity (only elements from one target cluster per cluster) and completeness (only one cluster for each target cluster)

*Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance (Vinh et al.)*
   * compares clustering result from some clustering approach with annotated target clustering
   * compare many evaluation approaches under 3 criteria:
      1 metric property: symmetric, positive definite, triangle inequality
      2 normalized: bounded, ideally [0,1] or [-1,1]
      3 constant baseline: e.g. more clusters does not naturally lead to higher scores
   * introduce adjusted mutual information, which fulfills criteria 2 and 3 but not 1
   * adjustment for chance, i.e. criterion 3, mostly needed when the number of data points is relatively small compared to the number of clusters
   * advocate for normalized information distance(fulfills criteria 1 and 2) as a "general purpose" measure

---++++ Argument Aspect Classification

*Argumentation Mining in Parliamentary Discourse (Naderi and Hirst)*
   * approach:
      * SVM with (similarity of) sum of word embeddings, stance, POS tags, and type dependencies as features
   * dataset:
      * ComArg corpus by Boltuzic and Snajder as training set
      * speeches from Canadian parliament as test set
      * use same aspect classes as ComArg
   * results:
      * word2vec works better than other embeddings (syntactic embeddings & skip-thought)
      * stance information helpful (in the setting where aspects are stance specific)
      * sum of word2vec vectors best feature set

*From Arguments to Key Points: Towards Automatic Argument Summarization*
   * dataset:
      * 24,000 argument - key point pairs
      * 6,515 arguments for 28 topics (~230 arguments per topic)
      * 8.67 key points per topic (up to 7 key points per topic and stance)
      * remove arguments with low likelihood of it being an argument and of the stance being correct
      * professional debater provided key points
      * all matching key points assigned to each argument
      * 67.5% of arguments assigned to one key point (5% assigned to multiple)
   * approach:
      1 score how well each pair matches. 4 approaches:
         * cosine similarity of TF-IDF
         * cosine similarity of word embeddings (GloVe and BERT)
         * BERT trained on part of dataset
         * BERT trained on bigger natural language inference dataset
      2 choose key point(s) for each argument based on the scores. 4 approaches:
         * choose all key points over certain threshold
         * choose best scored key point for each argument
         * choose best scored key point if the score exceeds a certain threshold
         * choose two best arguments if one exceeds some higher threshold and other exceeds lower threshold; else like before
   * results
      * BERT-large trained on fold of dataset works best
      * pretraining on bigger natural language inference dataset does not improve results

---++++ Argument Clustering

*Summarising the points made in online political debates (Egan et al.)*
   * approach:
      * use POS tagging and FrameNet frames: same words in same roles -> same cluster
      * choose best sentence for each cluster based on length and bigrams
      * choose sentences from biggest clusters
      * try to present each sentence with a counterargument by looking for same frame with opposing word
      * group sentences together if their arguments often appear in the same posts
      * add some questions in the end
      * for the "layout" approach: add section headings based on which rule was applied to choose a certain sentence
   * dataset: internet argument corpus
   * experiments:
      * baseline: state of the art extractive summarization system
      * have turkers choose which of two summaries is better
      * this approach significantly better than baseline
      * even better when section headings are used

---++++ Short Text Clustering

*Self-Taught Convolutional Neural Networks for Short Text Clustering (Xu et al.)*
   * approach:
      * use unsupervised dimensionality reduction on bag-of-words
      * binarize the output
      * fit cnn with word embeddings as input to these binary vectors
      * apply k-means
   * 3 datasets:
      * web search results for different search phrases
      * questions from stack overflow with some chosen tags as clusters
      * paper titles with clusters as categorized in database
   * results:
      * cnn performs best on 2 datasets
      * same system using rnn with gru instead of cnn works better on stack overflow dataset

*A Dirichlet Multinomial Mixture Model-based Approach for Short Text Clustering (Yin and Wang)*
   * approach:
      * randomly assign all documents to clusters
      * reassign each document to its best cluster (the most similar but prioritize larger clusters)
      * repeat this step about 5 times
   * datasets:
      * google news headlines and/or snippets with clusters based on what google considers to be on the same topic
      * tweets corresponding to different search queries with each query being one cluster; manually annotated and only using tweets annotated as highly relevant for the query
   * results:
      * works better than k-means, agglomerative clustering, and some probabilistic model

*Comparative Study of Clustering Techniques for Short Text Documents (by Rangrej et al.)*
   * compare k-means, singular value decomposition, and graph based method using affinity propagation
   * compare cosine similarity and Jaccard similarity for k-means and affinity propagation
   * dataset: handpicked tweets from different search queries
   * evaluation function (cluster error) = (number of pairs clustered differently in gold clustering vs. performed clustering) / (number of pairs)

---++++ Framing

*Identifying Media Frames and Frame Dynamics Within and Across Policy Issues (Boydstun et al.)*
   * policy frames cookbook:
      * meant to be applied to opinions on political policy issues
      * list of 14 frames, each with a short desription
      * designed to be applicable to any policy issue -> very generic; additional topic-specific frames might be useful



---+!! Bachelor Thesis

*Supervisors:*
   * Daniil Sorokin
   * Maxime Peyrard
   * Teresa Botschen

---++ Meetings:

---+++ 25.09.2018:
Final Talk is schedueled for 11.12.2018 <br>
*TODOs:* <br>
   * prepare slides until the end of the week and send them
   * run Rouge on Frames

---+++ 11.09.2018:
*TODOs:* <br>
   * create a ranking of Rouge scores and human scores to find those which are overestimated/underestimated most
   * run Rouge-1 and Rouge-2 on only the entities
   * replace words with entities and run Rouge on this (might be hard to do, in that case won't be done)

---+++ 14.08.2018:
   * Entity Linker had some issue
   * Entity Linking will probably need several days -> run on server
   * TODOs from last meeting are still open
   * mid-term talk can be held on 02.10.
   * task description has been signed

---+++ 24.07.2018:
*TODOs:* <br>
   * try other baselines (ROUGE-2, ROUGE-WE, ...)
   * run entity linker on data
   * find some examples where ROUGE fails


---+++ 12.07.2018:
*TODOs:* <br>
Create Pipeline:
   1 Load data
   2 Rouge as baseline
   3 Compute 2 different correlations and plot results

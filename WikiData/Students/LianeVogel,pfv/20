%META:TOPICINFO{author="schiller" comment="" date="1551444834" format="1.1" reprev="16" version="20"}%
%META:TOPICPARENT{name="StudentsList"}%
---+++ 01.03.2019
*Current state*
   * Draft for thesis structure done
   * Knowledge retrieval for NELL and WordNet running
   * Modify EL that it uses the DBPedia spotlight results for DBPedia directly

*Open questions/tasks*
   * Retrieve knowledge from Wikidata, NELL, WordNet for IBM and UKP corpus
   * Start with Knowledge-BiLSTM experiments as soon as the knowledge is retrieved 
   * Send a draft for the framework chapter until next friday
   * Long-term: Merge project with error_analysis branch that includes methods for evalutaion
   * BACKLOG: Either use Virtuoso's CSV Bulkloading as discussed for ConceptNet import or convert the data into RDF format -> if it takes more than 1-2h, we drop ConceptNet 

---+++ 22.02.2019
*Current state*
   * EL ignores duplicate mentions
   * EL integrated into knowledge repo (untested)
   * Wikidata truthy import done
   * Draft for Introduction and Related Work done

*Open questions/tasks*
   * Structure for thesis until next meeting
   * Retrieve knowledge for UKP corpus from NELL and Wikidata
   * BACKLOG: Either use Virtuoso's CSV Bulkloading as discussed for ConceptNet import or convert the data into RDF format -> if it takes more than 1-2h, we drop ConceptNet
   * BACKLOG (only if it is no effort): Modify EL that it uses the DBPedia spotlight results for DBPedia directly

---+++ 13.02.2019
*Current state*
   * Wikidata truthy import running
   * Functionality to limit entities returned by the EL

*Open questions/tasks*
   * Integrate EL into the experiment project at https://git.ukp.informatik.tu-darmstadt.de/schiller/wikidata-access (fork/new branch from master branch)
   * Draft for Introduction and Related Work until 15th of February
   * BACKLOG: Either use Virtuoso's CSV Bulkloading as discussed for ConceptNet import or convert the data into RDF format -> if it takes more than 1-2h, we drop ConceptNet
   * BACKLOG (only if it is no effort): Modify EL that it uses the DBPedia spotlight results for DBPedia directly

---+++ 07.02.2019
*Current state*
   * EL adapted for all KBs currently integrated (WordNet, NELL, DBPedia, Wikidata)
   * Prepared EL-results for the topics look well enough to use

*Open questions/tasks*
   * Modify EL so that it returns a limited number of entities for a sentence (check for duplicate mentions first)
   * Integrate EL into the experiment project at https://git.ukp.informatik.tu-darmstadt.de/schiller/wikidata-access (fork/new branch from master branch)
   * Draft for Introduction and Related Work until 14th of February
   * BACKLOG: Either use Virtuoso's CSV Bulkloading as discussed for ConceptNet import or convert the data into RDF format -> if it takes more than 1-2h, we drop ConceptNet
   * BACKLOG (only if it is no effort): Modify EL that it uses the DBPedia spotlight results for DBPedia directly
   * BACKLOG (only if it is no effort): Import wikidata truthy

---+++ 01.02.2019
*Current state*
   * NELL is also imported now
   * EL improved with uses lemmatization step (check whether spacy has a better lemmatization than nltk)

*Open questions/tasks*
   * Either use Virtuoso's CSV Bulkloading as discussed for ConceptNet import or convert the data into RDF format -> if it takes more than 1-2h, we drop ConceptNet
   * Adapt EL for WordNet & NELL
   * Prepare EL-results for some sentences of the UKP Sentential ArgMin corpus (a few sentences from 3-4 topics) for a quality check
   * Draft for Introduction and Related Work until 14th of February

---+++ 25.01.2019
*Current state*
   * ConceptNet is not in the expected JSON-LD format, but in a "undefined" format
   * RDF version of NELL available and downloaded
   * Modified (and python ported) Inception entity linker can now use DBPedia Spotlight's annotation function as an entity-spotting module
      * Works for Wikidata and DBPedia
   * Short discussion regarding the presentation draft

*Open questions/tasks*
   * Either use Virtuoso's CSV Bulkloading as discussed for ConceptNet import or convert the data into RDF format
   * Import NELL
   * Adapt EL for WordNet (ConceptNet, NELL, when available)
   * Prepare EL-results for some sentences of the UKP Sentential ArgMin corpus (a few sentences from 3-4 topics) for a quality check
   * Create a draft for the thesis structure 


---+++ 18.01.2019
*Current state*
   * DBPedia spotlight seems to use the correct endpoint
      * Models are available for german and english and can be switched anytime (currently english version is running on the server)
      * In its current state the tool can only be used for DBPedia
   * BabelNet is only available in Lucene format -> import later
   * First version of the Inception EL is returning results for our wikidata endpoint (python re-implementation)

*Open questions/tasks*
   * Import ConceptNet
   * Import NELL
   * Inception EL:
      * Use "annotate" function of DBPedia spotlight instead of "spot"?
      * Use score returned by DBPedia Spotlight?
      * Try to speed up EL process
      * Test on UKP Sentential ArgMin dataset
      * Expand EL for other KBs of the framework
   * Create a draft for the thesis structure 
   * Create a draft for the presentation


---+++ 04.01.2019
*Current state*
   * Wikidata, DBPedia, and WordNet imported
   * DBPedia Spotlight is running and returns results

*Open questions/tasks*
   * Import BabelNet
   * Reimport previous english DBPedia graph
   * Check if DBPedia Spotlight uses the correct endpoint (e.g. setting the graph to WordNet, which should result in different results or an error)
   * Ask the DBPedia Spotlight developer if the models can handle different knowledge bases and, if not, whether new models can be trained
   * If DBPedia Spotlight is no option for other knowledge bases, use the [[https://github.com/inception-project/inception/blob/e29340571b96493d800f2425561a52b5ac47ed99/inception-concept-linking/src/main/java/de/tudarmstadt/ukp/inception/conceptlinking/service/ConceptLinkingServiceImpl.java][Inception entity linker]] build for [[https://wiki.ukp.informatik.tu-darmstadt.de/pub/UKP/Teaching/FinalThesis/20180530_REC_ConceptLinking.pdf][this]] thesis (either in its Java implementation, or implement via python)
   * Start with related work (see 07.12.2018)
   * Create a draft for the thesis structure 
   * Create a draft for the presentation

---+++ 21.12.2018
*Current state*
   * Importing geodata from other planets than earth caused problems in virtuoso for wikidata (https://community.openlinksw.com/t/non-terrestrial-geo-literals/359) -> custom patch fixed the problem
      * Since virtuoso had to be compiled again, DBPedia needs to be imported again, too
      * Wikidata almost completely imported
   * Docker DB Spotlight just loads the JAR and can be disregarded
      * JAR cannot be just modified to query our virtuoso server

*Open questions/tasks*
   * DBPedia Spotlight: 
      * Maven repo seems to be the best option -> set up and test
      * Perhaps send an email to the developers and ask directly for support
      * Models are only available for DBPedia -> models will probably not work for other KBs, i.e. training of new models has to be checked out
         * Alternative: Use same basic EL as Inception project does
   * Start with related work (see 07.12.2018)
   * While working on above tasks, import all KBs that are in RDF format and need no preprocessing (e.g. BabelNet)

---+++ 14.12.2018
No meeting

---+++ 07.12.2018
*Current state*
   * English and german DBPedia are imported
   * Fix in wikidata toolkit allows to use it for JSON->RDF conversion, will probably be ready for integration on Monday
   * DBPedia Spotlight is available as Java project, Docker, and Jar-file, although some files for execution might be missing

*Open questions/tasks*
   * Manage to get DBPedia Spotlight as Java-project or Jar-file running and use REST-Api to send queries
      * Write an email to the developers directly and ask for the best way to go in our setting with a self-hosted virtuoso dbpedia instance
   * Start writing the related work part (most importantly 1. & 2. for now)
      * Should be split into 
         1 "Knowledge Bases" (e.g. [[https://pdfs.semanticscholar.org/d1c8/993db306408254baeedf66d85df8f4cb8b91.pdf][FÃ¤rber et al. (2015)]], [[https://www.sciencedirect.com/science/article/pii/S1570826816000020][Ristoski and Paulheim (2016)]](?))
         2 "Entity Linking" (e.g. [[http://jodaiber.de/doc/entity.pdf][Daiber et al. (2013)]], [[http://videolectures.net/site/normal_dl/tag=1154610/sikdd2017_brank_wikipedia_concepts_01.pdf][Brank et al. (2017)]])
         3 (perhaps Argument mining)
         4 Work that integrates external knowledge (e.g. [[https://www.cs.cmu.edu/%7Ebishan/papers/kblstm_acl2017.pdf][Yang & Mitchell (2017)]], [[https://arxiv.org/abs/1805.07819][Kumar et al. (2018)]], [[https://arxiv.org/abs/1709.05453][Young et al. (2017)]], [[https://arxiv.org/abs/1606.00979][Zhang et al. (2016)]], [[https://arxiv.org/abs/1802.05930][Annervaz et al. (2018)]], [[http://www.aclweb.org/anthology/C18-1279][Deng et al. (2018)]], [[https://arxiv.org/abs/1705.02908][Song and Roth (2017)]](?), [[http://www.aclweb.org/anthology/W18-5211][Botschen et al. (2018)]])
   * Other Knowledge bases will be integrated as soon as the entity linking is working

---+++ 30.11.2018
*Current state*
   * Using Wikidata toolkit for JSON->RDF conversion failed as it is outdated and doesn't recognize newer classes of properties (e.g. Forms)
   * Virtuoso is installed and running on the VMServer
   * DBPedia english and german versions are currently imported and will finish soon

*Open questions/tasks*
   * Wikidata cleanup: Two possible options left:
      * Convert JSON to JSON-LD with e.g. LinkedPipes (https://etl.linkedpipes.com/) and convert from JSON-LD to RDF with Virtuoso Middleware Sponger (http://vos.openlinksw.com/owiki/wiki/VOS/VirtSponger)
      * Clean up the RDF file by removing all lanuage tags except for english (en-us, en-ca, ...) and german (de) and urls with these language tags
   * Convert ConceptNet with Sponger into RDF and import into Virtuoso    
   * Import NELL, WordNet, BabelNet, Wikidata into Virtuoso
   * Try to compile DBPedia Spotlight java code and adapt URL to our Virtuoso SPARQL endpoint for testing

---+++ 22.11.2018
*Current state*
   * Script that iterates over the Wikidata JSON dump and removes all languages except for en/de and blacklist properties
   * Entity Linker THD is not an option, since it's just usable as a plugin to the GATE Framework
   * VMServer is available
   * Cryptic nodeIds RDF triples of KBs are "blank nodes" (https://en.wikipedia.org/wiki/Blank_node)

*Open questions/tasks*
   * Clean-up Wikidata as discussed in the last meeting, convert to RDF with Wikidata Toolkit and integrate in Virtuoso
   * For DBPedia, take en-core and de folders with triples and integrate into virtuoso (as separate graphs), follow guide at https://joernhees.de/blog/2014/11/10/setting-up-a-local-dbpedia-2014-mirror-with-virtuoso-7-1-0/
   * Try to compile DBPedia Spotlight java code and adapt URL to our Virtuoso SPARQL endpoint for testing

---+++ 16.11.2018
*Current state*
   * Compiled Wikidata toolkit doesn't filter the dump, runs into an exception at some point -> tool is still maintained and there might be an update soon, according to one of the developers
   * Entity Linker FEL is not maintained anymore, FOX only recognizes very few categories for entities
   * Application for Bachelor thesis discussed (https://www.informatik.tu-darmstadt.de/studium_fb20/im_studium/studienbuero/abschlussarbeiten_fb20/index.de.jsp)
   * DBPedia cleanup: Files are already split by languages, only need to be downloaded and imported as soon as the VMServer is up

*Open questions/tasks*
   * Wikidata cleanup: 
      * write a script that iterates over wikidata json dump to delete all languages except for en/de and all blacklist properties, then use wikidata toolkit to generate the RDF from it (or any other tool)
      * Do the nodeIds in the Wikidata Toolkit RDF output represent/replace the Wikidata statements? 
   * Add DBPedia instance to Virtuoso as soon as VMServer is available
   * Set up DBPedia Spotlight as soon as VMServer is available
   * Check out if THD Entity Linker has full source code available (https://ner.vse.cz/thd/)

---+++ 09.11.2018
*Current state*
   * Wikidata toolkit doesn't work properly with latest wikidata dump (always results in deleting most of the dump, leaving only a handful of triples)
   * Inference in Virtuoso is activated, there are some predefined inference rules and new ones can be defined
   * Setting up Virtuoso will be delayed until next week, when the VMServer is hopefully available
   * Most Entity Linkers so far do not allow to switch Knowledge Base and/or use SPAQRL endpoint for spotting entites
      * Most promising ones are [[https://github.com/dice-group/fox][FOX]], [[https://github.com/yahoo/FEL][FEL]], and [[https://github.com/dbpedia-spotlight/dbpedia-spotlight-model][DBPedia Spotlight]]
   * Deleting graphs from Virtuoso is possible if the correct URI was used when the knowledge base was imported

*Open questions/tasks*
   * Wikidata cleanup: instead of the jar-file of Wikidata Toolkit, use the code and build the tool
   * DBPedia cleanup: Follow the guide and cleanup DBPedia as well: language https://joernhees.de/blog/2014/11/10/setting-up-a-local-dbpedia-2014-mirror-with-virtuoso-7-1-0/
   * Download FOX and FEL, build, and check out results. Is it possible to make these EL work with other KBs?
   * Options to make EL work on several other KBs:
      * Take returned entities and build URIs to other KBs from them
      * Make use of sameAs-relations to get concepts of other KBs

*Backlog*
   * DBPedia Spotlight EL will be tested as soon as the VMServer is available and Virtuoso has a running DBPedia instance, as this is a prerequisite for the EL
   * WordNet RDF dump has a weird format and might require some preprocessing or proper SPARQL queries to give results as can be seen n the online wordnet search (finding synsets of a lemma, their hypernyms and other relations)

---+++ 02.11.2018
*Current state*
   * Wikidata cleanup tool doesn't seem to work properly -> in the worst case, we import the full Wikidata dump
   * Search for an Entity Linker is still in progress
   * Wikidata QIDs are not supported/consistend by all KBs and cannot be relied on (perhaps multiple Entity Linkers are needed)

*Open questions/tasks*
   * Wikidata cleanup: 
      * Most important is the filtering of the languages and blacklist properties (perhaps the multitude of blacklist properties is a problem)
      * Suggestions: 
         * test without blacklist properties
         * test without config-file
         * use only a small part of the Wikidata-dump (head -n1000 wikidata-dump.nt > wikidata-dump-fule.part.nt)
   * If VM Server is not available until Monday, we can set up Virtuoso w/ SPARQL Endpoint on Lisa with just Wikidata for now
   * For DBPedia, we should also only import english and german language https://joernhees.de/blog/2014/11/10/setting-up-a-local-dbpedia-2014-mirror-with-virtuoso-7-1-0/
   * We need to find an Entity Linker architecture + code that uses (or can be modified to use) a SPARQL Endpoint full text search
   * Activate inference setting for Virtuoso (are there other settings described in the master thesis by Daniil's former student?)
   * Is it possible to delete an imported RDF-graph from Virtuoso (how)?

---+++ 26.10.2018
*Current state*
   * Virtuoso running on local machine with SPARQL endpoint and [[https://wordnet-rdf.princeton.edu/about][WordNet 3.1]]
   * Full text search is working
   * Multiple knowledge graphs can be imported and simultaneously queried by defining Graph IRIs for each
   * Waiting for VMServer to be deployed

*Open questions/tasks*
   * Since Wikidata is quite large, we need to reduce its size for performant queries:
      * Tool to filter languages and properties, etc.: https://www.mediawiki.org/wiki/Wikidata_Toolkit/Client
      * Config-datei anlegen:
         * Only german and english
         * Only use properties that are not on the [[%ATTACHURLPATH%/property_blacklist.txt][blacklist]] (remove from [[https://github.com/maxlath/wikidata-cli/blob/master/docs/read_operations.md#wd-props][list of all properties]])
   * Investigate Entity Linkers until VMServer is available:
      * See agenda from 12.10.2018
      * Check which KBs support Wikipedia/Wikidata QIDs (on which the Entity Linker will be trained on)
      * Do we need separate Entity Linkers for KBs that do not support QIDs?


---+++ 19.10.2018
*Current state*
   * Virtuoso running on local machine with SPARQL endpoint and WordNet 3.1

*Open questions/tasks*
   * Is it possible to use other query languages (SQL, CIPHER, etc.)?
   * How to enable full text search? (https://github.com/dotnetrdf/dotnetrdf/wiki/UserGuide-Full-Text-Querying-With-SPARQL)
   * How is it possible to use multiple knowledge bases in one Virtuoso endpoint?
   * Provide detailed documentation for installation and configuration

---+++ 12.10.2018
*Organizational*
   * Sign contract, intellectual proprty statement, task descriptions
   * Mandatory meetings on Tuesdays at 11:30 in B002
   * Thesis template und Presentation template: https://wiki.ukp.informatik.tu-darmstadt.de/Students/InformationenFuerDiplomanden

*Proposed/Discussed next steps (Overview)*
   1 Work into Virtuoso (document all steps)
      2 Build a minimal working sample by  installing Virtuoso, adding e.g. WordNet (specs for virtuoso KB server: https://docs.google.com/document/d/17gru5AApGwa-F7_4gZ_adMjlmBUYviD6oynSgiz0gMM/edit)
      3 Add a SPARQL endpoint for WordNet/Virtuoso (check functionality by comparing it to official WordNet SPAQRL endpoint)
      4 Find out how to add multiple KBs with their own endpoints (is a combined endpoint possible?)
   2 Entity linker
      3 Find out which entity linkers can be used for the proposed KBs and how they work (is a SPAQRL endpoint sufficient?), e.g. from:
         *     Unsupervised Entity Linking with Abstract Meaning Representation, ACL16 
         *     CoType: Joint Typing of Entities and Relations with Knowledge Bases, WWW17 
         *     Fast and Space-Efficient Entity Linking in Queries, WSDM15 
         *     Tagme: on-the-fly annotation of short text fragments (by Wikipedia entities), CIKM10 
         *     Local and Global Algorithms for Disambiguation to Wikipedia (Wikifier), ACL11 
         *     DBpedia Spotlight, iSemantics13 
      4 Test entity linkers with Virtuoso endpoint







---+++ 21.09.2018
   * discuss proposal for task description

-- Main.BenjaminSchiller - 2018-10-12

%META:FILEATTACHMENT{name="property_blacklist.txt" attachment="property_blacklist.txt" attr="" comment="" date="1540557214" size="6370" user="schiller" version="1"}%

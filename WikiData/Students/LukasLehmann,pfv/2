%META:TOPICINFO{author="lehmann" comment="" date="1555927876" format="1.1" reprev="1" version="2"}%
%META:TOPICPARENT{name="StudentsList"}%
---++ Informationen

   * Student: Lukas Lehmann
   * Betreuer: Johannes Daxenberger
   * Typ: Bachelorthesis
   * Anmeldung: 11.04.2019
   * Deadline: ...
   * Thema: Interpretable Neural Argument Mining
   * Prüfungsordnung: 2015

---++ Zusammenfassung der Meetings

---+++ Meeting vom 11.04.2019

   * Besprechung der bisherigen Ansätze zu interpretierbaren Modellen
   * Diskussion der Möglichkeiten bzgl. intrinsischer Evalutaion -> weitere Ansätze zu recherchieren
   * mögliche Verwendung eines IOB-getaggten Corpus diskutiert
   * Nutzung & Kenntlichmachung von externem Code
   * Struktur und Inhalte der Thesis (v.a. Zielpublikum)
   * Zugang zu BiCLSTM soll bald erfolgen
   * Zugangsdaten zu Wiki erhalten
   * offizielle Anmeldung für das Studienbüro übergeben
  
---++ Ideen

---+++ State-of-the-art-Modell (BiCLSTM)

   * Integration von Topic-Information in i- und c-Gate der LSTM-Zelle
   * bidirektionales Modell
   * Klassifikation basierend auf h_n_forward und h_1_backward
   * https://aclweb.org/anthology/D18-1402

---+++ Modell-unabhängige Ansätze (auf BiCLSTM angewendet)

---++++ LIME

   * lokales, lineares Surrogatmodell
   * wird auf gesampleten Daten rund um zu erklärende Instanz trainiert (beim Samplen könnte man hier wie bei Anchors vorgehen)
   * Daten werden bzgl. ihrer Distanz zu Originalinstanz gewichtet
   * Gewichte des trainierten linearen Modells als Importance Weighting über die Eingabedimensionen
   * für jedes Wort werden die Importance Weightings all seiner Dimensionen aufsummiert, um ein Importance Weighting für Wörter zu erhalten
   * https://arxiv.org/pdf/1602.04938.pdf (hier wird als Input Representation ein Bag-of-Words-Ansatz verwendet)

---++++ Anchors

   * für eine zu erklärende Instanz wird eine “Perturbation Distribution” erstellt (Idee ist, dass Eingabewörter durch Wörter mit gleichem POS-Tag ersetzt werden, ähnliche Wörter (nach Embedding-Distanz) sind wahrscheinlicher)
   * ein Anchor ist eine Teilmenge von Wörtern der zu erklärenden Instanz, sodass mindestens ein Anteil tau der Instanzen aus der Perturbation Distribution, die diese Teilmenge enthalten, gleich klassifiziert wird wie die zu erklärende Instanz
   * bei der Erstellung eines Anchors wird seine Coverage (Anteil der Instanzen der Perturbation Distribution, die den Anchor enthalten) maximiert
   * https://homes.cs.washington.edu/~marcotcr/aaai18.pdf

---++++ Layer-wise Relevance Propagation (LRP)

   * ausgehend von den Output-Neuronen (rückwärts durch das Netz) wird für jedes Neuron j ein Relevanzwert R_j berechnet
   * daraus ermittelt man dann R_d für alle Inputdimensionen d und Relevanzwerte für Wörter als Summe der Relevanzwerte ihrer Dimensionen
   * https://www.aclweb.org/anthology/W17-5221

---+++ Interpretierbare Modelle

---++++ BiLSTM mit SparseAttention

   * funktioniert grundsätzlich so wie “outer-att” (https://aclweb.org/anthology/D18-1402)
   * verwendet statt des klassischen Softmax-Attentionmechanismus einen Sparsemax (https://arxiv.org/pdf/1602.02068.pdf)
   * Topic-Information wird genauso im Attentionmechanismus integriert wie bei outer-att
   * Attention Weights werden dann als Importance Weighting über die Eingabewörter interpretiert
   * auch wenn Attention als Erklärung kritisch zu betrachten ist (https://arxiv.org/pdf/1902.10186v1.pdf), könnte SparseAttention hier gute Erklärungen liefern, da durch die Sparsity (einige Attention Weights sind 0) zumindest dargestellt werden kann, welche Wörter wenig Einfluss auf die Entscheidung haben

---++++ TreeLSTM mit SparseAttention

   * zunächst müssen Eingabesätze mit einem Constituency Parser in eine Baumstruktur umgewandelt werden, die dann binarisiert wird
   * es wird ein Binary TreeLSTM (https://www.aclweb.org/anthology/P15-1150) verwendet, wobei die Wörter eines Satzes nur in den jeweiligen Blättern einfließen (bei inneren Knoten sind die x-Werte 0)
   * es wird SparseAttention auf alle Hidden States (Blätter und innere Knoten) des TreeLSTM angewandt, um ein Importance Weighting zu erhalten (Topic Information wird auch wieder hier integriert)
   * die Idee ist, dass durch die zusätzliche Anwendung von Attention auf Teilsätze (innere Knoten) analysiert werden kann, welche Teilsätze wichtig für eine Entscheidung sind (nicht nur einzelne Wörter)

---++++ k-sparse Sentence Embeddings

   * Grundlage bildet (z.B.) das bekannte BiCLSTM-Modell
   * das finale Sentence-Embedding (im BiCLSTM-Fall z.B. conc(h_n_fw, h_1_bw)) wird dann durch ein k-sparse-Layer befördert, welches alle außer die k am meisten aktivierten Dimensionen auf 0 setzt
   * Backpropagation erfolgt dann nur durch diese k Dimensionen
   * nun könnte man für jede Dimension diejenigen Trainingsinstanzen heraussuchen, die diese besonders stark aktivieren
   * für eine einzelne Testinstanz, welche im Modell mit k-sparse-Layer in ein Sentence-Embedding mit k relevanten Dimensionen umgewandelt und klassifiziert wurde, wäre eine Erklärung dann durch diejenigen Trainingsinstanzen gegeben, die diese Dimensionen besonders stark aktivieren (die Annahme wäre, dass das ähnliche Sätze sind, wie die Testinstanz)
   * basierend auf: https://arxiv.org/pdf/1809.08621.pdf

---++++ Tensor Product Recurrent Network

   * TPRN funktioniert grundsätzlich ähnlich wie RNN, in jeder Iteration t wird ein Zustand v_t aus v_t-1 und dem Wort w_t berechnet
   * für jedes Wort wird mittels a_s und a_r (diese sind fast one-hot) ein Symbol aus einer Symbolmatrix S und eine Rolle aus einer Rollenmatrix R ausgewählt
   * um a_s und a_r als one-hot sicherzustellen, wird eine Quantisierungsfunktion gewichtet zur Loss-Funktion hinzuaddiert
   * da a_s und a_r nicht ganz one-hot sind, kann man die “Auswahl” über das Maximum von a_s bzw. a_r definieren oder über einen Threshold 
   * auf den v_t-States könnte man dann wieder wie für das BiLSTM einen Attention-Mechanismus (bzw. evtl. SparseAttention), um die Topic-Information zu integrieren und ein Importance Weighting zu erhalten
   * TPRN kann auch bidirektional verwendet werden, in dem Fall erhält man jeweils zwei Rollen & Symbole pro Wort
   * als Erklärung für die Entscheidung des Classifiers würde dann das Importance Weighting und die Rollen & Symbole (repräsentiert durch Vertreter dieser) für jedes Wort fungieren (also wenigstens etwas mehr Informationen als bei BiLSTM mit SparseAttention)
   * nach den Autoren von http://www.interpretable-ml.org/nips2017workshop/papers/07.pdf & https://arxiv.org/pdf/1705.08432.pdf kodieren Symbole häufig lexikalisch-semantischen Inhalt und die Rollen häufig grammatikalische Rollen, wie Subjekt/Objekt; man könnte hierzu analysieren, inwiefern das auch bei unserem Task zutrifft

---+++ Evaluation

---++++ Importance Weighting Based Sentence Vectors

   * Ansatz folgt https://arxiv.org/pdf/1612.07843.pdf
   * sollte für alle Erklärungsansätze funktionieren, die sich auf ein Importance Weighting über Einabewörter reduzieren lassen
   * für jede Instanz wird ein Sentence Vector als Summe der mit Importance Weighting gewichteten Word Embeddings berechnet und normiert (euklidische Norm)
   * wir erhalten ein Dataset aller Sentence Vectors, gelabelt mit der Prediction des originalen Modells für den jeweiligen Satz
   * K-nearest-neighbors Classification verwendet, um Sentence Vectors zu klassifizieren, eine hohe Accuracy soll ein gutes Importance Weighting implizieren ("Explanatory Power Index")
   * Anwendung auf Anchors: Interpretation eines Anchors als Importance Weighting mit Wert 1, wenn Wort in Anchor und 0 sonst
   * Anwendung auf TreeLSTM: Importance Weight für jedes Wort ist eigenes Attention Weight + 1/2 Attention Weight des Parent-Knoten + 1/4 Attention Weight des Parent-Knoten vom Parent-Knoten + ...

---++++ Schrittweises Entfernen von Wörtern

   * Ansatz folgt https://www.aclweb.org/anthology/W16-1601 bzw. https://www.aclweb.org/anthology/W17-5221
   * sollte für alle Erklärungsansätze funktionieren, die sich auf ein Importance Weighting über Einabewörter reduzieren lassen
   * für korrekt klassifizierte Testinstanzen werden nach und nach die Wörter mit den höchsten Importance Weights entfernt (Embeddings auf 0 gesetzt) und der Abfall in der Accuracy der Classification beobachtet, ein stärkerer Abfall könnte ein Indiz für ein "richtigeres" Importance Weighting sein
   * analog könnten für falsch klassifizierte Testinstanzen nach und nach Wörter mit den niedrigsten Importance Weights entfernt werden und evtl. ein Anstieg in der Accuracy beobachtet werden, ein stärkerer Anstieg könnte ein Indiz für ein "richtigeres" Importance Weighting sein (Vorsicht: Die Importance Weightings unterscheiden sich hier dadurch, dass z.B. Attention niedrigere Gewichte (nahe 0) i.d.R. unwichtigen Wörtern vergibt und z.B. LRP niedrigere Gewichte (negativ) i.d.R. opponierenden Wörtern)

---++++ Stabilität von Erklärungen bei Rauschen

   * man könnte den Inputvektoren minimales Rauschen hinzufügen, sodass die Prediction sich nicht ändert, und beobachten, inwiefern die Erklärung sich ändert
   * man könnte diese Änderung für vergleichbare Erklärungen (z.B. Importance Weightings) quantifizieren und vergleichen (für eine "gute" Erklärung sollte keine große Änderung auftreten)
   * fraglich ist, inwiefern es legitim ist, zufälliges Rauschen zu den Word Embeddings zu addieren, da diese dann keine echten Wörter mehr abbilden, oder ob stattdessen Wörter durch ähnliche Wörter im Embedding Space ersetzt werden müssen

%META:PREFERENCE{name="TOPICTITLE" title="TOPICTITLE" type="Local" value="Lukas Lehmann"}%

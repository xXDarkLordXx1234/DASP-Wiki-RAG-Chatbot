%META:TOPICINFO{author="diehl" comment="save topic" date="1465841737" format="1.1" reprev="23" version="24"}%
%META:TOPICPARENT{name="StudentsList"}%
---++ 2016-06-13
   * Status Update:
      * Short Term:
         * Extended Similarity Measurements
            * Currently uses WordNet-Path Based Similarity (Lexical features are optional and currently turned off)
            * Adding TF-IDF weighting to terms lead to a big improvement
            * Further experimented with Word Embeddings to build sentence vectors
               * is currently way too slow to apply for entire propositions
                  * Problem: If a lemma is not in the database it leads to very long searches
            * Subjects are filtered from the propositions since they always contain a match
            * Similarities are safed in a file
         * Slightly changed proposition extraction to discard non-alphanumeric tokens
   * Next Steps:
      * Mid Term:
         * Research Self Organizing Maps
         * Develop a better strategy for similarity measurement
            * Use multiple measures simultaneously
               * Problem: How to weight every measure (some are better than others)
            * Improve Word Embeddings
               * Improve performance
               * Add TF-IDF weighting
            * Maybe some sort of ML can help improve the results
      * Short Term:
         * Implement first version of clustering algorithm

---++ 2016-06-07
   * Status Update:
      * Short Term:
         * Implemented first version for measuring similarities
            * Currently only uses lexical features
               * Only works on obvious cases
               * Cannot detect negations
            * Experimented with Word Embeddings to build sentence vectors
               * Cannot detect negations
            * Subjects are filtered from the propositions since they always contain a match
         * Slightly changed proposition extraction to safe lemmas for now (might be changed again later)
            * Helps for proposition extraction
   * Next Steps:
      * Mid Term:
         * Research Self Organizing Maps
      * Short Term:
         * Further experiment and improve the similarity measures
         * Implement save file for similarity measures
         * Register Thesis

---++ 2016-06-01
   * @info(CS): Today the task force on "Realtime analysis and UI" will participate in our meeting to discuss some technical requirements.
   * @info(CS): Currently, the entire document is preprocessed with the entire preprocessing pipeline. This is very time consuming. Is it possible to first select only th sentences that include the subject and than run the preprocessing only on those sentence? This could speed up the system tremendously. 
   * Status Update:   
      * Midterm:
         * Created a short overview of different similarity measures ([[%ATTACHURLPATH%/similarity_summary.pdf][pdf]])
            * CS: Great. What about "semantic similarity measures" are those covered by "Corpus Based Similarity"?
            * maybe wordnet is an option?
      * Short-term:
         * Finished 1-Pager
         * Improved Proposition Extraction
            * If the sentence contains a negation, this is saved in the result file
            * "for" gets now added to the proposition if necessary
            * *Problem*: double propositions (sometimes the exact same proposition is two times contained in the text due to the HTML-Fetcher)
               * Maybe don't consider sentences, if they are identical to previous ones (but maybe this removes potential findings too...)
         * Improved proposition extraction for German
            * Added a bunch of rules (maybe too much, see [[%ATTACHURLPATH%/PropFile.txt][Todesstrafe]])
               * Currently I'm not able to test the proposition extraction since I lack the RAM to run the german parser consistently
                  * Simple solution --> ordered additional RAM, should arrive this week
                  * CS: Thank you, this is great! another solution could be to run the system on one of our compute servers. However, it is easier if you have it on your own machine.
               * *Problem*: It seems that I'm mainly extracting informal sentences, which is kind of the exact opposite of what we need
                  * Possible Reasons:
                     * The quality of the extracted german texts is significantly lower, than for english texts (way too much newspaper articles, teacher/school specific content, too little blogs or similar platforms)
                        * CS: I expected that... thanks for figuring this out!  
                     * The rules are not good/complete enough
                        * CS: The syntax of german is more complex, right?
                  * Possible Solution:
                     * Improve the extraction (but how?)
                     * Extract a set of relevant sentences from the files manually or build a set of basic sentences for the german languages and build rules around them
         * Improved sentiment analysis for English
            * Negates the sentiment if a negation is present in the proposition
            * Result files for abortion n=50 ([[%ATTACHURLPATH%/negative.txt][negative.txt]],[[%ATTACHURLPATH%/neutral.txt][neutral.txt]],[[%ATTACHURLPATH%/positive.txt][positive.txt]])
         * Implemented sentiment analysis for German
            * Works exactly like english one (still need to do more testing)
         * Expanded GUI
            * It is now possible via the GUI to force the program to rerun an experiment(deletes old experiment)
   * Next Steps:
      * Midterm: 
         * Expanding the GUI
      * Short-term:
         * Rework German proposition extraction
         * Test german proposition extraction and sentiment analysis
         * Test sentiment analysis with polarity detection for entire sentence and StanfordSentiment
         * Extract two seperate propositions if a conjunction is present (currently one is extracted)
         * Try out similarity measures

---++ 2016-05-25
   * Status Update:
      * Midterm:
         * Added big data dependency as a LDA disadvantage and an overly detailed reference slide ([[%ATTACHURLPATH%/clustering_summary.pdf][pdf]])
      * Short-term:
         * Improved Proposition Extraction
            * Restructured Proposition Extraction to remove redundancy in the code
            * Added POS to the output of the PropFile (This is necessary to use the Subjectivity Lexicons)
            * Added the document ID and the sentence begin to the output of the PropFile to make the sentence findable
            * Fixed problem with root
            * Example Files: [[%ATTACHURLPATH%/PropFile.txt][school uniforms]], [[%ATTACHURLPATH%/PropFile.txt][Abortion]], [[%ATTACHURLPATH%/PropFile.txt][Death Penalty (no root check)]]
         * Implemented simple proposition extraction for German
            * Added stemming to preprocessing to improve topic identification
            * Still only works for simple cases ([[%ATTACHURLPATH%/PropFile.txt][Todesstrafe]])
         * Implemented very simple sentiment analysis for English
            * Currently adds up occurences of positive and negative subjectivity clues and decides based on this information
         * Implemented GUI
   * Next Steps:
      * Midterm: 
         * Research Similarity measures for text clustering
      * Short-term:
         * Improve Proposition Extraction for German version
         * Adapt GUI to enable better testing
         * Improve Sentiment Analysis for English version
         * Implement Sentiment Analysis for German version
           
---++ 2016-05-18
   * Status Update:
      * Midterm:
         * Extended the overview of pro and cons for different clustering methods and added citations ([[%ATTACHURLPATH%/clustering_summary.pdf][pdf]])
      * Short-term:
         * Implemented simple Proposition extraction with various cases
            * Example result files with n=50, subject=abortion yielded 250 Propositions ([[%ATTACHURLPATH%/PropFile.txt][txt]])
            * Can still be extended by implementing additional rules
               * Still a minor or problem: DKPro implementation does not save the root of the sentence, d an alternative to detect main clauseuse
   * Next Steps:
      * Midterm: 
         * Research Similarity measures for text clustering
      * Short-term:
         * CS: Generate PropFiles.txt of other topics
         * CS: Is it possible to generate PropFiles in german? 
         * CS: Personal 1-pager until 1st June: https://docs.google.com/document/d/10at5K9vxuslrSt650uQ1ER7K5wkrf-ndLUT_lgvVHFs/edit#heading=h.ptjwmksp6e7v
         * Implement Proposition Extraction for German version
         * Implement Sentiment Analysis for English version
            * http://alt.qcri.org/semeval2016/task6/
         * Implement Prototype for GUI

---++ 2016-05-11
   * Status Update:
      * Midterm:
         * Defined first version of topic list ([[%ATTACHURLPATH%/topics.txt][txt]])
         * Created short overview of pro and cons for different clustering methods ([[%ATTACHURLPATH%/ProsCons.txt][txt]])
      * Short-term:
         * Faroo Web Search not usable
            * tried own implementation as well as already implemented JFreeWebSearch
            * No results for queries containing too many words
         * Extended boilerplate removal to add a dot if there is no punctuation before a linebreak
         * Slightly changed html retrieval
            *  htmls are directly downloaded via tika instead of saving them first
         * Implemented German version (Retrieval, Preprocessing)
         * Experimented with Proposition extraction
            * No good results with manual created rules ([[%ATTACHURLPATH%/PropFile.txt][txt]])
               * Hard to find relevant rules manually
               * Quality of results is too low
               * Amount of found results is to low
               * English implementation completely unusable for German version
            * Implemented target extraction based on rules by Somasundaran ([[%ATTACHURLPATH%/PropFile.txt][txt]])
               * Way too much noise (even after filtering sentences without subject) 
               * English implementation completely unusable for German version
      * Next Steps:
         * Midterm: 
            * Write Related Work - Text Clustering and Sentiment Analysis
            * Elaborate Thesis Structure ([[%ATTACHURLPATH%/structure.pdf][pdf]])
         * Short-term:
            * Find appropriate method for proposition extraction

---++ 2016-04-27
   * Scheduling of the tasks
   * Administrative tasks
      * todo(CS): Put thesis on ukp homepage (DONE)
      * todo(CS): Wiki entry (running thesis) (DONE)
      * Scheduled talks:
         * 2016-08-16 midterm presentation
         * 2016-10-11 final presentation
   * https://sites.google.com/site/iggsasharedtask/tools -> german sentiment-analysis tools and lexicons
   * Clustering with CLUTO package or similar existing tools ([[http://glaros.dtc.umn.edu/gkhome/views/cluto][link]])
   * Status Update:
      * Got a Faroo API Key
      * Implemented Pro-Con Classification based on polarity of Propositions through Subjectivity Clues
         * Currently it looks like most of the triples are neutral and are therefore discarded (Cloning, n=20 -> 13 positive, 14 negative, 108 neutral triples)
            * More rules for Proposition Extraction could help.
            * Coreference Solution could help
         * Only polarity of predicate is checked currently
      * Drafted first version of Thesis Structure ([[%ATTACHURLPATH%/structure.pdf][pdf]])
      * Tried java word2vec from file
         * Somehow i get different similarities when entering the same word combination twice
   * Next Steps:
      * Midterm:
         * Write Related Work - Text Clustering and Sentiment Analysis
         * Elaborate Thesis Structure
         * Define a reasonable query list / topics for demonstration
      * Short-term: 
         * Implement Faroo Web Search!
         * Improve proposition extraction and sentiment analysis (de/en)
         * Revise boilerplate removal (line break bug)
         * Implement German version (adapt preprocessing, etc.)
         * Add list of extracted proposition to the wiki
         * Implement similarity based on word2vec
         * Get to know CLUTO


---++ 2016-04-20
   * Task description ([[%ATTACHURLPATH%/TaskDescription_v03.pdf][pdf]] / [[%ATTACHURLPATH%/TaskDescription_v03.docx][doc]])
   * @info(CS): SVN access should work now.
      * @info(CD): SVN access works, code committed
      * <del>@todo: Send link to CS</del>
      * <del>@todo: include readme.txt that describes how to run the system</del>
   * Status Update:
      * Preprocessing implemented: Segmentation, Lemmatization, Parsing (No Sentiment Analysis is used)#
         * Currently only english; 
         * @todo: Should be adapted to german as well
      * Simple Proposition Extraction implemented
         * Extracts only propositions where the subject is the nominal subject (nsubj) and a direct object (dobj) is targeted
            * e.g. "uniforms improve attendance"
      * (Mainly) implemented general Architecture including the HashClass that is responsible for assigning folder names and detect duplicates
         * Classes in the pipeline can check if work is needed
      * Literature Research: Only researched argument generation and stance recognition
         * <del>@todo: draft thesis structure</del>
      * Found a German lexicon with polarity clues online [[http://www.ulliwaltinger.de/sentiment/][here]]
   * Next Steps:
      * Implement Pro-Con Classification based on polarity of Propositions through Subjectivity Clues
      * Research Clustering Literature (Especially "A survey of text clustering algorithms")
      * Implement naive clustering
   * @info(CS): Aspect Extraction with Automated Prior Knowledge Learning
      * http://www.aclweb.org/anthology/P14-1033
      * Might be relevant to the work. 
      * There is a lot related work in sentiment analysis (keywords: aspect extraction / target extraction).
         * http://de.slideshare.net/midorazaz/aspect-extraction-a-survey 
      * Some approaches also use clustering / topic modeling approaches (see related work of the paper above)
      * The evaluation is also interesting since it is based on user ratings. I could imagine a similar approach for your thesis.
   * @info(CS): Identifying Prominent Arguments in Online Debates Using Semantic Textual Similarity
      * http://www.aclweb.org/anthology/W15-0514
      * Includes some information about argument similarity

---++ 2016-04-13 
   * SVN for students?
      * Yes, but I have no access (https://scruffy.ukp.informatik.tu-darmstadt.de/svn/dkpro_students)
   * Status Update:
      * Java Web Search implemented 
         * Currently using Google Custom Search (100 queries per day)
         * Hint:
            * Cache results of (each) step(s) on file system (use Hash as key for storage (maybe include classname as well))
            * Use for each step a clear defined interface (in order to exchange the modules)
            * Parameters: number hits, language (de/en)
   * Next Steps:
      * Implement general architecture (define interfaces, data flow, caching, etc. and implement it)
      * Implement Proposition Extraction
      * Research Clustering Literature

---++ 2016-04-06 (open) Questions & Answers
   * Discuss current state of task description
   * What are arguments in the context of this thesis?
      * Actually, it is not clear if the extracted propositions are "arguments", they could be "argument components" with unknown type or just "propositions/statements" on the queried subject.
   * What are potential data sources?
      * Question in that context: How does the ideal data look like?
      1 Query Web search engine (e.g. faroo) per API or manually (potential violation of ToS) and save top n results
         * Maybe use more than 1 query to get results on a subject (e.g. 1 for pro, one for con)
      2 <del>Existing corpora with debate data (Probably difficult to find a fitting German corpus)</del>
   * What are possible methods for evaluating?
      * User Study:
         * Users evaluate result on subjects according to relevance and correct side (pro/con) via web interface
            * <del>Potential problem -> completeness of the collected information not guaranteed, how could this be evaluated?</del>
         * Use automatically extracted pro con lists for several subjects and ask participants e.g. if polarity of items is correct, if clusters are redundant, if items are meaningful, etc.
         * Evaluate results using IAA scores and conduct error analysis
         * evaluation strategy similar to Levy, Ran, Yonatan Bilu, Daniel Hershcovich, Ehud Aharoni, and Noam Slonim. 2014. Con-text dependent claim detection. In Proceedings of the 25th International Conference on Computational Linguistics (COLING 2014), pages 14891500, Dublin, Ireland.
   * Could document type/name and metadata help improve the Pro/Con Detection?
      * Most documents state in their title, if they argue for or against the subject or both
      * This could potentially be exploited in combination with html-metadata to easier identify Pro/Con statements
   * How to detect indirect references to the subject?
      * Research State of the Art, good solutions should be available
      * This task is called co-reference resolution 
   * Proposition extraction: http://u.cs.biu.ac.il/~stanovg/propextraction.html
   * Clustering
      * How important is domain knowledge or similarity measures?
      * We might need Semantic Similarity (DKPro Similarity)
      * Research clustering of short documents (single sentences / microblogs / twitter)

   
---++ 2016-03-30
   * Discussed Workflow of the thesis and signed contract(s)
   * Clarified expectations and requirements
   * Started elaborating the topic
      * Aggregation of arguments (what are arguments in the context of this thesis?)
      * Potential data sources (English or German or both)
      * "Search engine" that summarizes/aggregates argumentation relevant propositions structured as pro and con.
      * Evaluation?


-- Main.ChristianStab - 2016-03-30

%META:FILEATTACHMENT{name="TaskDescription_v03.pdf" attachment="TaskDescription_v03.pdf" attr="" comment="" date="1460979768" size="266300" user="stab" version="1"}%
%META:FILEATTACHMENT{name="TaskDescription_v03.docx" attachment="TaskDescription_v03.docx" attr="" comment="" date="1460979782" size="124683" user="stab" version="1"}%
%META:FILEATTACHMENT{name="structure.pdf" attachment="structure.pdf" attr="" comment="" date="1462729384" size="55558" user="diehl" version="3"}%
%META:FILEATTACHMENT{name="topics.txt" attachment="topics.txt" attr="" comment="" date="1462723171" size="279" user="diehl" version="1"}%
%META:FILEATTACHMENT{name="ProsCons.txt" attachment="ProsCons.txt" attr="" comment="" date="1462731729" size="603" user="diehl" version="2"}%
%META:FILEATTACHMENT{name="PropFile.txt" attachment="PropFile.txt" attr="" comment="" date="1464722066" size="83885" user="diehl" version="9"}%
%META:FILEATTACHMENT{name="clustering_summary.pdf" attachment="clustering_summary.pdf" attr="" comment="" date="1464121491" size="102012" user="diehl" version="2"}%
%META:FILEATTACHMENT{name="similarity_summary.pdf" attachment="similarity_summary.pdf" attr="" comment="" date="1464722873" size="96829" user="diehl" version="1"}%
%META:FILEATTACHMENT{name="negative.txt" attachment="negative.txt" attr="" comment="" date="1464723317" size="4202" user="diehl" version="2"}%
%META:FILEATTACHMENT{name="neutral.txt" attachment="neutral.txt" attr="" comment="" date="1464723343" size="10286" user="diehl" version="3"}%
%META:FILEATTACHMENT{name="positive.txt" attachment="positive.txt" attr="" comment="" date="1464723361" size="2605" user="diehl" version="3"}%

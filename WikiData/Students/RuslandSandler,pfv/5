%META:TOPICINFO{author="sandler" comment="" date="1591979411" format="1.1" version="5"}%
%META:TOPICPARENT{name="StudentsList"}%
-- Main.KevinStowe - 2020-02-06

12.06.2020:
   * worked on this week:
      * mainly reading documentation about the transformers library from HuggingFace and general theory about decoding
      * trying out different decoding strategies (beam search, top_k, top_p) and methods that improve the output (repetition_penalty, n_gram penalty, temperature)
   *  plans for next week:
      * continue working on baseline model
      * work on data procurement according to procedure explained on 29.05.2020
      * get familiar with ukp cluster (if necessary --- so far I was able to do computations on my notebook)


05.06.2020:
   * finished this week:
      * getting familiar with the ukp site
      * introduction of further functionality to baseline model
         * prediction of multiple sentences (amount can be chosen)
         * generation of general sequences (without being limited to a fixed sentence amount)
         * making use of "past" attribute to improve computation time (with help of new function: predict_sequence_past)
         * additition of top-k parameter to prediction functions (to choose randomly out of top-k predictions) to reduce repetition
   * plans for next week:
      * continue working on baseline model
      * work on data procurement according to procedure explained on 29.05.2020
      * get familiar with ukp cluster (if necessary --- so far I was able to do computations on my notebook)


29.05.2020:
   * finished this week:
      * finished reading all relevant resources
      * research in regards to data procurement
            * current idea is to scrap data from https://www.reddit.com/r/Jokes/ similar to the data procurement in the paper "Language Models are Unsupervised Multitask Learners"
            * difference is that we won't follow links, but take data directly from reddit (from the post's headline and content. comments may also include humor, but require too much filtering)
      * read PyTorch documentation and HuggingFace documentation
      * first commit in repository. includes
            * model class that will be extended by baseline model and the finetuned model later on.
            * class includes a predict method which can generate n candidates for the next word of a given text
      * set up riot client for communication
   * plans for next week:
      * continue working on baseline model
      * work on data procurement according to procedure explained above
      * get familiar with the ukp site and cluster

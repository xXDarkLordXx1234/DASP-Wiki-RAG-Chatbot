%META:TOPICINFO{author="sandler" comment="" date="1592581415" format="1.1" version="6"}%
%META:TOPICPARENT{name="StudentsList"}%
-- Main.KevinStowe - 2020-02-06

19.06.2020:
   * worked on this week:
      * implemented reddit scrapper to acquire data for fine-tuning model
      * wrote code to predict joke continuations for acquired data from reddit
         * the predictions are performed for different configurations for the generate() function of the transformers library from HuggingFace
         * the configurations are stored in a csv file
         * the predicted joke continuations are saved in a file alongside the prompt which was used to predict them and the "original" joke
   *  plans for next week:
      * maybe try out some more configurations for the baseline model, but work on it is mainly finished
      * work on finetuning the baseline model
      * write the thesis
      * get familiar with ukp cluster (if necessary --- so far I was able to do computations on my notebook)


12.06.2020:
   * worked on this week:
      * mainly reading documentation about the transformers library from HuggingFace and general theory about decoding
      * trying out different decoding strategies (beam search, top_k, top_p) and methods that improve the output (repetition_penalty, n_gram penalty, temperature)
   *  plans for next week:
      * continue working on baseline model
      * work on data procurement according to procedure explained on 29.05.2020
      * get familiar with ukp cluster (if necessary --- so far I was able to do computations on my notebook)


05.06.2020:
   * finished this week:
      * getting familiar with the ukp site
      * introduction of further functionality to baseline model
         * prediction of multiple sentences (amount can be chosen)
         * generation of general sequences (without being limited to a fixed sentence amount)
         * making use of "past" attribute to improve computation time (with help of new function: predict_sequence_past)
         * additition of top-k parameter to prediction functions (to choose randomly out of top-k predictions) to reduce repetition
   * plans for next week:
      * continue working on baseline model
      * work on data procurement according to procedure explained on 29.05.2020
      * get familiar with ukp cluster (if necessary --- so far I was able to do computations on my notebook)


29.05.2020:
   * finished this week:
      * finished reading all relevant resources
      * research in regards to data procurement
            * current idea is to scrap data from https://www.reddit.com/r/Jokes/ similar to the data procurement in the paper "Language Models are Unsupervised Multitask Learners"
            * difference is that we won't follow links, but take data directly from reddit (from the post's headline and content. comments may also include humor, but require too much filtering)
      * read PyTorch documentation and HuggingFace documentation
      * first commit in repository. includes
            * model class that will be extended by baseline model and the finetuned model later on.
            * class includes a predict method which can generate n candidates for the next word of a given text
      * set up riot client for communication
   * plans for next week:
      * continue working on baseline model
      * work on data procurement according to procedure explained above
      * get familiar with the ukp site and cluster

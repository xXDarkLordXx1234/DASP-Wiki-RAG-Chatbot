%META:TOPICINFO{author="youssef" comment="save topic" date="1513331432" format="1.1" reprev="26" version="26"}%
%META:TOPICPARENT{name="StudentsList"}%
---+!! Bachelor thesis: Impact of activation functions in deep learning for NLP

*Start date*: 01.11.2017

*Supervisor*: Dr. Steffen Eger


---++ Current Status
---+++ 13.12.2017
   * All Experiments are done, but i had to reduce the number of parameters for CNN and MLP of MR
   * I evaluated all the results
   * I worked on Theory & Background in the thesis. Im not sure if what i wrote is enough. I dont know what to add anymore...
---+++ 07.12.2017
   * POS-Tagging and Arg.Min experiments are finished
      * SE: cool! general remark: if you start new experiments, please do it not on apu, but rather desktop-titanx or other servers that you have access to
      * SE: Is MR finished? If not: do you think it's because of SGD? how many epochs do you have? If you: if it doesn't finish by next week, stop it, and run with adadelta or rmsprop instead of sgd 
   * I plotted the results of POS-Tagging and Arg.Min, and the accuracy results of TREC and SUBJ
      * Would be nice if you can share with me (e.g., make an attachment here)
   * The Experiments of Document Classification are taking too long. I may have to reduce the number of parameters again. 
      * SE: Yes, do so, if necessary.
  
---+++ 29.11.2017
   * experiments on newsgroup (document classification) are running
   * experiments on movie-reviews are still running 
   * experiments on SUBJ, TREC, and PE are done 
   * I added swish and penalized tanh to related work

---+++ 22.11.2017
   * experiments on movie-reviews dataset are not done yet 
   * Implementation of CNN for document classification is done (I added the parameter to the wiki)
   * I implemented swish and penalized tanh and added them to the experiments 
   * I'm still working on the theory & background section in the thesis


---+++ 15.11.2017
   * experiments on movie-reviews dataset are still running (maxout, leakyrelu, prelu are done)
   * I'm working on CNN and RNN for document classification and sequence tagging
   * I'm also working on the theory & background section in the thesis

---+++ 08.11.2017
   * Filled out application form [...]

---++ Meeting Minutes

---+++ 14.12.2017
   * Repeat experiments with 5 remaining activation functions from Swish paper:
      * MLP: just 1 layer, dropout 0.2,0.5; hidden units (4 different ones), optmizer (adam, rmsprop)
      * CNN: replace or remove adadelta
      * RNN: keep everything 
   * Replace English Univ. Dependency Data with German (https://github.com/UniversalDependencies/UD_German)
   * For ArgMin: Use ALL remaining documents for testing
   * Report same graphics, but wiht arhitmetic mean and harmonic mean (https://en.wikipedia.org/wiki/Generalized_mean)

---+++ 30.11.2017
   * Score each activation function. For example, as follows:
      * compute averages over all parameter settings. A parameter setting consists of: (dataset,hidden layers,dropout choice,...)
         * can compute different averages: harmonic mean, arithmetic mean, etc., see here: https://en.wikipedia.org/wiki/Generalized_mean
      * also compute standard deviation over the parameter settings for each activation function
      * then plot in 2-d space
   * RNN: take architecture from NR. Change activation function of LSTM unit (=recurrent unit); best is to tell me which and I give you feedback again  
      * For Arg.Min. report Macro-F1 on Component Level (label set is: {B-Claim,I-Claim,B-Premise,I-Premise,B-MajorClaim,I-MajorClaim,O}). For POS tagging, report accuracy.
      * In case of questions, ask me
   * How to display your data in your thesis: https://arxiv.org/pdf/1705.02364.pdf
   * Sample for a good related work section: see Andreas Schäffler
   * For phase 2: prune away bad "parameter settings"; apply some of the good the activation functions from the swish paper; but can still combine them via {max,+,*}.  

---+++ 23.11.2017
   * Email to MB because of desktop-titanx and also ask why second job gets cancelled immediately
   * Start to implement RNN (either Keras-based or Tensorflow-Based)
   * For Phase 2: Normalization either batch normalization or something else
   * Read the Swish paper and add it to related work (or theory). Also read what they said about beta 
   * For theory chapter: if uncertain, can also post question in stats.stackexchange or cs.stackexchange

---+++ 16.11.2017
   * Can send me list about parameters including datasets, so that we can possibly reduce the amount of experiments
      * possibly reduce one dataset
   * Tensorflow RNN is on the webpage for DL4NLP: HEX 11
   * Which parameters are you using for RNN? Please insert into Wiki
   * Come to some of the UKP meetings on Tuesday
   * We also talked about the plans for the 2nd phase (different activation functions per layer + combination of activation functions)

---+++ 09.11.2017

   * For CNN: play around with different filter sizes, potentially other hyperparameters
      * can also play around with more optimizers if time permits: AdaGrad, AdaDelta, RMSprop
   * For CNN: adaptation of code is completely ok, if you have good reasons for doing so
   * Can also prepare RNN for Sequence Tagging Task: which implementation do you want to use?
      * TK in Tensorflow
      * NR in Keras
   * I gave you a potential structure of the thesis (>40 pages including everything)
      * For analysis: std is bad, mean is good
   * I also gave you feedback on the related work section
   * Maxout: k=4 (Optional k=2)

-- Main.SteffenEger - 2017-11-09

---+!! Hyper-Parameters & Datasets

   * MLP & Sentence Classification: 
      * Datasets: movie-reviews (sent2vec) , TREC (InferSent), Subjectivity (InferSent), student-essays (PE) (sent2vec & InferSent)
         * Hyper-Parameters for movie-reviews: 
            * Optimizer: Adam, SGD
            * Functions :  'x^3', 'sin', 'sigmoid', 'tanh', 'relu', 'elu', 'selu', 'softplus', 'softsign', 'linear', 'leaky relu', 'prelu', 'maxout'
               * Update:  'x^3', 'sin', 'sigmoid', 'tanh', 'relu', 'elu', 'selu', 'linear', 'leaky relu(alpha =0.01)', 'prelu', 'maxout (k=3)', 'swish (beta=1)', penalized tanh
            * layers:
               * layers: 1,2,3 (with adam), 1,2 (with sgd)
            * Dropout:
               * 0.2, 0.5, 0.8 with 1 layer
               * 0.2, 0.5, 0.8 with 2 layers and adam 
               * 0.2, 0.5 with 2 layers and sgd
               * 0.2, 0.5 with 3 layers and adam 
            * Hidden units:
               * with 1-2 layers: 50, 100, 300, 600, 1200
               * wieh 3 layers:  300, 1200
         * Update: For TREC, SUBJ:
            * Optimizer: Adam
            * Layers 1,2
            * Dropout: 0.2
            * Hidden units: 50,100,300
         * For PE:
            * Optimizer: Adam
            * Layers: 1,2
            * Dropout: 0.2
            * Hidden units: 50,100,300

   * CNN:
      * Hyper-Parameters of CNN for document classification:
         * Optimizers = ['adam', 'adadelta']&#8232;
         * convolutional layers = [1]
         * Filters = [75, 100, 125, 150]
         * kernel_sizes = 3
         * Activation functions: see MLP
   * RNN:
      * POS-Tagging:
         * Dataset: UD
         * Functions: 'x^3', 'sin', 'sigmoid', 'tanh', 'relu', 'elu', 'selu', 'linear', 'leaky relu',  'swish', 'penalized tanh'
         * Layers : 1
         * Units: 75, 100, 150
         * network-type: LSTM, simple RNN
         * Word Embeddings: levy_deps.words, dependency based embeddings 
      * Argument-Mining:
         * Dataset: conll
            * 300 train sentences
            * 199 dev sentences
            * 449 test sentences
         * Functions: 'x^3', 'sin', 'sigmoid', 'tanh', 'relu', 'elu', 'selu', 'linear', 'leaky relu',  'swish', 'penalized tanh'
         * Layers : 1
         * LSTM-Size: 75, 100, 150
         * network-type: LSTM, simple RNN
         * Word Embeddings: levy_deps.words
-- Main.PaulYoussef - 2017-11-15



 

%META:FILEATTACHMENT{name="seq-tagging-results.zip" attachment="seq-tagging-results.zip" attr="" comment="" date="1512650791" path="seq-tagging-results.zip" size="231625" user="youssef" version="1"}%
%META:FILEATTACHMENT{name="eval-1.zip" attachment="eval-1.zip" attr="" comment="" date="1513005991" path="eval-1.zip" size="617141" user="youssef" version="1"}%
%META:FILEATTACHMENT{name="thesis-draft.pdf" attachment="thesis-draft.pdf" attr="" comment="" date="1513186650" path="thesis-draft.pdf" size="339052" user="youssef" version="1"}%

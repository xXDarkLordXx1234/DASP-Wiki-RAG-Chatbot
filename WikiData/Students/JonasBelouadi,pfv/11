%META:TOPICINFO{author="belouadi" comment="" date="1541950416" format="1.1" reprev="9" version="11"}%
%META:TOPICPARENT{name="StudentsList"}%
---++ General Information

*Supervisors*: 
   * Yevgeniy Puzikov, M.Sc.


*Start Date* : XX-OCT-2018 

*Mid-Term Presentation Date* : XX-APR-2019

*Final Presentation Date* : XX-APR-2019   

*End Date*: 01-MAY-2019


-- Main.YevgeniyPuzikov - 2018-10-22

---
---++ Week 5.11 - 11.11

   * did read [[https://arxiv.org/pdf/1810.04864.pdf][Sequence-to-Sequence Models for Data-to-Text Natural Language Generation: Word- vs. Character-based Processing and Output Diversity]]
      * as the name sugggests it compares different embedding techniques
         * word embeddings
         * character embeddings
            * seem to have the highest output diversity in comparison
         * subword-based embeddings
            * don't work so well
      * the evaluation techniques from this paper are also relevant for the thesis
         * BLEU and ROGUE-L
            * [[https://aclweb.org/anthology/Q/Q16/Q16-1029.pdf][Xu et al. (2016)]] report that BLEU strongly correlates with grammaticality and meaning preservation.
            * The [[https://github.com/tuetschek/e2e-metrics][E2E challenge evaluation script]] uses BLEU, ROGUE-L and a bunch of similar metrics. Could be useful
         * diversity measured by the number of unique sentences and words
         * Shannon text entropy for diversity
            * [[http://aclweb.org/anthology/W18-5019][Oraby et al. (2018b)]] introduce it as a mesaure for the amount of variations in the output
   * did read [[https://arxiv.org/pdf/1810.06640.pdf][Adversarial Text Generation Without Reinforcement Learning]]
      * they use an additional encoder-decoder model which produces sentence embeddings, which lie in a continuous space and can be used to train a GAN because of that. In our case we could use phrase embeddings to genrate descriptions but the results would most probably be inferior to seq2seq models.
         * [[https://arxiv.org/pdf/1811.02549.pdf][Caccia et al. (2018)]] confirm that assumption (though only for reinforcement learning GANs)
    
      
---+++!! scheduled for this week
   * look for evaluation methods
   * further look into GAN based methods
   * do thesis stuff
   * upload the thesis to gitlab

---
---++ Week 29.10 - 4.11
   * pushed the web scraper code to [[https://git.ukp.informatik.tu-darmstadt.de/UKP-Students/sigmaaldrich_scraper.git][gitlab]]
   * did a keras NLG tutorial to understand how that stuff actually works
   * started writing the thesis
   * did read [[https://www.aaai.org/ocs/index.php/AIIDE/AIIDE11/paper/viewFile/4066/4431][DEXTOR: Reduced Effort Authoring for Template-Based Natural Language Generation]]
      * the paper only describes methods to speed manual authoring up
         * *TODO*: look for automated template-based approaches
   * did read [[http://aclweb.org/anthology/W17-2629][Adversarial Generation of Natural Language]]
   * did read [[https://arxiv.org/pdf/1702.07983.pdf][Maximum-Likelihood Augmented Discrete Generative Adversarial Networks]]
      * GANs seem to generate data only from noise from a continuous space
         * text is discrete, to circumwent this reinforcement learning is used
            * hard to train
            * *TODO*: look for other methods ✅

---
---++ Week 22.10 - 28.10
   * did read [[https://arxiv.org/pdf/1409.0473.pdf][Neural Machine Translation by Learning to jointly Align And Translate]] properly
   * did read [[http://aclweb.org/anthology/N18-2010][Natural Language Generation by Hierarchical Decoding with Linguistic Patterns]] properly
      thee hierarchical approach seems to be an improvement on standard seq2seq approaches, but also requires an additional POS-tagger
   * *TODO*: read the remaining papers ✅

---
---++ Week 15.10 - 21.10
   * wrote a web scraper for some artclies on [[https://www.sigmaaldrich.com/life-science/biochemicals/biochemical-products.html?TablePage=16181993][www.sigmaaldrich.com]]
      * right now it only scrapes articles from one page
      * *TODO*: maybe extend the scraper to scrape more articles
      * *TODO*: strangely  the headless html query doesn't fetch all articles, look into that
      * *TODO*: push the code to gitlab ✅
   * searched for potential baselines and skipped through the papers
      * potential baselines
         * [[https://www.aaai.org/ocs/index.php/AIIDE/AIIDE11/paper/viewFile/4066/4431][DEXTOR: Reduced Effort Authoring for Template-Based Natural Language Generation]]
         * [[https://arxiv.org/pdf/1409.0473.pdf][Neural Machine Translation by Learning to jointly Align And Translate]]
      * *TODO*: read through them properly ✅
   * searched for the state of the art and skipped through the papers
      * potential papers
         * [[http://aclweb.org/anthology/W17-2629][Adversarial Generation of Natural Language]]
         * [[https://arxiv.org/pdf/1702.07983.pdf][Maximum-Likelihood Augmented Discrete Generative Adversarial Networks]]
         * [[http://aclweb.org/anthology/N18-2010][Natural Language Generation by Hierarchical Decoding with Linguistic Patterns]] 
      * *TODO*: read through them properly ✅
   * signed all that bureaucratic stuff

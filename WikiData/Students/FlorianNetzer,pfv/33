%META:TOPICINFO{author="netzer" comment="save topic" date="1513795187" format="1.1" reprev="31" version="33"}%
%META:TOPICPARENT{name="StudentsList"}%
---++++ *Bachelor Thesis* : Deep Learning for Citation Count Prediction


*Start date* :  05.10.2017

*Supervisor*: Dr Steffen Eger



---++++ *Current Status*
   * 20.11.2017
      * Experiment reruns:
         * slower than I hoped due to some unexpected crashes. But I try to run as much parallel as possible.
         * CNNs about 50% done.I guess it will take about 4-5 days until i have everything. Unlikely that i will have time for approaches  combined with metadata. Standard deviation for the results I have is down to 2-3% for modeling, around 4-5% for prediction.  
          
   * 13.11.2017
      * Using average pooling instead of max pooling and reducing the dimensions seems to give more stable results.There is still some variance but at least no entirely random results.
      * fasttext seems to work ok, but not as good as the other models including yogatama. I will try if optimizing the parameters improves it.  
   * 7.11.2017
      * rerun some of the cnn experiments:
         * results very unstable. It is difficult to determine when to stop training. There is a big variance between results on dev and test.
         * SE: that's the impression I also had
         * SE: how do you do it anyway? You should usually run for several epochs and use the model that performed best on the dev set, given your performance measure such as F1
            * That is what I do
               * SE: good
         * SE: what's the dev set in prediction? is it also from the future? 
            * The dev set is from the same years as the train set. This is definitely a reason why there is variance for the prediction task. But changing it would be against the setup of the task. 
               * You're right. So keep it as it is. This will always entail some additional variance.
         * SE: if there's a large discrepancy between dev and test, consider increasing dev
            * Worth a try. Might make the trainings set for the CL papers to small.
         * SE: in the final experiments, please report means and standard deviations
            * I am still trying to optimize the parameters. But when that is done I will use the remaining time to run the models multiple times. 
         * SE: how's fasttext going?
            *  Seems to work, but the metrics the fasttext tool uses are different to what I used. I will build something to transfer the results and use my own metrics. I need to use the command line program for fasttext because the python wrapper seems to be buggy. 
         * SE: general comment: apu is pretty busy at the moment. Can you partly move to other servers or is it because of data issues that you need to work on apu?
            * I could move the preprocessed data to another server. The problem with desktop-titanx is that it might have to little memory available for most of my experiments, but I will try out what works.
               * SE: Yes, just see what work. I'm also talking to other students to move, so you might be able to stay.
      * writing
         * working on model section, dataset section and related work. Nothing really ready for feedback yet.
   * 29.11.2017
      * fixed error in CNN +novelty
         * results(experimentsv6.pdf) still not as good as just CNN
      * metabased approaches
         * results(metabased_experiments.pdf) mixed but some are ok
   * 22.11.2017
      * novelty:
         * including LLR score into cnn did not work well.
         * changed mlp for novelty. l2 regularisation and different dimensions.
         * reduced size for novelty words by filtering to 7.5k/20k. results not as good as before but ok.
         * build a model for novelty which uses counts of binned LLR scores instead of word counts. Should represent the actual novelty idea better but the results are not as good.
      * started extracting meta-based features as in Luca Weihs. Already got most of the author features: h-index, pagerank, citations, papers ...
   * 15.11.2017
      *  included abstract into body. Improved some results a bit.
      *  readability + novelty 
         * implemented readability as a set of different typical standard readability scores
         * novelty as counts of unusual frequently used words per year found with log-likelihood ratio test
         * compared mlp and log.reg. on those features. results: 
            * readability scores are not really useful.
            * Novelty performed good. In some task close to the cnn or yogatama.
         * I build a new cnn which takes the ll-ratio score of a word as a additional input. results coming tomorrow. 
   * 8.11.2017
      * changed hep binning to four classes which are more evenly distributed. 
      * improved conclusion extraction. around 400 conclusions more for the CL dataset, 5000 more for HEP
      * Tried optimizing the models. No to big improvements
      * ran experiments on introduction and conclusion
         * results with f1 metric coming tomorrow morning. some results still missing 
   * 1.11.2017
      * accumulated older papers for the CL datasets prediction task. Improved results and lowered variance between dev and test results. new statistics + label distribution in datasetv2.pdf
      * Improved results(see attached experimentsv2.pdf) for CNN by changing the layers and dimensions. including author and venue input into the Dense layers at the end of the CNN model seems to improve results in some cases(no venue information for hep papers).  As good or even better results than yogatama baseline so far!:)
      * changed yogatama baseline to resemble better the model described in the paper. Test showed no big differences but I did not rerun all the Experiments yet. UPDATE: did reruns. results better for CL dataset. experimentsv2.pdf updated
      *  tried mlp on yogatama and vdcnn. no good results so far.
   * 25.10.2017
      * Created Statistics for the datasets(see attached dataset.pdf). Problem: train/dev split is to small for the CL dataset prediction task. This is because most papers are in 2016 and 2017. The smallest time span for predictions is citations within one year because there is no information about the citation month. Therefore the train/dev split has to be, to avoid overlapping with test split, in 2014, which is small. 
      * Evaluation of models on abstract and full text(see attached experiments.pdf). So far only the yogatama baseline has some good results. I noticed some difference in my implementation to the one described in the paper. I will change it and hopefully improve it further. VDCNN not tested yet but I made my own implementation which should work. I am also trying to use an MLP instead to logistic Regression on the yogatama features.   
   * 18.10.2017
      * Began evaluating different models. So far no interesting results. Models need to be optimized.
         * SE: Better than baseline? Do you tokenize your data? On which level do you work: abstract, intro, conclusion, full text?
            * yogatama(with author and venue info) on tokenized abstract   works ok for modelling task. about 6.5% better accuracy than baseline . baseline level for prediction. 
            * yogatama on full text better(modelling task), almost 10% improvement over baseline.
            * simple cnn at baseline
            * fasttext slightly over baseline
            * vdcnn not running yet :(

   * 17.10.2017:
      * program to create dataset on apu /home/netzer/semanticScholarDBcomplete:
         * run.sh: runs every step to create the full dataset. category can be changed in script(see comments). I did not test how long a full run takes.
         * finished CL dataset: semanticScholar_csCL_FULL.jsonl . Every line is a paper + metdata encoded as json
         * sample paper : example.txt
      * all arxiv papers: /home/netzer/arxiv
   * 11.10.2017:
      * Datensatz mit papern aus  high-energy physics ist fertig.
      * Ich habe angefangen das Modell von Yogatama zu implementieren. Bisher habe ich nur Ridge Regression verwendet. Im paper wir diese mit einem  Generalized Linear Model mit Time Series Regularization verglichen. F체r letzteres gibt es keine fertigen Implementierungen. Ich bin mir nicht sicher wie viel Aufwand das w채re.
      * Sonst bin ich dabei ein Framework zu bauen, mit dem ich die Experimente mit verschieden Evaluierungsszenarios,Datens채tzen und Modellen strukturiert durchf체hren kann. 
   

---++ Meeting Minutes

---++++ *Meeting 14.12.2017*
   * Add more papers, describe more details, if there are few papers in some area, say so
   * Explain the occurrence of words like "mesure" in the LLR statistics
   * \log, \tanh in Latex
   * Use computing power of our servers to finish outstanding experiments. Include average and std indicators
   * Next week, we see last time before your submission --> please give me something to read by next Tuesday at the latest
      * For hand-in of thesis, find out what you have to do (give to the Sekretariat and two versions to me, I think, plus TUBama)

---++++ *Meeting 30.11.2017*
   * Answer the following questions in the thesis:
      * are the results stable across averages? Some look random ...
      * how does fasttext perform? 
      * How do the models perform on introduction, conclusion, abstract?
      * What do you do with the title of the paper?
      * Optional (but very interesting): can you combine your text-based model with your meta-data features? 
      * How many features does Yogotama have? What happens when you choose only 1-grams? Only 1-grams and 2-grams?
      * What use does novelty have? Is it just feature reduction? How much faster is it than Yogotama?
      * Best is if you schedule your experiments appropriately:
         * make good/better use of the compute power that you have available (two machines, several cores on each)
      * How well does abstract perform (plus title)? 
         * how well abstract + meta-data?
         * Thought experiment: conclusion+meta-data?
      * Which authors have high pagerank? On which network is this computed? Co-authorship network?
      * Statistics: how many papers have venues, etc.?
      * Combined model: novelty + CNN on full-text
   * Start writing from tomorrow onwards. Do send me things to read and make suggestions on early on.
   

---++++ *Meeting 23.11.2017*
   * Implement meta-data based approaches
      * In best case, evaluate directly
   * Don't forget averaging and variance indications
   * Fill out remaining tables and experiments
   * Don't forget fasttext
   * Check why combininig CNN+Novelty didn't work well
      * You can also directly combine your original novelty framing (count+llr score for the most novel words) with the CNN (directly feeding into the softmax or with some dense layers in between)
   * Start Writing from December first: I will give you 2.5 feedbacks
      * Describe approaches well with e.g., figures, etc.
   * SE: I can send you a Master thesis
   * Make an outline of how you want to strucutre the thesis (cf. https://wiki.ukp.informatik.tu-darmstadt.de/bin/view/Students/SteffenKuchelmeister)
   * Co-authorship network Pagerank: can you prepare a list of high collaborative authors?

---++++ *Meeting 16.11.2017*
   * try to combine to LLR score representation with the CNN
   * try out other values than 10K for the most novel words
   * Final thesis: report averages rather than single-point estimates
   * Use regularizer with MLP (e.g. https://keras.io/regularizers/)
   * Possibly: other readability measures
   * Experiments with how to determine the 10K most novel words over the training data - concatenation over all the years? Make dimensionality smaller then? (because 10 * 10K is a large number)
   * Afterwards:
      * look at meta-data based approaches, e.g., extract features as in Luca Weihs and apply some learner
      * can also look at other papers and their meta-data approaches
   * In general: start writing in early December at the latest


---++++ *Meeting 09.11.2017*
   * *Info*: 
      * For meta-data based features, may have a look here: https://github.com/Lucaweihs/impact-prediction, http://ai2-website.s3.amazonaws.com/publications/JCDL2017.pdf
      * Another definition of novelty: 
         * https://pdfs.semanticscholar.org/ccef/bfea8303fd4a5b79d0b7f3dfd467d321a2d8.pdf, Section 3.2
         * SE: I do think that this would be difficult to compute, because we would need to extract all the references ...
   * *TODOS:*
      * novelty & readability features
      * Both with log.reg. and MLP
   * *Catchup & general:*
      * What is the body exactly composed of? Everything? (abstract, intro, ...)
      * Optional: Make a single CNN representation of abstract, intro, body, conclusion, then concatenate
         * Could be better than a single model on all text 
      * Yogotama: Binary+log(features): exclude one of them
      * Fasttext: add some experiments
      * Code sustainability: document your code well
      * Reproducibility: store your commands in little "run.sh" scripts, so that you know in the end what you did
      * Optional: MLP with Yogatama.
      * Do something with the _title_: e.g. a separate category or include it in the abstract and/or body
      * To run a command independently of the console, type: "nohup sh run.sh > store.dat &"
      * Maybe add more training data by adding first half of 2016
         * Either ignore the fact that this isn't exactly prediction anymore
         * Or: try to get month citations in 2017

---++++ *Meeting 02.11.2017*
   * Expand train set for cs.CL: e.g., include first 6 months from 2016 in train set
      *  also re-run with fasttext
   * Work on conclusion extractor and perform experiments on Introduction and Conclusion
   * HEP label distribution: split ">2" in "3" and ">3" or similarly
   * Document your code well so that we could re-use it very simply for new data (e.g. cs.ML). Keep it flexible
   * Play around with the Yogotama baseline:
      * use only unigrams
      * omit the binary features and keep only log counts. 
         * Did you log the counts? Double check
   * CNN: use more filters than 75 - this might boost your performance. Play around with other hyperparams
   * Implement F1 and report F1 performance. E.g.: do we have high recall for papers in the class ">2"?
   * If time permitting:
      * Play around with novelty scheme:
         * log-likelihood ratio test: http://ucrel.lancs.ac.uk/llwizard.html
         * Idea: Small corpus and big corpus. This can calculate (and rank) you the words whose frequency is most unexpected given the big corpus
            * Big corpus = all previous years, Small corpus = current year. Represent each document e.g. as the log counts of the 100 most unexpected words in the current year

---++++  *Meeting 26.10.2017*
   * Try outs:
      * To alleviate data sparsity, accumulate everything from 2008 to 2015 take citations in corresponding next year. As test use 2016 to 2017
         * Additionally, may crawl e.g. cs.ML
      * Make a histogram of the label distribution: papers and their citations
      * check if own implementation of vdcnn really works or look for different ones on the net
      * Report F1 macro in addition to accuracy
      * look into the classifiers: which errors do they commit? do they always predict majority? what is the majority?
      * include/exclude author,venue from Yogotama and from Neural Nets
      * Fix the problem with yogotama (features only from abstract)
      * try out MLP with Yogatama; use one dense hidden layer and maybe ReLU or PRELU as activation function

---++++  *Meeting 19.10.2017*
   * Parameters to play around with:
      * Stop words on/off
      * Evaluate deep learning approaches on abstract, intro, conclusion, whole text
      * Can include/exclude author, venue, etc. both in Yogotama baseline and in Deep Learning approaches
         * You could do it as follows (or differently): "{Title}%%%{Author1,Author2,...}%%%{Abstract}%%%{Intro}%%%{Body}%%%{Conclusion}%%%{References}" as a long sequence of text.
      * Use ridge regression or alternatively MLP for Yogotama et al. baseline
   * TODOs:
      * fix/optimize the conclusion extractor
      * document "run.sh" a little better: what do the arguments stand for? where is the output written to?
      * make a statistics:
         * how many papers overall? how many in your train/dev/test splits?
---++++ *Meeting 12.10.2017*

   * Please provide SE with the following:
      * Sample full texts on apu (ideally already pre-processed) with meta data
      * Program to download meta data from SemanticScholar
      * All full texts stored on apu
   * Very important:
      * *Structure* and *document* your coding
         * Can document by placing everything in simple bash scripts, for example, that could always be re-run
   * When forecasting, you can play around with the forcecast gap, e.g., 1 year, 2 years, etc.
   * For n-gram vectors you can play around with their sizes (e.g. same number of 1-grams than 2-grams?)
   * TODOs for next week: evaluate Yogatama et al. baseline as well as deep learning models on apu (FastText, Very Deep Conv Nets, Standard Conv Nets)
   

---++ Possible thesis structure
   * Introduction [(1-2 pages)]
   * Related work [(2-3 pages) Focus on 1-meta-data approaches, 2-full text approaches, 3-other/mixed: early citations, time series,short titles/abstract]
   * Datasets [3 pages]
      * dataset creation
      * description. What is in the datasets?
   * Theory (CNN and other learners,novelty+readabilty) [<2 pages, can also merge with Section "Models", also include "prediction vs. modeling", be precise!]
   * Models (specific implementation) [>3 pages, with figures]
      * text based: CNN, Yogatama, fasttext
      * novelty+readabilty: mlp/log.reg.
      * metabased: mlp/log.reg./gbrt
   * Experiments [>10 pages]
      * evaluation-scenarios/datasplits
      * metrics
      * results/plots/tabels for different models
   * Discussion and Error analysis [5-7 pages]
   * Conclusion and future work [1-2 pages]




-- Main.SteffenEger - 2017-10-09

%META:FILEATTACHMENT{name="dataset.pdf" attachment="dataset.pdf" attr="" comment="statistics for the datasets" date="1508940784" path="dataset.pdf" size="52777" user="netzer" version="1"}%
%META:FILEATTACHMENT{name="experiments.pdf" attachment="experiments.pdf" attr="" comment="results on experiments.(25.10)" date="1508940861" path="experiments.pdf" size="23654" user="netzer" version="1"}%
%META:FILEATTACHMENT{name="experimentsv2.pdf" attachment="experimentsv2.pdf" attr="" comment="experiments results 2.11." date="1509612225" path="experimentsv2.pdf" size="24148" user="netzer" version="2"}%
%META:FILEATTACHMENT{name="datasetv2.pdf" attachment="datasetv2.pdf" attr="" comment="statistics for the datasets" date="1509551348" path="datasetv2.pdf" size="97851" user="netzer" version="1"}%
%META:FILEATTACHMENT{name="experimentsv3.pdf" attachment="experimentsv3.pdf" attr="" comment="" date="1510211951" path="experimentsv3.pdf" size="26565" user="netzer" version="1"}%
%META:FILEATTACHMENT{name="datasetv3.pdf" attachment="datasetv3.pdf" attr="" comment="" date="1510211984" path="datasetv3.pdf" size="100164" user="netzer" version="1"}%
%META:FILEATTACHMENT{name="experimentsv4.pdf" attachment="experimentsv4.pdf" attr="" comment="" date="1510760876" path="experimentsv4.pdf" size="28131" user="netzer" version="1"}%
%META:FILEATTACHMENT{name="experimentsv5.pdf" attachment="experimentsv5.pdf" attr="" comment="" date="1511429428" path="experimentsv5.pdf" size="24805" user="netzer" version="1"}%
%META:FILEATTACHMENT{name="experimentsv6.pdf" attachment="experimentsv6.pdf" attr="" comment="experiments with novelty cnn added" date="1511975929" path="experimentsv6.pdf" size="30305" user="netzer" version="1"}%
%META:FILEATTACHMENT{name="experiments_metabased.pdf" attachment="experiments_metabased.pdf" attr="" comment="experiments for metabased approoches" date="1511975968" path="experiments_metabased.pdf" size="21391" user="netzer" version="1"}%
%META:FILEATTACHMENT{name="thesis-florian-netzer.pdf" attachment="thesis-florian-netzer.pdf" attr="" comment="first version of the thesis including introduction, related-work , datasets and model section." date="1513601366" path="thesis-florian-netzer.pdf" size="476759" user="netzer" version="1"}%

%META:TOPICINFO{author="NiklasJakob" date="1172504500" format="1.1" reprev="16" version="16"}%
%META:TOPICPARENT{name="NiklasJakob"}%
---+Meeting-Protokolle

---++ 27.02.2007
   * [[%ATTACHURL%/Ausarbeitung-NJ-07_02_26.doc][Ausarbeitung-NJ-07_02_26.doc]]: Diskussion des Vorschlags zur Ausarbeitung

   * Prüfen der neuen UIMA Komponenten

---++ 12.02.2007

   * Diskussion der Vorschläge fürs Scoring-Interface -> Die Score Sets werden in "Property" Dateien erstellt; Wenn der Benutzer beim Aufruf der "Initialize" Funktion des Algorithmus kein ScoreSet übergibt wird das zu Algorithmus+Wissesquelle passende gewählt. Wenn der Benutzer sein eigenens ScoreSet verwenden möchte, erstellt er sich eine neue Instanz der Klasse und überschreibt deren default Werte.

   * Diskussion der Klasse für die Eingabedaten - Ist es problematisch die UIMA "Datenkette" damit zu unterbrechen? -> Stellt kein Problem dar. Die InputLexeme Klasse muss noch um "uimaBeginOffset" und "uimaEndOffset" erweitert werden, um nach Durchlauf der Algorithmen die Ausgabedaten wieder auf die UIMA Daten abbilden zu können.

   * Alle komplexen Datenstrukturen im Code werden auf ihre abstrakte Implementierung umgestellt: ArrayList, LinkedList, Vector -> List, HashSet -> Set, HashMap -> Map 

   * Folgende UIMA Annotatoren werden zur Verarbeitung der Eingabedaten erstellt: WordNumberAnnotator, SentenceNumberAnnotator, ParagraphNumberAnnotator, LexicalChainAnnotator

   * Vielleicht kann man http://de.wikipedia.org/wiki/Spezial:Meistbenutzte_Kategorien als Filter für die Kategorien verwenden.

---++ 05.02.2007, 15:45 Uhr

   * Nachtrag: Eine Tabelle mit den Gewichtungen der Relationen sollte eigentlich unabhängig für jeden Algorithmus und jede Wissensquelle einstellbar sein -> muss also noch heraus abstrahiert werden. => Für das Score mapping wird eine abtrakte Klasse o.ä. erstellt, sodaß die Gewichtungen unabhängig von Algorithmus und semantischem Netz einstellbar sind.

   * Für die Dateneingabe müssen folgende Wortfeatures bereits von UIMA annnotiert sein: Part-of-speech Tag, Wortposition im Satz, Satznummer, Paragraphnummer

   * Für die Datenausgabe werden von mir folgende Features annotiert: Wort gehört zur lexikalischen Kette, verwendeter Algorithmus, beim Algorithmus verwendetes semantisches Netz, eindeutiger Identifier des Wortes im verwendeten semantischen Netz

   * Um den Entwicklungsprozess am Laufen zu halten werde ich eine eigene Schnittstelle für die Algorithmen definieren die keine UIMA Objekte enthält. Stattdessen werden die Daten aus dem UIMA Vorlauf (im Annotator?) in das eigene Format gebracht.

   * Eine lokale Installation der Wikipedia wird erforderlich, da die Laufzeit des Algorithmus durch die Mero - / Holonym Implementierung stark gestiegen ist.

---++ 15.01.2007, 14:00 Uhr

   * Wikipedia API: Filter erstellen um Wikipedia-"Debug"-Kategorien wie " Cleanup from ...", "All pages needing cleanup", "Articles for deletion", "All pages needing to be wikified" bei Page.getCategories() herauszufiltern? (Tragen ja keinerlei semantische Informationen)
      * @action (TZ): wird noch eingebaut
   * @action (TZ): Umlaut-Fehler bei Redirects => beheben

---++ 18.12.2006, 16:00 Uhr

   * Vorhandene UIMA Annotator und Analysis Engines kopieren

   * Pro / Contra Abwägung zur Klasse "Synset" für alle Wissensdatenbanken. Eventuell neues Interface dafür erstellen?   
      * evtl. zweistufige Abstraktion - aber Abhängigkeiten zum Mapping von den Relationen der Wissensquellen zum Interface

   * Tips zur Wikipedia + API Installation

   * Relevante Funktionen der Wikipedia API:
      * Page (wiki.getPage()):
         * getCategories()
         * Unterschied zwischen getId() und getPageId()?
         * getIsDisambiguation()
         * getRedirects()
         * getPageLinks()
      * Category (wiki.getCategory()):
         * getPages()
      * CategoryGraph (???):
         * getChildren()
         * getParents()
         * stellen "Neighbours" semantische Relation dar?
      * Comparator
         * getDisambiguationPageSet() ?
         * getRelatednessValue() ?

Wikipedia Relationen lassen sich nicht sinnvoll auf rein-taxonomische Relationen wie bei den bestehenden Algorithmen abbilden - es ist eine Generalisierung notwendig (3 Klassen)


---++ 04.12.2006, 15:30 Uhr

   * UIMA zum parsen des bereits getaggedten Eingabetextes? Wie ist das Ausgabeformat des POS Taggers?
   * _Wird am kommenden Montag in einer kleinen Sitzung besprochen_

   * GermaNet Packages und deren Abhängigkeiten abgleichen... habe ich alles was ich benötige?
   * _Es wurde nochmal ein komplettes Paket kopiert._

   * Synset "Offset" Eigenschaft in GermaNet vorhanden ("Identifier" eines Wortes)? Ausblick auf Möglichkeit bei Wikipedia API?
   * _Der Offset scheint im GermaNet vorhanden zu sein, es ist jedoch bisher per API nicht möglich auf diesen Wert zuzugreifen (idSynMap). Ggf. muss die API dahingehend erweitert werden. Bei der Wikipedia API ist ein Offset/Identifiert vorhanden._

   * UML Diagramm besprechen und ggf. bereits verfeinern.
   * _Die Funktionsnamen und Parameter sollten weiter Abstrahiert werden, die Synset Klasse wird aus dem SemanticNet Interface entfernt. Ich werde abwägen ob der Datentyp "Synset" in diesem Zusammenhang erhalten werden soll, oder ob es, da dieses Konstrukt im Kontext der Wikipedia nicht direkt Existiert, durch ein abstrakteres Modell ersetze wird._

   * Problem des Zugriffs auf "leere" Datenstrukturen (z.B. Array / ArrayList / Vector) --> NullPointerException --> try + catch vermeidbar / eleganter lösbar?
   * _Das Problem kann umgangen werden indem man die Datenstrukturen nicht mit dem Objekt "Float" sondern dem elementaren Datentyp "float" erzeugt._

---++20.11.2006, 15:30 Uhr

   * IP-Vereinbarung: Zusätzlicher Vertrag wird zum nächsten Treffen unterzeichnet mitgebracht.

   * Eingabedaten: Das Konzept des CAS Objekts als Datenquelle wurde verfeinert und für gut befunden.

   * Programmstruktur: Zwecks Erweiterbarkeit und guter Struktur soll für die Algorithmen ein Java Interface ("LexicalChainExtractor"?) definiert werden. Ebenso soll für den Zugriff auf die Semantischen Netze ein Interface ("SemanticNet"?) definiert werden.

   * Eingabedaten: Es muss wahrscheinlich eine Klasse die UIMA implementiert definiert werden, welche den Input ersetzt.

   * Programmstruktur: Ein UML Diagramm der gesamten Anwendung wird erstellt welches bei fortschreitender Analyse des Zugriffs auf die Informationsquellen verfeinert wird.

---++06.11.2006, 15:30 Uhr

   * Format der Eingabedaten: Ziel ist es Texte von der UIMA Platform verarbeiten zu können. Dazu könnte den Algorithmen z.B. ein CAS Objekt zur Analyse übergeben werden. Dazu muss wahrscheinlich eine eigene Klasse zur Annotation geschrieben oder ein eigenes Token definiert werden.

   * Eingabeparameter: Der Threshold Wert zur Bestimmung der "Strong Chains" sollte bei beiden Algorithmen frei einstellbar sein. (Von den Autoren definiert sind "Strong Chains" alle diejenigen deren Wert um die zweifache Standardabweichung höher als der Mittelwert aller Chains ist)

   * Zugriff auf die Informationsquellen: Der Zugriff auf die Wikipedia, WordNet und GermaNet sollte eventuell über eine Dummy Klasse erfolgen, sodaß dies später auch über CAS geschehen kann.

   * Wikipedia API: Die wichtigsten Funktionen sind wahrscheinlich getCategories() und isDisambiguation() und für die Verwendung der Wikipedia als Informationsquelle elementar.

   * Evaluation: Es existieren Texte die manuell hinsichtlich lexikalischer Ketten ausgewertet wurden. Diese könnten bei der Evaluierung der Algorithmen verwendet werden. Zusätzlich könnte man empirisch die Qualität von automatisch extrahierten Ketten bewerten lassen.

   * Programmstruktur: Die Referenzimplementierung des Galley/McKeown Algorithmus wird von C++ nach Java portiert. Aus der Implementierung des Silber/McCoy Algorithmus müssen sämtliche Zugriffe auf das MMAX Eingabeformat entfernt und ersetzt werden.

   
-- Main.NiklasJakob - 22 Nov 2006


%META:FILEATTACHMENT{name="Ausarbeitung-NJ-07_02_26.doc" attachment="Ausarbeitung-NJ-07_02_26.doc" attr="" comment="Ausarbeitung 26.02.07" date="1172504426" path="Ausarbeitung-NJ-07_02_26.doc" size="97280" stream="Ausarbeitung-NJ-07_02_26.doc" user="Main.NiklasJakob" version="1"}%

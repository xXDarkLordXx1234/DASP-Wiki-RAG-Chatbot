%META:TOPICINFO{author="tariverdian" comment="" date="1679594214" format="1.1" version="2"}%
%META:TOPICPARENT{name="StudentsList"}%
-- Main.HaritzPuerto - 2022-09-07

---+++ Meeting 2023-03-16
* Status:
  * Comparison of correct baseline (Roberta-base HGN)

---+++ Meeting 2023-02-26
* Status:
  * 
* TODO:
  * Will investigate whether the position of the fusion layer is relevant, last two layers, last layer, ..
  
---+++ Meeting 2023-03-08
* Status:
  * Prettify Visualizations/Plots of F1-scores
* TODO:
  * Wrong baseline, Redid correct results with Roberta-base instead of R-Large


---+++ Meeting 2023-02-21
* Status:
  * Prettify Visualizations/Plots of F1-scores
  * Tables for best-performing models
  

---+++ Meeting 2023-02-03
* Status:
  * same HGN params size for both graph models  (6.9M)
  * varying sizes of Txt adapter {16,32,64,128,256,512,1024}
  * Comparison to total parameter size of Multimodal approach, 
  * Entity linking with Gated attention?

---+++ Meeting 2023-01-26
* Status:
  * multimodal: only count top and bottom txt adapter
  * adapter parameter size should be the same as in multimodal
* TODO:
  * Fixed Total size, varying Txt size, corresponding HGN size, For total size of 13M, 17M

---+++ Meeting 2023-01-18
* Status:
  * Motivation: vanilla adapter with bigger hgn_hidden_size
  * should have better results
  * setup: Redo experiments

* TODO:
  * total number of parameters should be the same or at least the hgn should be smaller (Important for the analysis experiments)
  * Read leonardos work and check if the setup is similar (If not maybe better results with the vanilla model)
  * (Show in his domain results with vanilla model), may be losing information in mapping BERT embeddings to node embedding( 768 to 64/32)

---+++ Meeting 2023-01-05
* Status:
  * exp constellation:
    * 2 with hgn Adapter:
      *  hgn vanilla with 300 (atx_hid_dim) predict_layer (sum 0.5M parameters)
      *  hgn vanilla with 768   (3.5M parameters)
   * 2 with same multimodal head:

---+++ Meeting 2022-12-29
* Status:
  * Add one or two more data points for the green graph in performance/training parameters
  * Comparison Vanilla Adapter with multimodal
* TODO:
  * Exp setup:
    * hgn parameter size the same, Different adapter sizes (But also same parameter sizes)

---+++ Meeting 2022-12-21
* Status:
  * First impression that unbalanced is performing similar or even worse to a balanced constellation of txt to graph adapter size
  * On Multimodal-model not really clear yet, need more experiments to compare
  * both models need more experiments with bigger adapter size (i.e. adp_size=120)
* TODO:
  * make models even more unbalanced → x3, x4 hgn size vs txt size
    * → show tendency (is unbalanced better, worse?)
  * graph in the style of (StructAdapt-Paper) [score[f1, sp_f1, joint_f1] to param size, for different param configs]
  * identify best-performing model for multimodal and MM_v4
    * Run more epochs to see later behaviour (still balanced better?)
  * Experiments on HGN+vanilla:
    * Take txt adapter size 81 and match train param to 18.96M (Only compare to multimodal since it appears to be better)


---+++ Meeting 2022-12-14
* Status:
  * Compared to larger Txt Adapter, HGN Adapter is better in all metrics
  * Q: Why do the authors of QAGNN use an external memory source, if context info is already given and general knowledge is available in the LM?

* TODO:
  * Similar approach in K-adapter, try adapting encoder instead of layer (https://aclanthology.org/2021.findings-acl.121.pdf)
  * Transforming layer in between hgn-adapter,(nn.Linear(hidden_size, expected_size))
 

---+++ Meeting 2022-12-08
* Status:
  * Evaluated Multimodal and HGN+Vanilla Adapters on the same parameter sizes

* TODO:
  * Investigate Multimodal with bigger/smaller structadapt; Influence on performance?


---+++ Meeting 2022-12-01
* Status:
  * Mid Master Presentation

* TODO:
  * Investigate methods for AdapterFusion (https://arxiv.org/abs/2005.00247)
  * Ablation Study without PLM  


---+++ Meeting 2022-11-24
* Status:
  * Experiments with models, comparing in one table
  * Figures for HNG Architecture and Multimodal Architecture
  
* TODO:
  * Compare models with a focus on the same number of trained parameters
  * Investigate the effect of the adapter on model performance
  * Different configurations on StructAdapt
  * For fair experimental setup adapt sizes of Heads (Predict Layer)


---+++ Meeting 2022-11-17
* Status:
  * Removing the top adapter in text modality improves performance. Why?
  * Is the Usage of 2 adapters common? 
* TODO:

---+++ Meeting 2022-10-28
* Status:
   * HGN is working now. F1 = 62 (after 3 epochs)
   * New creating new HGN with adapters
   *Play with different model modifications:Plot different Adapter size



---+++ Meeting 2022-10-12
* Status:
  * Making changes to the multimodal model (graph and text embedding)
    - gathering results for different parameter settings
    - BERT compatibility
    
---+++ Meeting 2022-10-05
* Status:
  * Compare results on:
    - HGN (Roberta) fine-tuned
    - HGN w/ Vanilla Adapters
    - HGN w/ MultiModal StructAdapt 
    - New multimodal adapters (Multimodal v2)
  
---+++ Meeting 2022-09-30
* Status:
  * Create a new multimodal model with parallel modalities inside BERT-Layer
  * Run experiments layerwise and models fusion

---+++ Meeting 2022-09-21
* Status:
  * Background research on catastrophic forgetting
  * Possible solutions with altering context data in one preprocessing step like (https://arxiv.org/pdf/2203.03910.pdf)
  * set up SLURM for struct adapt reproduction

---+++ Meeting 2022-09-14
* Status:
  * Run HGN+Adapters in SLURM

---+++ Meeting 2022-09-08
* Status: 
  * Run HGN in SLURM
  * Get baselines done

---+++ Meeting 2022-09-01
* Status: Read papers on (HotpotQA, HGN 2020, Structadapt)





%META:PREFERENCE{name="ALLOWTOPICCHANGE" title="ALLOWTOPICCHANGE" type="Set" value="SewinTariverdian"}%
%META:PREFERENCE{name="PERMSET_CHANGE" title="PERMSET_CHANGE" type="Local" value="details"}%
%META:PREFERENCE{name="PERMSET_CHANGE_DETAILS" title="PERMSET_CHANGE_DETAILS" type="Local" value="SewinTariverdian"}%

%META:TOPICINFO{author="mischnaewski" comment="" date="1634995033" format="1.1" reprev="6" version="6"}%
%META:TOPICPARENT{name="StudentsList"}%
---+!! Bachelors Thesis: Reasoning over Knowledge Graphs for Generative Question Answering

*Supervisor: Leonardo Ribeiro*

---++++ Week 25.08 - 31.08.2021

   * Completed setting up Slurm, Wiki, including environments
   * Computed perplexity
   * Metrics in ELI5:
      * F1 for ROUGE-{1,2,L}
      * Human Judges
      * ROUGE-20%
      * FILL-1

---++++ Week 01.09 - 07.09.2021

   * Ran baselines 
      * This time with full dataset because of access to cluster
      * Models: t5-base & bart-base
      * Datasets: ELI5 & GOOAQ 
         * (ELI5 transformed to pairs of questions and answers instead of 1:N relation)
      * Completed:
         * t5-base on gooaq

---++++ Week 08.09 - 14.09.2021

   * Background research
      * Look into SOTA on ELI5
      * Look into using GNNs for QA
      * Evaulate potential approaches for improving QA using GNNs
      * Summarize in presentation for Leonardo
   * Run QA-GNN as a baseline (and proof of working on ukp slurm cluster)
      * Had issues with running due to old gcc version
         * Fixed by installing gcc over conda (important: first install gcc, then install pytorch-geometric etc. as otherwise geometric uses the wrong path)

---++++ Week 15.09 - 21.09.2021

   * Fix Baselines
      * Baselines use incorrect model parameters making them inferior to expected scores
      * Reformulate background research (tabular data etc.)
      * Perform entity extraction similar to QA-GNN
      * Perform subgraph extraction similar to QA-GNN
         * After that, think about extending subgraphs as they probably will not be large enough to contain the answer

---++++ Week 22.09 - 28.09.2021

   * Add selftext to input
      * Does not improve performance
   * Compare Conceptnet and DBPedia for ELI5
      * Conceptnet seems more adequate as knowledge is more general which ELI5 is likely to need more
      * Questions that are part of ELI5 are usually understood by the average person 
      * Conceptnet is more refined
   * Subgraph extraction
      * Performance issues due to some computations only required in QAGNN

---++++ Week 29.09 - 05.10.2021

   * Improve space efficiency of training scripts
   * Switch from DBPedia as main KG to Conceptnet
   * Run subgraph extraction 
      * Slow due to still present performance issues
      * Adjustment needed as QA-GNN often relies on the existance of answer candidates to build relevant hops
      * Script is often preempted
   * While subgraph extraction is running, work on adding the main ELI5 baseline using fairseq
   * Write thesis itself

---++++ Week 06.10 - 12.10.2021

   * Complete subgraph extraction
   * Complete graph linearization 
   * Run experiment with graph linearization on extracted subgraphs but on reduced dataset size (1/20th)
      * Results slightly worse than previously. Perplexity much worse than previously

---++++ Week 13.10 - 19.10.2021

   * Due to low priority on slurm cluster, restrict dataset size to 1/20th for quicker development
   * Rerun all experiments on 1/20th of the datasets
      * Rerun experiments 5 times for stable performance average
      * Complete first draft of midterm presentation (5th of November)
   * Start work on model for injecting dense knowledge representation between transformer encoder and decoder.
      * Specifically transformer for conditioning on two sequences

---++++ Week 20.10 - 26.10.2021

   * Suspend model conditioned on two sequences due to time constraints
   * Improve midterm presentation based on feedback
   * Add linearization used by Fan et al. (2019b)
   

%META:TOPICINFO{author="nothvogel" comment="save topic" date="1528972410" format="1.1" reprev="8" version="8"}%
---+!! %TOPIC%

---++ Meeting notes

---+++ 14.06.2018
   
   * Best-Worst scaling HIT setup
      * Try to use simple language, avoid domain-specific/technical terms
      * Need options for: Can't decide (e.g. all pairs equally similar), bug report
      * Keep assumptions of best-worst algorithm in mind
      * How to model task: 1 fixed pair on top, compare with other variable pairs? Make it simple for workers
   * Problem: Evaluating word encodings/embeddings and distance metrics
      * We want to use the best combination to pre-select interesting pairs for annotation
      * Testing on other corpora depends on the data set quality and might introduce bias
      * Collecting expert annotations on our new dataset in house might not be sufficient for Best-Worst Scaling
         * We can only produce a small number of annotations for the dataset
         * Best-Worst Scaling requires a certain number of annotations to work correctly
      * Later: Test combinations again on our new, then annotated dataset
   * Check if the existing UKP corpus / Misra et al corpus is sufficient
   * Code examples for preprocessing / HIT setup / training similarity systems available in UKP gitlab
   * Open questions: 
      * Pairwise preference methods applicable for ArgSim problem? See below.
      * About Habernal Gurevych 2016 annotation method: 
         * You can reject workers based on their low computed competence (MACE?) or other criteria such as too short submission times.
         * How to handle transitive properties / potentionally conflicting preference annotations? E.g. a>b, b>c but c>a

---+++ 11.06.2018

   * Meeting cancelled on my request, rescheduled on Thursday

---+++ 25.05.2018

   * Assumption: Arguments generated by in-house parser are good enough
   * Argument quality still needs to be considered: Sentence length, readability, confidence of parser etc.
   * Filter arguments: Use simpler rules (no annotation for argument quality)
   * Take notes while reviewing related work; needs to be included in thesis
   * Thesis deadline: ~ mid december

---++++ Todo:
   
   * Evaluate existing UKP arg sim corpus: Enough data for sampling?
   * Study Best-Worst scaling in depth, make sure assumptions of the algorithm are satisfied
   * Create better mockup for Best-Worst scaling HIT
   * Literature review / related work on:
      * Preprocessing: How arguments are preprocessed, how interesting pairs are selected
         * JD: hier bitte folgendes ansehen: http://argumentation.bplaced.net/arguana-publications/papers/wachsmuth18a-acl.pdf (Sect. 5.1 sim. computation + embedding arg. similarity)
            * How to evaluate which distance metric performs best? 
            * ConceptNet was used because it is smaller (encoding or model size?)
            * Word Movers Similarity
      * Pairwise annotation for assessing arg sim
      * Habernal Gurevych 2016 (annotation method)
      * Preference/ranking based methods for crowdsourcing/annotating similarity:
         * Applicable for arg sim problem? [[https://www.microsoft.com/en-us/research/wp-content/uploads/2013/02/wsdm2013-preference-chen-et-al.pdf][Chen et al 2013]] or [[https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7980235&tag=1][Wang et al 2017]]

-- Main.PhilippNothvogel - 2018-05-25

%META:PREFERENCE{name="TOPICTITLE" title="TOPICTITLE" type="Local" value="Philipp Nothvogel"}%

%META:TOPICINFO{author="nothvogel" comment="" date="1533038085" format="1.1" reprev="23" version="23"}%
---+!! %TOPIC%

---+++ UKP Arg Sim corpus stats
   * #Topics: 28
   * Original (unfiltered): 3640 argument pairs, 130 pairs per topic
   * Filtered (removed pairs with DTORCD gold label):
      * 3064 argument pairs
      * Average: 109.43 pairs per topic
   * Non-argument count per topic and stance trends:
      * [[https://1drv.ms/x/s!Ag8wvhyPqjGGgexsLg4XauwZBPJZIQ][Excel Table]] 
      * Updates automatically as I make changes.
      * Stance explanation: One character per hit, displays if arguments in that hit are mostly
         * pro: +
         * mixed: ~
         * con: -  

---++ Meeting notes

---+++ Upcoming
   * Status of literature review

---+++ 30.07.2018
   * Rework guidelines: 
      * Move explanation in examples to the right
      * Add shading to show similarity levels
      * Add small text about the given topic (bullet points, short)
   * Non-arg test results: Use the best 15 topics for now (median <= 1.5 bad args per hit)
   * Priority: Deploy to AMT for
      * 1: Small test round with JD and me. Keep track time needed, make adjustments to the HIT if necessary 
      * 2: Larger in-house study over all topics (to create gold standard...)
   * Try to get some topics on AMT Sandbox until Thursday 14:00
   * Not neccessary to check BWS assumptions for first small tests, but make sure to add it later
   * Estimated cost for crowdsourcing 1500 argument pairs using BWS, with n=number of items to annotate: 
      * Optimal: Generating 2*n BWS questions, 10 annotations from different workers per HIT: [[%ATTACHURLPATH%/full.txt][full]]
      * Minimal: Mohammad et al ('Capturing Reliable Fine-Grained Sentiment Associations[...]', 2016) concluded in their study that it was enough to generate 1.5*n BWS questions and collect 3 annotations per BWS question. Using these values: [[%ATTACHURLPATH%/min.txt][min]]
      * Reduced: Mohammad et al were working with much smaller items (only a few words) and we probably need some more annotations just to be safe. We could generate something like 1.7*n BWS questions and collect 7 annotations per BWS question: [[%ATTACHURLPATH%/red.txt][reduced]]
      * If we implement a function that checks the BWS assumption (each item appears approx. in the same number of questions) we can probably get as low as 1.5*n BWS questions
      * We don't know how many annotations per question we need. We could test a smaller subset with 10 annotations, then run reproducability tests to determine the minimum number of required annotations per question (similar to Muhammad et al) to save costs
   
---+++ 24.07.2018
   * HIT 3d printing test results:
      * Agreement
         * Best: 55%
         * Worst: 45%
         * Best & Worst combined: 25%
         * Problem: Many `non-arguments`. Hard to decide best/worst
   * How to deal with `non-arguments`?
      * topics like 3d printing, big data, cloud storing and cryptocurrency contain many sentences that fit the given topic, but are not arguments (i.e. companies advertising their product)
      * if there are too many non-arguments it gets hard to decide which pair is best/worst (in particular with 2 or more 'bad' pairs in a single BWS question)
      * we need at least 15-20 topics / 1500-2000 pairs in the end
      * Check the other topics of the existing UKP arg sim corpus:
         * generate 10 test HITs for each topic
         * create a table with the 28 topics and rows for hit 1-10 (so we can have statistics per BWS question/hit too).
         * count how many non-argument pairs there are (if at least 1 argument in that pair is bad, it counts as 1 non-arg pair).
         * also keep track of general stance of the arguments on the topic (pro/con/mixed).
      * Alternative/Fallback solution: Generate more arguments from other dataset on argument stance
   * Rework HIT guidelines and examples:
      * improve examples: correct typos, grammar. Do not need to copy them 1:1 from the corpus
      * add `general tips` to guidelines: ignore typos/grammar, ...
      * what to explain (and what to omit): text should be short so workers actually read it
         * 1 sentence: what is an argument with respect to a given topic? Possibly as introduction for examples
      * Give 5 examples to sepeare them visually from the actual task (4 pairs)
      * Sort them in one direction (i.e. from low to high similarity), possibly underline it graphically (shading?)
      * Replace criteria column with `explanation`: use bullet points, no full text
      * Do not mention `aspect` if it is not neccessary. aspect is not the only criterium (scope, overlap...)
   * I received amt access info today. Will do some tests. Save results of internal tests already (need it for agreement checks and gold standard).

---+++ 16.07.2018
   * HIT guidelines: Do not formalize too much. Test without scale, try textual explanation.
   * New HIT tests with JD and me:
      * 20 HITs, single topic (different from fracking)
      * Including guidelines draft, without can't decide
   * We need enough pairs for the different topics (and if possible with different stances). Count:
      * How many pairs, topics are available (excluding DTOCD)
      * Per topic: How many pairs? Stance (same stance / mixed)?
      * Calculate average pairs per topic
   * AMT Sandbox: JD will give me account info soon. To test the HIT, deployment to AMT, result collection (+ in-house tests later)
   * Captchas:
      * Should be made obvious (i.e. `Select this pair as 'best'`).
      * 10-20 different phrases to make automating difficult
      * About every 10th HIT should be a captcha
   * https://arxiv.org/pdf/1709.01887.pdf >> last 2 Pages (Appendix) contain examples for crowdsourcing guidelines used ("Figure 10:  Definitions used for Facet and AFS in MT HIT"), see also http://www.aclweb.org/anthology/N15-1046.pdf
   * annotation time per HIT (110-120sec.)
   * Agreement over 16 questions: 81% best, 68% worst, 50% best and worst combined (excluding questions marked as can't decide)
   * Controversial questions are usually the ones where some pairs are very similar
   * Do we even need (or want to have) the `can't decide` option for cases where some pairs are very similar to each other? 
      * If you are not sure, it should be fine to select either one
      * If we collect more annotations for these questions, similar pairs are likely to receive a score that is close to each other.
      * Some kind of bug report function might still be useful.


---+++ 09.07.2018
   * generate hits as seperate html files is fine (as long as it is reproducable/backed up and we know which annotation belongs to which particular hit/data)
   * Todo:
      * Fix HIT validation (prevent best==worst)
      * Try not using external js libraries if possibleto prevent problems
      * Make sure HIT is compatible with all modern browsers
      * Agree on HIT guidelines: Similar to misra et al?
         * Do we need a scale that is this fine-grained? Table can get quite longâ€¦
         * Examples in table or in qualification/training phase?
         * Do we need the explicit definition of FACETS?
      * Solve some test HITs and keep track of time needed / eventual problems

---+++ 26.06.2018
   * Ranking papers linked below probably not applicable for our problem (goal: ranking vs numeric score)
   * In house study: 
      * Not possible to collect enough annotations in house to apply full best-worst scaling
      * Instead used to test HIT setup, make sure task is well-defined and expert agreement is sufficient
      * Compare with AMT worker results later
      * Use existing topics/argument pairs from UKP corpus for now, create own corpus later
      * Leave incorrect arguments in the dataset for now, only filter if we encounter them too often
   * Worker competence / HIT setup:
      * Set Additional requirements for workers (AMT parameters)
      * Use qualification test and a few training examples.
      * Use Captchas / Control Questions to filter bad workers
   * Todo:
      * Research how worker competence can be included in the BWS model. If nothing else is found, use MACE.
      * Develop/Implement HIT layout (1 reference argument fixed at the top that appears in all 4 pairs?)
      * Implement argument pair sampling (4-touples) for BWS and connect it to the 'frontend'

---+++ 14.06.2018
   
   * Best-Worst scaling HIT setup
      * Try to use simple language, avoid domain-specific/technical terms
      * Need options for: Can't decide (e.g. all pairs equally similar), bug report
      * Keep assumptions of best-worst algorithm in mind
      * How to model task: 1 fixed pair on top, compare with other variable pairs? Make it simple for workers
      * Kiritchenko and Mohammad (2016) find that 2n BWS questions are enough to obtain reliable scores. (with n: number of items (argument pairs)
         * They conclude that you would only need 3 annotations per BWS question to obtain reliable annotations.
         * However, they annotate words/very small phrases only.  
         * We probably need to collect more annotations initially
      * It seems like there is no restriction on set size that should be annotated with BWS (existing corpora could be used)
   * Problem: Evaluating word encodings/embeddings and distance metrics
      * We want to use the best combination to pre-select interesting pairs for annotation
      * Testing on other corpora depends on the data set quality and might introduce bias
      * Collecting expert annotations on our new dataset in house might not be sufficient for Best-Worst Scaling
         * We can only produce a small number of annotations for the dataset
         * Best-Worst Scaling requires a certain number of annotations to work correctly
      * Later: Test combinations again on our new, then annotated dataset
   * Check if the existing UKP corpus / Misra et al corpus is sufficient
      * UKP corpus contains some invalid arguments: Missing spaces, webdriver errors, ads...
      * Possible to use subset with lower MACE threshold?
   * Code examples for preprocessing / HIT setup / training similarity systems available in UKP gitlab
   * Open questions: 
      * Pairwise preference methods applicable for ArgSim problem? See below.
      * About Habernal Gurevych 2016 annotation method: 
         * You can reject workers based on their low computed competence (MACE?) or other criteria such as too short submission times.
         * How to handle transitive properties / potentionally conflicting preference annotations? E.g. a>b, b>c but c>a

---+++ 11.06.2018

   * Meeting cancelled on my request, rescheduled on Thursday

---+++ 25.05.2018

   * Assumption: Arguments generated by in-house parser are good enough
   * Argument quality still needs to be considered: Sentence length, readability, confidence of parser etc.
   * Filter arguments: Use simpler rules (no annotation for argument quality)
   * Take notes while reviewing related work; needs to be included in thesis
   * Thesis deadline: ~ mid december

---++++ Todo:
   
   * Evaluate existing UKP arg sim corpus: Enough data for sampling?
   * Study Best-Worst scaling in depth, make sure assumptions of the algorithm are satisfied
   * Create better mockup for Best-Worst scaling HIT
   * Literature review / related work on:
      * Preprocessing: How arguments are preprocessed, how interesting pairs are selected
         * JD: hier bitte folgendes ansehen: http://argumentation.bplaced.net/arguana-publications/papers/wachsmuth18a-acl.pdf (Sect. 5.1 sim. computation + embedding arg. similarity)
            * How to evaluate which distance metric performs best? 
            * ConceptNet was used because it is smaller (encoding or model size?)
            * Word Movers Similarity
      * Pairwise annotation for assessing arg sim
      * Habernal Gurevych 2016 (annotation method)
      * Preference/ranking based methods for crowdsourcing/annotating similarity:
         * Applicable for arg sim problem? [[https://www.microsoft.com/en-us/research/wp-content/uploads/2013/02/wsdm2013-preference-chen-et-al.pdf][Chen et al 2013]] or [[https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7980235&tag=1][Wang et al 2017]]

-- Main.PhilippNothvogel - 2018-05-25

%META:FILEATTACHMENT{name="full.txt" attachment="full.txt" attr="" comment="" date="1533036615" size="214" user="nothvogel" version="1"}%
%META:FILEATTACHMENT{name="min.txt" attachment="min.txt" attr="" comment="" date="1533037114" size="217" user="nothvogel" version="1"}%
%META:FILEATTACHMENT{name="red.txt" attachment="red.txt" attr="" comment="" date="1533037480" size="226" user="nothvogel" version="1"}%
%META:PREFERENCE{name="TOPICTITLE" title="TOPICTITLE" type="Local" value="Philipp Nothvogel"}%

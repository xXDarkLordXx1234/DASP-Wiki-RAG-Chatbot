%META:TOPICINFO{author="nothvogel" comment="" date="1531739747" format="1.1" reprev="13" version="17"}%
---+!! %TOPIC%

---++ Meeting notes

---+++ Upcoming
   * https://arxiv.org/pdf/1709.01887.pdf >> last 2 Pages (Appendix) contain examples for crowdsourcing guidelines used ("Figure 10:  Definitions used for Facet and AFS in MT HIT"), see also http://www.aclweb.org/anthology/N15-1046.pdf
   * Status of literature review
   * annotation time per HIT
   * Agreement over 16 questions: 81% best, 68% worst, 50% best and worst combined (excluding questions marked as can't decide)
   * Controversial questions are usually the ones where some pairs are very similar
   * Do we even need (or want to have) the `can't decide` option for cases where some pairs are very similar to each other? 
      * If you are not sure, it should be fine to select either one
      * If we collect more annotations for these questions, similar pairs are likely to receive a score that is close to each other.
      * Some kind of bug report function might still be useful.

---+++ 09.07.2018
   * generate hits as seperate html files is fine (as long as it is reproducable/backed up and we know which annotation belongs to which particular hit/data)
   * Todo:
      * Fix HIT validation (prevent best==worst)
      * Try not using external js libraries if possibleto prevent problems
      * Make sure HIT is compatible with all modern browsers
      * Agree on HIT guidelines: Similar to misra et al?
      * Solve some test HITs and keep track of time needed / eventual problems

---+++ 26.06.2018
   * Ranking papers linked below probably not applicable for our problem (goal: ranking vs numeric score)
   * In house study: 
      * Not possible to collect enough annotations in house to apply full best-worst scaling
      * Instead used to test HIT setup, make sure task is well-defined and expert agreement is sufficient
      * Compare with AMT worker results later
      * Use existing topics/argument pairs from UKP corpus for now, create own corpus later
      * Leave incorrect arguments in the dataset for now, only filter if we encounter them too often
   * Worker competence / HIT setup:
      * Set Additional requirements for workers (AMT parameters)
      * Use qualification test and a few training examples.
      * Use Captchas / Control Questions to filter bad workers
   * Todo:
      * Research how worker competence can be included in the BWS model. If nothing else is found, use MACE.
      * Develop/Implement HIT layout (1 reference argument fixed at the top that appears in all 4 pairs?)
      * Implement argument pair sampling (4-touples) for BWS and connect it to the 'frontend'

---+++ 14.06.2018
   
   * Best-Worst scaling HIT setup
      * Try to use simple language, avoid domain-specific/technical terms
      * Need options for: Can't decide (e.g. all pairs equally similar), bug report
      * Keep assumptions of best-worst algorithm in mind
      * How to model task: 1 fixed pair on top, compare with other variable pairs? Make it simple for workers
      * Kiritchenko and Mohammad (2016) find that 2n BWS questions are enough to obtain reliable scores. (with n: number of items (argument pairs)
         * They conclude that you would only need 3 annotations per BWS question to obtain reliable annotations.
         * However, they annotate words/very small phrases only.  
         * We probably need to collect more annotations initially
      * It seems like there is no restriction on set size that should be annotated with BWS (existing corpora could be used)
   * Problem: Evaluating word encodings/embeddings and distance metrics
      * We want to use the best combination to pre-select interesting pairs for annotation
      * Testing on other corpora depends on the data set quality and might introduce bias
      * Collecting expert annotations on our new dataset in house might not be sufficient for Best-Worst Scaling
         * We can only produce a small number of annotations for the dataset
         * Best-Worst Scaling requires a certain number of annotations to work correctly
      * Later: Test combinations again on our new, then annotated dataset
   * Check if the existing UKP corpus / Misra et al corpus is sufficient
      * UKP corpus contains some invalid arguments: Missing spaces, webdriver errors, ads...
      * Possible to use subset with lower MACE threshold?
   * Code examples for preprocessing / HIT setup / training similarity systems available in UKP gitlab
   * Open questions: 
      * Pairwise preference methods applicable for ArgSim problem? See below.
      * About Habernal Gurevych 2016 annotation method: 
         * You can reject workers based on their low computed competence (MACE?) or other criteria such as too short submission times.
         * How to handle transitive properties / potentionally conflicting preference annotations? E.g. a>b, b>c but c>a

---+++ 11.06.2018

   * Meeting cancelled on my request, rescheduled on Thursday

---+++ 25.05.2018

   * Assumption: Arguments generated by in-house parser are good enough
   * Argument quality still needs to be considered: Sentence length, readability, confidence of parser etc.
   * Filter arguments: Use simpler rules (no annotation for argument quality)
   * Take notes while reviewing related work; needs to be included in thesis
   * Thesis deadline: ~ mid december

---++++ Todo:
   
   * Evaluate existing UKP arg sim corpus: Enough data for sampling?
   * Study Best-Worst scaling in depth, make sure assumptions of the algorithm are satisfied
   * Create better mockup for Best-Worst scaling HIT
   * Literature review / related work on:
      * Preprocessing: How arguments are preprocessed, how interesting pairs are selected
         * JD: hier bitte folgendes ansehen: http://argumentation.bplaced.net/arguana-publications/papers/wachsmuth18a-acl.pdf (Sect. 5.1 sim. computation + embedding arg. similarity)
            * How to evaluate which distance metric performs best? 
            * ConceptNet was used because it is smaller (encoding or model size?)
            * Word Movers Similarity
      * Pairwise annotation for assessing arg sim
      * Habernal Gurevych 2016 (annotation method)
      * Preference/ranking based methods for crowdsourcing/annotating similarity:
         * Applicable for arg sim problem? [[https://www.microsoft.com/en-us/research/wp-content/uploads/2013/02/wsdm2013-preference-chen-et-al.pdf][Chen et al 2013]] or [[https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7980235&tag=1][Wang et al 2017]]

-- Main.PhilippNothvogel - 2018-05-25

%META:PREFERENCE{name="TOPICTITLE" title="TOPICTITLE" type="Local" value="Philipp Nothvogel"}%

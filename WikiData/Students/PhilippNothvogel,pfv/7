%META:TOPICINFO{author="nothvogel" comment="save topic" date="1528912561" format="1.1" reprev="3" version="7"}%
---+!! %TOPIC%

---++ Meeting notes

---+++ 11.06.2018

   * Meeting cancelled on my request, rescheduled on Thursday

---+++ 25.05.2018

   * Assumption: Arguments generated by in-house parser are good enough
   * Argument quality still needs to be considered: Sentence length, readability, confidence of parser etc.
   * Filter arguments: Use simpler rules (no annotation for argument quality)
   * Take notes while reviewing related work; needs to be included in thesis
   * Thesis deadline: ~ mid december

---++++ Todo:

   * Take a look at the data (corpora from Misra et al, UKP corpus). x
   * Create a mockup on paper how the HIT for argument similarity is going to look like x
   * Transform mockup for Best-Worst scaling
   * Literature review / related work on:
      * Preprocessing: How arguments are preprocessed, how interesting pairs are selected
         * JD: hier bitte folgendes ansehen: http://argumentation.bplaced.net/arguana-publications/papers/wachsmuth18a-acl.pdf (Sect. 5.1 sim. computation + embedding arg. similarity)
            * How to evaluate which distance metric performs best? 
            * ConceptNet was used because it is smaller (encoding or model size?)
            * Word Movers Similarity
      * Pairwise annotation for assessing arg sim x
      * Best-Worst scaling
      * Habernal Gurevych 2016 (annotation method) x
      * Preference/ranking based methods for crowdsourcing/annotating similarity:
         * Applicable for arg sim problem? [[https://www.microsoft.com/en-us/research/wp-content/uploads/2013/02/wsdm2013-preference-chen-et-al.pdf][Chen et al 2013]] or [[https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7980235&tag=1][Wang et al 2017]]

-- Main.PhilippNothvogel - 2018-05-25

%META:PREFERENCE{name="TOPICTITLE" title="TOPICTITLE" type="Local" value="Philipp Nothvogel"}%

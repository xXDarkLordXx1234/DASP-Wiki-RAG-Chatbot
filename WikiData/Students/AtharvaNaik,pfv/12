%META:TOPICINFO{author="naik" comment="" date="1639508259" format="1.1" reprev="10" version="12"}%
%META:TOPICPARENT{name="StudentsList"}%
---+ *Internship Report*
<p align="center">
-----+ Research Timeline:
   * Setup NAM and partial set of patterns.
   * Justify and formalize format of patterns and matching function.
   * Create a general checklist: 1. Make lexicon, 2. Handle named entities   
   * Add some task specific patterns (from NLI/QA literature). 

<p align="center">
-----+ Useful Links:
[[https://drive.google.com/file/d/1eCg8TL8MTn844ihuzG6Tmc2IAAo-8V5Q/view?usp=sharing][NAM Model Diagram]]
[[https://drive.google.com/file/d/1t8vvfQ0X0z0cwpEzAnuKKH1e5Zbz0dgF/view?usp=sharing][Co-finetuning procedure]]
[[https://docs.google.com/document/d/12e53jyVdUH4mmg3cjofBkIsbAtNEp53Hrkd9hJbsmzg/edit?usp=sharing][NAM Documentation]]
</p>

<p align="center">
-----+ Idea:
-----+++ Description
   * Neural Additive Models or NAMs are a family of inherently intelligible (on a feature) models, that learn a separate f
   * We propose a novel fine-tuning procedure and input masking technique
   * Our input masking technique produces N binary masks that mask out some words from the input
   * 
-----+++ The Rationale:
   * We can consider performing a task say NLI, as a multi-task setup where we perform several subtasks on parts of the input, and fuse together the results.
   * We can have an explicit association of a small set of parameters (each adapter) with the classification subtask, analogus to a feature network in NAM
   * We can gauge the relative contribution of each subtask to the overall task. Relying too much on a single task or a subset of tasks can be an indication of artifacts that make it possible to solve the overall task by solving only a subset of tasks, indicating a bias. 
-----+++ Challenges in adapting NAMs to NLP tasks:
   * It is non-trivial to extract informative features from textual data
   * Many approaches treat words as features for textual data (LIME, SHAP etc.)
   * Learning feature nets for each word is not possible. It is much more meaningful to create functions that extract meaningful phrases from the input text.
   * Our masking procedure does exactly that
   * We define several rule-based masks that either extract structurally meaningful inputs.
</p>

<p align="center">
-----+ Meeting Notes
14th December 2021:
   * Discussion on some baselines and metrics
   * Our framework is most similar to highlights.
14th September 2021:
   * Discussion about pattern features for NAM. 
   * Possible flaw pointed out by Nafise about this approach. We can't consider the bias features extracted by this method, because actual transformer like models might not use these features.
   * Suggestion to move towards adapting NAM as a post hoc method, or using another post hoc method.
   * Discussion on the possibility of using transformer models to extract features and then use them as input to NAM. However this approach is not desirable because it involves training two models one after other and is thus inferior to other, more direct post hoc approaches.
24th August 2021:
   * Setup pytorch version of NAM and try it out on some sample datasets.
   * Utilize general patterns from the checklist paper to create features for NAM.  
</p>
17th August 2021:
   * Discussed approach: Design (or learn) a set of features (like text spans such as NP in dep parse, common words etc.) and learn predictor for label using a NAM for dense representation of each feature. Then use NAM to break down relative importance of each feature (instead of the nullification test). Note: here we want to overfit the model on the dataset to model the labelling function, we don't want to generalize our model.
   * Need to look into vaibility of training NAM on dense features (like Word2Vec or SBERT embedding) instead of a single variables.
   * Proposed method is similar to LIME, but LIME perturbs local gradients, we are scoring dataset level features, to find their relative contributions in the labelling function that represents the annotation.
   * Need to check papers similar to LIME, to make sure that this hasn't been done before.
   * Start with MNLI as a case study for validating proposed method: 1)come up with feature set for NAM 2)fit NAMs on feature set to extract biases 3)test if NAMs relative ranking results correspond with the biases found by existing stress test/adversarial papers.
   * This can later be extended to the class of PA using new feature set (or learning it by using any existing post-hoc interpretability approach on a trained model/NAM)
10th August 2021: 
   * Discussion of idea about features that bias model prediction towards a certain class (similar to competency models paper, Trigger words from explainability tutorial slides) and maybe more generally the idea of Illegitimate features (information leakage) (as defined in the Empirical Methods book under the validity section)
   * Infrastructure/communications setup
   * One possible direction is Validity of features (circular features, bias features and information leakage) 
</p>

<p align="center">
-----+ Summaries
   * Competency Problems: <LINK>
   * QA Survey and Taxonomy: <LINK>
   * Validity Problems: <LINK> 
   * Neural Additive models: <LINK>
   * False perfection in machine learning: <LINK>
</p>

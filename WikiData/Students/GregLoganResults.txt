%META:TOPICINFO{author="BaseUserMapping_999" date="1360762007" format="1.1" version="11"}%
%META:TOPICPARENT{name="GregLogan"}%
---+ Experiment Results
---++ 2008-12-04

While attempting to code the clustering annotator I noticed that the UkpSentenceSplitter annotator was not doing a very good job of splitting sentences. I replaced it (and UkpTokenizer) with SimpleWhitespaceTokenizer, which changed the results in a very minor way. Current results are as follows:
   * DB's annotations (teaching is missing): 60/64 correct with 14 false positives and 4 misses, out of 91 questions. 
      * Precision: 0.8108108108108109 Recall: 0.9375 F-measure: 0.8695652173913043
   * CT's annotations: 100/107 correct with 11 false positives and 7 misses, out of 134 questions.
      * Precision: 0.9009009009009009 Recall: 0.9345794392523364 F-measure: 0.9174311926605505
   * GL's testing (part-1) annotations: 121/126 correct with 19 false positives and 5 misses, out of 176 questions. 
      * Precision: 0.8642857142857143 Recall: 0.9603174603174603 F-measure: 0.9097744360902256
   * GL's eval (part-2) annotations: 125/143 correct with 20 false positives and 18 misses, out of 189 questions.
      * Precision: 0.8620689655172413 Recall: 0.8741258741258742 F-measure: 0.8680555555555555
   * GL's total annotations: 246/269 correct with 39 false positives and 23 misses, out of 365 questions.
      * Precision: 0.8631578947368421 Recall: 0.9144981412639405 F-measure: 0.8880866425992779
---++ 2008-12-01

Evaluation is complete for the question annotation pipeline. Currently:
   * DB's annotations (teaching is missing): 61/64 correct with 14 false positives and 3 misses, out of 91 questions. 
      * Precision: 0.8133333333333334 Recall: 0.953125 F-measure: 0.8776978417266187
   * CT's annotations: 101/107 correct with 11 false positives and 6 misses, out of 134 questions. 
      * Precision: 0.9017857142857143 Recall: 0.9439252336448598 F-measure: 0.9223744292237444
   * GL's testing annotations: 122/126 correct with 20 false positives and 4 misses, out of 176 questions. 
      * Precision: 0.8591549295774648 Recall: 0.9682539682539683 F-measure: 0.9104477611940299
   * GL's evaluation annotations: 246/269 correct with 40 false positives and 23 misses, out of 365 questions. 
      * Precision: 0.8601398601398601 Recall: 0.9144981412639405 F-measure: 0.8864864864864864
I have begun work on the written report, as well as the clustering.

DB: Thanks. Which of these datasets did you use to develop the lexicon? Or are all these test sets?

GL: I used my annotations on all four categories of questions in the -part-1 test sets. The results for strictly that data are my 'testing annotations' (second from the bottom). The full dataset with both -part-1 and -part-2 data is the bottom entry.

---++ 2008-11-25

Pipeline now handles all test datasets, so I have much more input to test against. Currently getting:

103/126 opinionated correctly with 15 false positives and 23 misses, out of 176 questions total.

Requiring a minimum of 3 opinion words for short sentences and defining long questions as 6 or more sentences. The weight for the questions is 0.5.

The logic behind the classification is still being worked on, but currently stands as follows:
   * The count for the question is calculated by adding one for each single-word opinion patterns, and adding a the sum of a geometric series (ratio = 2) for multi-word opinion patterns.
   * If that sum if creater than a limit, mark the question as opinionated
   * If the number of sentences is over a certain limit and the count is greater than X mark the question as opinionated
I am still experimenting with the value for X, and I'm not 100% sure that I even need that whole category. The current formula is count &gt;= floor(opinionated_pattern_minimum - (sentenceCount * sentence_weight))

[[%ATTACHURL%/opinionwords.txt][opinionwords.txt]] is the newest version of my lookup.

---++ 2008-11-18

Initial experiments on environment-part-2 data looked good until I factored in the false positives:

22 Questions, 11 Opinionated, 9 Detected, 8 False Positives.

Questions are classed as opinionated by my code on one of three conditions:
   * The question has more than X number of words in the dictionary list
   * The question has more than Y sentences, and has at least Z opinion words
   * The question has only one sentence and has more than one opinion word

Other datasets are still in progress. Dictionary attached at [[%ATTACHURL%/opinionwords.txt][opinionwords.txt]]

[DB]: Have you split the dataset into a development set and a test set?
   $ IG: an absolutely *important point* , please take care of this!

GL: Not as of yet, but thank you for reminding me. The data is already divded into two parts, so I guess one part will be the training data and one the evaluation.

[DB]: Have you tried opinionated phrases like "what is the best" in addition to opinion words?

GL: Yes, and it doesn't appear to work the way I've implemented it. My annotator does a token based check, which of course means that "do" "you" "like" does not match "do you like". Unfortunately I don't see an easy way around this...

---++ Notes

Words like: basementcondimentdementcommandmentharassmentenslavementfermentfomentcementjudgementlamentmomentattachmentparlimenttenementamazement make SpellCorrector sad. Adding an upper limit to the size of the word appears to have fixed this.

Words like: 900000000000000000000000000000000000000000,000000000000000000000000000000000,00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000002! make TreeTagger sad

-- Main.GregLogan - 18 Nov 2008

%META:FILEATTACHMENT{name="opinionwords.txt" attachment="opinionwords.txt" attr="" comment="" date="1227622268" path="opinionwords.txt" size="1443" stream="opinionwords.txt" tmpFilename="/var/tmp/CGItemp24919" user="GregLogan" version="2"}%

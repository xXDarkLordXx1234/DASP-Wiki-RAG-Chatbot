%META:TOPICINFO{author="daxenberger" comment="" date="1551795212" format="1.1" reprev="29" version="29"}%
%META:TOPICPARENT{name="StudentsList"}%
---++ Talks/Deadline
   * 2019-03-19 final talk
   * 2019-03-25 planned submission deadline
   * 2019-05-02 thesis submission deadline

---+++ 2019-03-XX
   * discuss slides for final talk

---+++ 2019-03-05
   * handed in review version via email
   * hand-in of code and thesis
   * TODOs
      * error analysis
      * writing: evaluation section
      * finalize random seed experiments
      * slides for final talk

---+++ 2019-02-26
   * discussed non-determinism of neural networks: run with 10 different random seeds and report averaged results
   * error analysis
      * consider information gain
      * topic-wise error analysis 
   * note: next review version due 05.03. 
   * TODOs
      * error analysis
      * continue writing: evaluation section

---+++ 2019-02-19
   * next review version due 05.03. 
   * discussed whether discussion should be in its own section (for now: no)
   * discussed final talk 
   * experiments are finished
   * TODOs
      * finalize error analysis (see below)
      * continue writing: finalize Approach section, start results

---+++ 2019-02-15
   * discussed feedback about approach section
   * discussed results and clrified issue in plotting
   * 5. Conclusion
      * Results w.r.t. RQs
      * 5.1 Discussion
      * 5.2 Future Work
   * TODOs:
      * continue writing; remember to add a conclusion section, i.e. repeat RQs and present their results
      * error analysis (confusion matrix von BIO methods + investigate false positives and false negatives; for ranking methods show some examples)

---+++ 2019-02-07
   * TODOs:
      * continue writing
      * (DONE) experiment with different candiadate filters
      * (DONE) change plots: include upper bound

---+++ 2019-02-01
   * discussed current results & writing progress
   * Does candidate filtering help, e.g. improve performance or not?
   * TODOs:
      * (DONE) create a plot showing the recall on y-axis and the @n at x-axis to evaluate and understand how big the list for crowdworkers needs to be
      * (ONGOING) start writing approach section and elaborate the related work section; especially the argument mining part (DL: 7.2.2019)
      * (DONE) experiment with additional features (e.g. first half/second half, normalized postiion, preceeding and following words, contains noun/adjective, number of nouns, contains punctuation/stopword)


---+++ 2019-01-25
   * investigated results of support vector regression for ranking candidates
   * discuss current related work part
   * statistics on how many aspects per sentence to better understand the @n scores
   * TODOs:
      * (DONE) send screenshot of tucan registration to CS
      * start writing approach section and elaborate the related work section; especially the argument mining part (DL: 7.2.2019)
      * create a plot showing the recall on y-axis and the @n at x-axis to evaluate and understand how big the list for crowdworkers needs to be
      * experiment with additional features (e.g. first half/second half, normalized postiion, preceeding and following words, contains noun/adjective, number of nouns, contains punctuation/stopword)


---+++ 2019-01-17
   * Discussed current status
   * TODOs:
      * (ONGOING) Learn ranking model, e.g. regression
         * (DONE) Implement evaluation setup with "@-scores" and maybe thresholds on the scores (y-values)
         * (ONGOING) Experiment with various features (n-grams, pos, pos-patterns, length, position, dependencies, etc..)
      * Start writing approach section and elaborate the related work section; especially the argument mining part

---+++ 2019-01-10
   * Registration in Tucan?
   * discussed features for ranking function
   * TODOs:
      * Elaborate related work part and write introduction (DL: 17.01.2019)
      * (ONGOING) Learn ranking model, e.g. regression
         * (PARTLY DONE) Implement evaluation setup with "@-scores" and maybe thresholds on the scores (y-values)
         * Experimenbt with various features (n-grams, pos, pos-patterns, length, position, dependencies, etc..)
      * Experiment with candidate filtering or downsampling
      

---+++ 2019-01-03
   * Discussed related work section of the talk
      * change references from numbers to (author, year)
      * add more details about models (features, embeddings, parameters, which frameworks (keras, etc.))
      * candidate filtering: How? Why? -> it is important to describe the approach in detail also mention the problems involved with this (distribution, etc.)
      * Draw concludions with respect to the research questions
   * how to present related work in talk
   * TODOs:
      * (DONE) Check candidate extraction
      * Elaborate related work part and write introduction (DL: 17.01.2019)
      * Learn ranking model, e.g. regression
      * Train model on train, get scores on dev and test


---+++ 2018-12-20
   * Feedback on Related Work Section
      * Task definition: Describe our task in more detail and compare it to existing approaches in Sentiment Analysis.
      * Assessment of existing methods for our task: It is necessary to explain/discuss the difference of existing models/tasks compared to our goals. 
      * Sentiment aspects vs. argumentative aspects: What's the difference?
      * Reasons for not using the existing models for our task, e.g. those from task 1.2.
   * TODOs:
      * Slides for Mid-term talk (DL: 20.12.)
      * Prepare data for learning to rank model
         * (DONE) Get candidate scores
         * Learn ranking model, e.g. regression
         * Train model on train, get scores on dev and test

---+++ 2018-12-13
   * Structure of Talk
      * 1 Motivation
         * Task / Examples
         * Research Questions
      * 2 Related Work
      * 3 Approach
         * Data
         * Methods / Ideas
      * 4 Evaluation / Results
         * Evaluations-Setup
         * Sequence Labeling
         * Candidate Extraction / Ranking
      * 5 Conclusions (so far)
      * 6 Next Steps
   * TODOs
      * Slides for Mid-term talk (DL: 20.12.)
      * Prepare data for learning to rank model
         * Get candidate scores
         * Learn ranking model, e.g. regression
         * Train model on train, get scores on dev and test
      * (DONE) Review papers and continue writing related work section. (Deadline: 13.12.)
      * (DONE) Eperiments with LSTM / LSTM-CRF using word2vec/glove embeddings

---++ 2018-12-06
   * Experiument with syntactic information in LSTM mnodels
      * Try to change learning rate of the optimizer 
      * Try different batch_sizes
      * Try different activation function (tanh, relu, sigmoid)
   * Prepare data for learning to rank model
      * Get candidate scores
      * Learn ranking model, e.g. regression
      * Train model on train, get scores on dev and test
   * Search for other learning to rank methods
   * TODO / next steps
      * (DONE) Determine class distribution over IOB-labels in train, dev, and test sets
      * (DONE) Baselines: Majority baseline, random baseline
      * (ONGOING) Review papers and continue writing related work section. (Deadline: 13.12.)
      * (ONGOING) Eperiments with LSTM / LSTM-CRF using word2vec/glove embeddings
      * (DONE) Start implementing candidate extraction

---++ 2018-11-22
   * structure for thesis:
      * 3 Approaches for Extracting Argumenative Aspects
         * (introduction to the task including examples)
         * 3.1 Sequence Labeling Approaches
            * (3.2.1 Feature-Rich Conditional Random Field) 
            * (3.2.2 Neural Network Approaches using Word Embeddings)
            * (3.2.3 Neural Network Approaches with xyz)
         * 3.2 Ranking Approaches
            * (introduction: WHY ranking models? how?)
            * 3.2.1 Candidate extraction
            * 3.2.2 Scoring Candidates        
            * 3.2.3 Learning to Rank (different models)
         * 3.3 Chapter Summary
   * 4 Evaluation
      * 4.1 Data (maybe move to 3)
         * 4.1.1  Preprocessing
         * 4.1.2  Statistics (Class distribution)
      * 4.2 Evaluation Setup
         * 4.2.1  Data Splitting
         * 4.2.2  Evaluation Metrics (why are we using these measures? Do they allow us to compare ranking and sequence labeling models? If so how?)
      * 4.3 Model Selection (Results on DEV-Set; maybe move to 3)
         * (4.3.1 Evaluation of Candidate Selection)
      * 4.4 Results
         * (4.4.1 per topic)
         * (4.4.2 overall)
      * 4.5 Error Analysis
      * 4.6 Chapter Summary
   * TODO / next steps
      * Determine class distribution over IOB-labels in train, dev, and test sets
      * Baselines: Majority baseline, random baseline
      * (ONGOING) Review papers and continue writing related work section.  (*Deadline: 13.12.*)
      * (ONGOING) Eperiments with LSTM / LSTM-CRF using word2vec/glove embeddings
      * (ONGOING) Start implementing candidate extraction
   
---++ 2018-11-16
   * Problems with tagging -> indices don't match aspects
   * Differntiate between "model selection" and "model assessment" to prevent overfitted results.
      * model selection: Tuning parameters / features / etc on the dev set
      * model assessment: test generalizability of best model on the test set
   * TODO / next steps
      * Prepare structure for "approach/method" and "evaluation" section of thesis
      * (ONGOING) Review papers and continue writing related work section.  (*Deadline: 13.12.*)
      * (ONGOING) Eperiments with LSTM / LSTM-CRF using word2vec/glove embeddings
      * (ONGOING) Start implementing candidate extraction
      * (DONE) Use cross-topic setup for all experiments
      * (DONE) Features (prefex, relative position in sentence, contextual information, first half second half, num words preceding the current word)


      
---++ 2018-11-08
   * template for thesis
   * prepared data
   * Simple implementation of crf without cross topic setup -> pretty bad results (F1 score 0,207, recall 0,137)
   * @discussed data splitting: cross-topic: 4x train, 4x dev, 4x test (do feature selection / model selection on dev set only and report final scores on test)
   * @discussed candidate extraction method and its evaluation
   * repo: https://git.ukp.informatik.tu-darmstadt.de/stab/aspect-extraction 
   * thesis-template: https://wiki.ukp.informatik.tu-darmstadt.de/Students/InformationenFuerDiplomanden 
   * TODO / next steps
      * Review papers and continue writing related work section. 
      * Use cross-topic setup for all experiments
      * Features (prefex, relative position in sentence, contextual information, first half second half, num words preceding the current word)
      * Eperiments with LSTM / LSTM-CRF using word2vec/glove embeddings
      * Start implementing candidate extraction
   
   
---++ 2018-10-25
   * Structure for related work section
      * Argument Mining / Aspect-based Sentiment Analysis
      * Sequence Tagging/Labeling
         * CRF?
         * LSTM?
         * ...
      * Ranking Methods 
         * Regression?
         * Learning to Rank?
         * Levy?
      * Chapter Summary
   * *Evaluation Setup*: Cross-topic -> train in n-1 and test on n (n = topic)
   * Keyword: Active Learning
   * TODO / next steps
      * Review papers and continue writing related work section. 
      * Start implementing 
         * Implement a crf (or other sequence taggers) example 
         * Preapre data and data splitting 
         * Implement evaltuation measures

---++ 2018-10-19
   * paperwork
   * discuss task description 
   * discussed ranking approaches for aspect extraction 
   * evaluation metrics (How can e compare ranking and sequence modeling approaches?)
      * Solution 1: consider only best ranked candidate as actual "hit" and evaluate using token-level IOB/BIO tagging scheme
      * Solution 2: use approach described in http://www.aclweb.org/anthology/S14-2004
   * *TODOS / next steps*
      * (DONE) *Formulate a good argument why we use ranking methods instead of common sequence labeling methods.* in the current version of the task description 
      * (DONE) Get familiar with keras / tensorflow -> LSTM-CRF
      * (DONE) Evaluation Setup: How to split the data? Cross-topic vs. In-topic vs. "we don't care about the topic"
      * Review papers and start writing related work section. 

---++ 2018-10-11 
   * Forms for account, IP, etc. 
   * Discuss data to use for this project
   * Task description
   * Evaluation setup
   * @discussed: (1) selquence labeling vs. (2) ranking approaches
   * *Participate in the advanced seminar*: Dienstag 11:30 Raum B002+
   * *Todos / next steps*
      * Prepare forms and contract / etc.
      * Get familiar with the following methods CRF, LSTM-CRF and sequence-tagging (these models are for instance used for named-entity recognition,)
      * Get familiar with keras / tensorflow -> LSTM-CRF
      * Think about evaluation strategies w.r.t. to compare tagging and ranking approaches
      * Evaluation Setup: How to split the data? Cross-topic vs. In-topic vs. "we don't care about the topic"



-- Main.ChristianStab - 2018-10-11

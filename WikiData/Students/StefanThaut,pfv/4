%META:TOPICINFO{author="thaut" comment="" date="1582296385" format="1.1" reprev="1" version="4"}%
%META:TOPICPARENT{name="StudentsList"}%
*Important Dates:*
   * 28.01.2020 (mid-term presentation)
   * 07.04.2020 (final talk)
   * 21.04.2020 (submission deadline) 

---++ Week 21.10. - 27.10.:
   * Familiarization with the task
   * Review of literature on suitable corpora and BERT embeddings

---++ Week 28.10. - 03.11.:
   * Description of the corpora
   * First sections of the thesis about the corpora, their annotation schemes and related foundations
   * Introduction into concepts of Transfer Learning and recap of pytorch library

---++ Week 04.11. - 10.11.:
   * Preprocessing of the corpora -> summarization of their (separated) data into one CSV-File per corpus
   * Preprocessing of Lauschers gold labels into format for comparison with the output of Lauschers model prediction
   * Description of the evaluations of the particular corpora in their introducing paper
   * Locally setup of the Lauscher model
   * Implementation of the annotation tool for the annotation study and first version of the corresponding instructions

---++ Week 11.11. - 17.11.:
   * Reproduction of the scores of the Lauscher model
   * Error tracking regarding the Lauscher model and its experimental setup
   * Setup of the ARC model on Slurm
   * Preprocessing of Lauscher corpus for input into the ARC model
   * Revision of the instructions for the annotation study

---++ Week 18.11. - 24.11.:
   * Adjustment of the ARC model to the (preprocessed) Lauscher corpus
   * First experiments with the ARC model on the Lauscher corpus without finetuning and removal of the classification layer
   * Tests of the REST-API of the Lauscher model (for better reproduction of Lauscher scores)

---++ Week 25.11. - 01.12.:
   * Server problems on Slurm
   * Revision of the dataset for the annotation study and final version of its instructions
   * Revision of the splits of Lauscher corpus (now fully random) and reevaluation (per label as well as averaged) of Lauschers model on these (hopefully final) splits
   * First preliminary results of the annotation study
   * Gathering of possibilities of how to modify the ARC model for Transfer Learning and implementation of them

---++ Week 02.12. - 08.12.:
   * Bugfixing on Slurm
   * Deployment of the annotation study
   * Conversion of the other corpora into the ARC format and creation of respective splits
   * Preparation of the experiments of Transfer Learning with several configurations (with/out finetuning, various corpora, various splits, ...)

---++ Week 09.12. - 15.12.:
   * Evaluation of the TL experiment (creation of summarizing table, confusion matrices and metrics values for the distinct labels)
   * Added majority agreement for the annotation study
   * Calculating the majority baseline for TL experiments and added these values to the table
   * Validation of the calculation of the inter annotator agreements

---++ Week 16.12. - 22.12.:
   * Adjusting the TL code for baseline tests for each corpus and split (train ARC model on resp. split-train-data of corpus and test on the corresponding test data)
   * Bugfixing of Krippendorffs Alpha for annotation study
   * Review of comments regarding the code
   * Evaluation of the baseline tests (add these values to the table)
   * Consistency check of ARC model on ARC data itself (check if labels "0", "[SEP]" and "[CLS]" are also predicted)

---++ Week 23.12. - 29.12.:
   * Check if TL code is consistent and reliable; Can TL be implemented in other ways in Pytorch?
   * Adapt TL model to SciBERT
   * Start TL experiments with SciBERT as pretrained model
   * Bugfixing of finetuning in TL code
   * Averaging the results of the experiments on all splits
   * Describe some Preprocessing steps in the thesis
   * Start to write ML foundations section in thesis

---++ Week 30.12. - 05.01.:
   * Continuation with ML section in thesis; Report some evaluation in the thesis
   * Start writing on BERT/SciBERT section in thesis
   * Review literature on Multi Task Learning; Search for models

---++ Week 06.01. - 12.01.:
   * Preprocessing of the data for MTL experiments
   * Adapt mt-dnn model for our experiments; Prepare the code for executing the experiments
   * First preparation of midterm presentation
   * Evaluation of further results from the annotation study
   * Pairwise TL experiments
   * Started MTL experiments

---++ Week 13.01. - 19.01.:
   * Bugfixing of code for pairwise TL experiments
   * Bugfixing of MTL code
   * Evaluation of the first MTL results
   * Adaptation of the mt-dnn model to the same evaluation metrics as the AURC model (used for TL experiments)
   * Preparation of midterm presentation
   * Further MTL experiments

---++ Week 20.01. - 26.01.:
   * Preprocessing of AURC corpus for MTL experiments
   * MTL experiments with AURC corpus
   * Formulation of abstract for presentation
   * Preparation of midterm presentation

---++ Week 27.01. - 02.02.:
   * Final preparations for midterm presentation and holding presentation
   * Adapting the TL code for avoiding the prediction of BERT specific labels and repetition of the TL experiments
   * Pairwise MTL experiments
   * Evaluation of TL results for training more epochs

---++ Week 03.02. - 09.02.:
   * Further MTL experiments
   * Further TL experiments (without prediction of BERT labels)
   * Adaptation of mt-dnn model for using SciBERT
   * Binarizing the data for TL experiments (according to results of annotation study)
   * Revision of meta learning literature
   * Some repetitions of experiments with other seeds
   * Adaptation of TL model for binarized experiments
   * Start writing on the introduction of the thesis

---++ Week 10.02. - 16.02.:
   * Implementation of a meta learning model for sequence labeling task
   * First experiments with MetaL
   * Adaptation of the MetaL model with a representative evaluation possibility
   * Adaptation of the MetaL model for finetuning the meta learned model
   * Continue writing on the introduction
   * Bugfixing MetaL code
   * Final evaluation of the annotation study; final preprocessing (binarizing) of data for TL experiments; final binarized experiments with TL
   * Review of related work chapter

---++ Week 17.02. - 23.02.:
   * MetaL experiments
   * Further review of related work chapter
   * Review of TL and MTL code for more user-friendly execution; preparation of code for further repetition of essential experiments with other seeds to get more reliable results

-- Main.TilmanBeck - 2019-11-12

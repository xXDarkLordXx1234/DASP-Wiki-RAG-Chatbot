%META:TOPICINFO{author="StefanHenss" date="1349947293" format="1.1" version="1"}%
%META:TOPICPARENT{name="StefanHenss"}%
---+ Keyword Assignment<br /><br />---++ Current Design<br />  * State: Currently selected keywords.<br />  * Actions: Add keyword (either from the document or from a given vocabulary), remove keyword, stop<br />  * Rewards: 0.001 for correct assignment, 0.001 for wrong assignment, 0.005 for removing wrong keywords, -0.02 for removing correct keywords and the selection's f-score for the terminal state. <br />  * Future Rewards: Discount factor of 0.25. Will be increased when crawler performance improves.<br />  * Predictions: The observed reward from past actions is stored and future reward predicted through supervised learning.<br /><br />---++ Current Features<br />| *Feature* | *Description* |<br />| addAction | Is this action adding a keyword? |<br />| removeAction | Is this action removing a keyword? |<br />| stopAction | Is this action stopping keyword assignment? |<br />| keywordLength | The length of the keyword that is either added or removed. |<br />| textContains | Is this keyword used in the text itself? |<br />| keywords | How many keywords are currently assigned? |<br /><br />---++ Evaluation<br />As with other scenarios like summarization, we can use a cross-validation setup. First, a policy is learned on training data, using rewards as above. Second, the policy is used to extract keywords for validation texts and the outcome is evaluated using IR metrics. Also, rewards from the training phase can be sampled and used to analyze and optimize the predictive performance.
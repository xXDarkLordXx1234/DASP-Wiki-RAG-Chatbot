%META:TOPICINFO{author="BaseUserMapping_999" date="1360762007" format="1.1" version="18"}%
%META:TOPICPARENT{name="HakanEroglu"}%
---+ Logbuch

%TOC%

---++ Dezember 2008
---+++ 16.12.2008

- Implementierung folgender Komponenten:
   1 *BlogReader* : Erstellung eines CAS-Objects mit Nodes und Edges
   2 *TransformJUNG.java* : Transformation zu einem JUNG-based Graphen
   3 *Results*: Komponente zur Berechnung des PageRanks
   4 *Printer*: Klasse zur Ausgabe der Rankings

Desweiteren Berechnung von "Characteristic Path Length", "Clustering Coefficient"

- ToDo:
   * Berechnung von "Connectedness"
   * Berechnung von "Characteristic Path Length for Tags"
   * Berechnung von "Cumulative probability distribution of node strength"

---+++ 17.12.2008
   * Erstellung einer Kopie von JUNG's PageRank zu FolkRank, um eigene Justierungen vorzunehmen.
   * Weitere Effizienzverbesserungen der Komponenten durch streichen unnötiger Zuweisungen und Berechnungen

---+++ 18.12.2008
   * Effizientere Berechnung von "Characteristic Path Length", "Clustering Coefficient"
   * Überlegungen über einen Mechanismus zur Erstellung einer effizienten Messung einer Trend Analyse
   * Des weiteren habe ich noch Überlegungen angestellt, noch weitere Charakteristika eines Folksonomy-Graphen einzubeziehen (z.B. Zentralität)

---+++ 19.12.2008
   * Analyse ADTree, context-Vector Berechnung
   * Semantic Network

---+++ 23.12.2008
   * ERLEDIGT: Berechnung von "Connectedness"
   * ERLEDIGT: Berechnung von "Characteristic Path Length for Tags"
   * TODO: Einbinden aller Charakterisika-Werte des Folksonomy-Graphen ins result-sheet für die spätere TrendAnalyse

---+++ 26.12.2008
   * Ausarbeitung TextRank, Adaptiver PageRank und FolkRank

---+++ 28.12.2008
   * ERLEDIGT: Berechnung von "Cumulative probability distribution of node strength"
   * Ausgabe der Analyse-Result-XML files mit allen Charakteristika des Folksonomy-Graphen

---+++ 01.01.2009
   * Implementierung einer Komponente, die aus einer Sammelung von TrendAnalyse-Files ein File generiert - geordnet nach Datum für die TrendAnalyse eines Themas in einem Zeitstrahl
   * Beginn der Implementierung eines Crawlers für Bibsonomy mithilfe des Bibsonomy APIs

---+++ 02.01.2009
   * Beginn Evaluierung des KeyExtraction Systems

---+++ 05.01.2009
   * Implementierung BibSonomy Crawler für Bibsonomy Corpus
   * Optimierung des Codes für effizientere Verarbeitung der Daten und Speicherverwaltung

---+++ 11.01.2009
   * Schreiben von UIMA Komponenten
   * Visualisierung der Graphen

---+++ 14.01.2009
   * Configuration of my new ubuntu PC in B117

---+++ 22.01.2009
   * Visualization of the folksonomy, co-occ graph

---+++ 03.02.2009
   * Finished programming of the TrendCPE which produces a chronological ordered Hashtable for each TAG, USER and POST
   Following values now can be visualized e.g. with JFree Chart: 
  	* private double rank1;
	* private double rank2;
	* private double rank3;
	* private double node_strength;
	* private double node_strength_nn;
	* private double degree;
	* private double cluster;
	* private double betweenness;
	* private double frequency;
	
	* private double characteristic_path;
	* private double clustering_coefficient;
	* private double cooccurence_coefficient;

---+++ 09.02.2009
   * Investigations how to figure out if a time line of values represents a trend
   * Chart Visualization using JFree now reliably possible for all measurements
   * Implementation of a CPE for the calculation of tag similarity using Wikipedia Similarity API
    --> Later using these information to rank these tags higher (PageRank and FolkRank)
    
---+++ 02.03.2009
   * Redesign of delicious Crawler (included some IP-hiding methods to skip the delicious blocking)
   * Calculation a sample dataset of delicious
   * Using MySQL darabase to process the freely available CiteULike database dump text file (first inserting into MySQL and then creating daily dumps)
   * Faxed the application for obtaining license for the recent Bibsonomy dataset
   * Working on the thesis

---+++ 17.03.2009
   * Obtained access to selma for computation of the large scale datasets of CiteULike, Bibsonomy and delicious
   * Writing on my thesis (status: 40 pages now without pictures and charts)